{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Contact Tracing",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrichr/visual-contact-tracing/blob/master/Visual_Contact_Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nLayYgLb2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
        "# get configs\n",
        "!git clone https://github.com/facebookresearch/detectron2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# May need to restart your runtime prior to this to let installation take effect\n",
        "\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85G46fHoT2kd",
        "colab_type": "text"
      },
      "source": [
        "Upload a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO2km6rXT10I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrAFRjpUsr4",
        "colab_type": "text"
      },
      "source": [
        "Or load from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uebmGrqTUwEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=$file_id\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id\" -o $file_name\n",
        "  with open(file_name, 'rb') as f:\n",
        "    data = f.read()\n",
        "    print('downloaded', len(data), 'bytes to', video_filename)\n",
        "\n",
        "\n",
        "file_id = '0Bzf1l8WmTwu0eUluQ2h1NWZQRjQ'\n",
        "video_filename = 'salsa_cpp_cam4.avi'\n",
        "download_file_from_google_drive(file_id, video_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eqWJ2UYAc6",
        "colab_type": "text"
      },
      "source": [
        "Re-encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvcgfcI1YC5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess as sp\n",
        "\n",
        "import imageio\n",
        "                                                                    \n",
        "ffmpeg = imageio.plugins.ffmpeg                                                \n",
        "try:                                                                           \n",
        "    ffmpeg.download()                                                          \n",
        "except:                                                                        \n",
        "    pass                                                                       \n",
        "ffmpeg_exe = ffmpeg.get_exe()\n",
        "\n",
        "video_path = video_filename \n",
        "start_time_seconds = 1\n",
        "duration_seconds = 10\n",
        "video_filename_reenc = video_filename + '-reenc.avi'\n",
        "cmd_parts = [\n",
        "  ffmpeg_exe,\n",
        "  '-i', video_path,\n",
        "  '-vcodec', 'h264',\n",
        "  '-acodec', 'aac',\n",
        "  #'-c', 'copy',\n",
        "  '-strict',\n",
        "  '-2',\n",
        "  '-ss', str(start_time_seconds),\n",
        "  '-t', str(duration_seconds),\n",
        "  '-y',\n",
        "  '-loglevel', 'debug',\n",
        "  '-an',\n",
        "  video_filename_reenc\n",
        "]\n",
        "print('Running cmd:\\n', ' '.join(cmd_parts))                                \n",
        "result = sp.run(                                                            \n",
        "    cmd_parts, stdout=sp.PIPE, stderr=sp.STDOUT, universal_newlines=True    \n",
        ")                                                                           \n",
        "print('Result:\\n', result.stdout)                                           \n",
        "\n",
        "with open(video_filename_reenc, 'rb') as f:\n",
        "  data = f.read()\n",
        "  print('wrote', len(data), 'bytes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lX3kQNvr8iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import tqdm\n",
        "\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "\n",
        "DEFAULT_CONFIG = 'detectron2/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'\n",
        "DEFAULT_CONF_THRESH = 0.7\n",
        "DEFAULT_OPTS = [\n",
        "  'MODEL.WEIGHTS',\n",
        "  model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"),\n",
        "]\n",
        "\n",
        "default_args = [DEFAULT_CONFIG, DEFAULT_OPTS, DEFAULT_CONF_THRESH]\n",
        "\n",
        "def setup_cfg(config=DEFAULT_CONFIG, opts=DEFAULT_OPTS, conf_thresh=DEFAULT_CONF_THRESH):\n",
        "    # load config from file and arguments\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(config)\n",
        "    cfg.merge_from_list(opts)\n",
        "    # Set score_threshold for builtin models\n",
        "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = conf_thresh\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = conf_thresh\n",
        "    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = conf_thresh\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "mp.set_start_method(\"spawn\", force=True)\n",
        "setup_logger(name=\"fvcore\")\n",
        "logger = setup_logger()\n",
        "logger.info(\"Arguments: \" + str(default_args))\n",
        "\n",
        "cfg = setup_cfg()\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "video_input = video_filename_reenc\n",
        "print('video_input:', video_input)\n",
        "output = f'{video_filename_reenc}-out.mp4'\n",
        "print('output:', output)\n",
        "\n",
        "video = cv2.VideoCapture(video_input)\n",
        "print('video:', video)\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "basename = os.path.basename(video_input)\n",
        "\n",
        "output_fname = './' + output\n",
        "print('output_fname:', output_fname)\n",
        "#assert not os.path.isfile(output_fname), output_fname\n",
        "output_file = cv2.VideoWriter(\n",
        "    filename=output_fname,\n",
        "    # some installation of opencv may not support x264 (due to its license),\n",
        "    # you can try other format (e.g. MPEG)\n",
        "    #fourcc=cv2.VideoWriter_fourcc(*\"x264\"),\n",
        "    #fourcc=cv2.VideoWriter_fourcc(*\"MPEG\"),\n",
        "    fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    fps=float(frames_per_second),\n",
        "    frameSize=(width, height),\n",
        "    isColor=True,\n",
        ")\n",
        "print('output_file:', output_file)\n",
        "assert os.path.isfile(video_input)\n",
        "\n",
        "metadata = MetadataCatalog.get(\n",
        "    cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
        ")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "instance_mode = ColorMode.IMAGE\n",
        "video_visualizer = VideoVisualizer(metadata, instance_mode)\n",
        "\n",
        "def process_predictions(frame, predictions):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    if \"panoptic_seg\" in predictions:\n",
        "        panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
        "        vis_frame = video_visualizer.draw_panoptic_seg_predictions(\n",
        "            frame, panoptic_seg.to(cpu_device), segments_info\n",
        "        )\n",
        "    elif \"instances\" in predictions:\n",
        "        predictions = predictions[\"instances\"].to(cpu_device)\n",
        "        vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
        "    elif \"sem_seg\" in predictions:\n",
        "        vis_frame = video_visualizer.draw_sem_seg(\n",
        "            frame, predictions[\"sem_seg\"].argmax(dim=0).to(cpu_device)\n",
        "        )\n",
        "\n",
        "    # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "    return vis_frame\n",
        "\n",
        "def _frame_from_video(video):\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "        if success:\n",
        "            yield frame\n",
        "        else:\n",
        "            break\n",
        "\n",
        "frame_gen = _frame_from_video(video)\n",
        "from google.colab.patches import cv2_imshow\n",
        "frames = []\n",
        "all_predictions = []\n",
        "vis_frames = []\n",
        "for i, frame in enumerate(tqdm.tqdm(frame_gen, total=num_frames)):\n",
        "\n",
        "  frames.append(frame)\n",
        "  predictions = predictor(frame)\n",
        "  all_predictions.append(predictions)\n",
        "\n",
        "  # TODO: do this after detecting infections\n",
        "  vis_frame = process_predictions(frame, predictions)\n",
        "  vis_frames.append(vis_frame)\n",
        "\n",
        "  if i < 1:\n",
        "    print('displaying frame', i)\n",
        "    cv2_imshow(vis_frame)\n",
        "  #output_file.write(vis_frame)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "print('all_predictions.shape:', all_predictions.shape)\n",
        "\n",
        "video.release()\n",
        "output_file.release()\n",
        "print('Done.')\n",
        "\n",
        "with open(output_fname, 'rb') as f:\n",
        "  print('wrote', len(f.read()), 'bytes')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu4ZnPXgk1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install cython_bbox\n",
        "#from cython_bbox import bbox_overlaps\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/issues/754#issuecomment-579463185\n",
        "JOINT_NAMES = [\n",
        "  \"nose\",\n",
        "  \"left_eye\", \"right_eye\",\n",
        "  \"left_ear\", \"right_ear\",\n",
        "  \"left_shoulder\", \"right_shoulder\",\n",
        "  \"left_elbow\", \"right_elbow\",\n",
        "  \"left_wrist\", \"right_wrist\",\n",
        "  \"left_hip\", \"right_hip\",\n",
        "  \"left_knee\", \"right_knee\",\n",
        "  \"left_ankle\", \"right_ankle\"\n",
        "]\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/blob/04958b93e1232935e126c2fd9e6ccd3f57c3a8f3/detectron2/utils/visualizer.py#L32\n",
        "KEYPOINT_THRESHOLD = 0.05\n",
        "\n",
        "# TODO: smoothing e.g. kalman or savgol\n",
        "# https://stackoverflow.com/a/52450682/95989\n",
        "max_num_instances = 0\n",
        "all_keypoints = []\n",
        "all_boxes = []\n",
        "for predictions in all_predictions:\n",
        "  #predictions['instances'].get_fields().keys()\n",
        "  #'pred_boxes', 'scores', 'pred_classes', 'pred_keypoints'\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  #import pdb; pdb.set_trace()\n",
        "  keypoints = np.asarray(instances.pred_keypoints)\n",
        "  boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  #print('keypoints:', keypoints.shape)\n",
        "  all_keypoints.append(keypoints)\n",
        "  all_boxes.append(boxes)\n",
        "  num_instances = keypoints.shape[0]\n",
        "  max_num_instances = max(max_num_instances, num_instances)\n",
        "print('max_num_instances:', max_num_instances)\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "\n",
        "# https://gist.github.com/meyerjo/dd3533edc97c81258898f60d8978eddc\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
        "    if interArea == 0:\n",
        "        return 0\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
        "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106\n",
        "def compute_pairwise_iou(a, b):\n",
        "  \"\"\"\n",
        "  a, b (np.ndarray) of shape Nx4 and Mx4.\n",
        "  The output is NxM, for each combination of boxes.\n",
        "  \"\"\"\n",
        "  Cs = []\n",
        "  for box_a in a:\n",
        "    row = []\n",
        "    for box_b in b:\n",
        "      # TODO: replace with cython_bbox\n",
        "      iou = bb_intersection_over_union(box_a, box_b)\n",
        "      row.append(1 - iou)\n",
        "    Cs.append(row)\n",
        "  C = np.array(Cs)\n",
        "  #print('a.shape:', a.shape)\n",
        "  #print('b.shape:', b.shape)\n",
        "  #print('C.shape:', C.shape)\n",
        "  return C\n",
        "\n",
        "\n",
        "def compute_distance_matrix(prev_boxes, cur_boxes):\n",
        "  # TODO: consider keypoint distance?\n",
        "  return compute_pairwise_iou(prev_boxes, cur_boxes)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C, prev_tracks):\n",
        "    \"\"\"\n",
        "    Computes the bipartite matching between the rows and columns, given the\n",
        "    cost matrix, C.\n",
        "    \"\"\"\n",
        "    C = C.copy()  # to avoid affecting the original matrix\n",
        "    prev_ids = []\n",
        "    cur_ids = []\n",
        "    while (C == np.inf).sum() != C.size:\n",
        "      #print('*' * 40)\n",
        "\n",
        "      # Find the lowest cost element\n",
        "      min_idx = C.argmin()\n",
        "      i, j = np.unravel_index(min_idx, C.shape)\n",
        "      min_val = C[i][j]\n",
        "      #print('min_idx:', min_idx, 'min_val:', min_val, 'i:', i, 'j:', j)\n",
        "\n",
        "      # Add to results\n",
        "      #print('adding to results:')\n",
        "      prev_ids.append(i)\n",
        "      cur_ids.append(j)\n",
        "      #print('prev_ids:', prev_ids)\n",
        "      #print('cur_ids:', cur_ids)\n",
        "      \n",
        "      # Remove from cost matrix\n",
        "      track = prev_tracks[i]\n",
        "      #print('track:', track)\n",
        "      track_idxs = [\n",
        "        idx for idx in range(len(prev_tracks))\n",
        "        if prev_tracks[idx] == track\n",
        "      ]\n",
        "      #print('track_idxs:', track_idxs)\n",
        "      C[:, j] = np.inf\n",
        "      for track_idx in track_idxs:\n",
        "        #print('removing track_idx:', track_idx)\n",
        "        C[track_idx, :] = np.inf\n",
        "      #num_removed_costs = (C == np.inf).sum()\n",
        "      #print('num_removed_costs:', num_infs)\n",
        "\n",
        "    return prev_ids, cur_ids\n",
        "\n",
        "\n",
        "def compute_matches(prev_boxes, cur_boxes, prev_tracks):\n",
        "  assert len(prev_boxes) == len(prev_tracks)\n",
        "  #matches = -np.ones((max_num_instances, ), dtype=np.int32)\n",
        "  matches = -np.ones((len(cur_boxes), ), dtype=np.int32)\n",
        "  if not prev_boxes.size:\n",
        "    return matches\n",
        "  C = compute_distance_matrix(prev_boxes, cur_boxes)\n",
        "  prev_inds, next_inds = bipartite_matching_greedy(C, prev_tracks)\n",
        "  #print('prev_inds:', prev_inds, len(prev_inds))\n",
        "  #print('next_inds:', next_inds, len(next_inds))\n",
        "  assert(len(prev_inds) == len(next_inds))\n",
        "  for i in range(len(prev_inds)):\n",
        "    #print('i:', i, 'next_inds[i]:', next_inds[i], 'prev_inds[i]', prev_inds[i])\n",
        "    matches[next_inds[i]] = prev_inds[i]\n",
        "    #print('matches:', matches)\n",
        "  return matches\n",
        "\n",
        "def get_frame_tracks(matches, prev_tracks, next_track_id):\n",
        "  frame_tracks = []\n",
        "  for i, m in enumerate(matches):\n",
        "    #print('i:', i, 'm:', m, 'len(prev_tracks):', len(prev_tracks ))\n",
        "    if m == -1 or m >= len(prev_tracks):  # didn't match to any\n",
        "      frame_tracks.append(next_track_id[0])\n",
        "      next_track_id[0] += 1\n",
        "      if next_track_id[0] >= MAX_TRACK_IDS:\n",
        "        # TODO: handle this\n",
        "        print('Exceeded max track ids')\n",
        "        next_track_id[0] %= MAX_TRACK_IDS\n",
        "    else:\n",
        "      frame_tracks.append(prev_tracks[m])\n",
        "  #print('prev_tracks:\\t', prev_tracks, 'len(prev_tracks):', len(prev_tracks))\n",
        "  print('frame_tracks:\\t', frame_tracks, 'len(frame_tracks):', len(frame_tracks))\n",
        "  return frame_tracks\n",
        "\n",
        "def visualize_predictions(frame, predictions):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    predictions = predictions[\"instances\"].to(cpu_device)\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
        "    # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "    return vis_frame\n",
        "\n",
        "# compute tracks, inspired by:\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L272\n",
        "MAX_TRACK_IDS = 999\n",
        "all_prev_boxes = []\n",
        "T = 10\n",
        "video_tracks = []\n",
        "next_track_id = [0]\n",
        "for frame_id, (frame, predictions) in enumerate(zip(frames, all_predictions)):\n",
        "  print('\\nframe_id:', frame_id)\n",
        "\n",
        "  instances = predictions['instances'].to(cpu_device)  \n",
        "  cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  prev_boxes = np.vstack(all_prev_boxes) if all_prev_boxes else np.array([])\n",
        "  all_prev_tracks = video_tracks[\n",
        "    max(0, frame_id - len(all_prev_boxes)) :\n",
        "    max(0, frame_id)\n",
        "  ]\n",
        "  #print('len(all_prev_tracks):', len(all_prev_tracks))\n",
        "  prev_tracks = np.hstack(all_prev_tracks) if all_prev_tracks else np.array([])\n",
        "  #print('prev_boxes.shape:', prev_boxes.shape)\n",
        "  #print('prev_tracks.shape:', prev_tracks.shape)\n",
        "\n",
        "  matches = compute_matches(prev_boxes, cur_boxes, prev_tracks)\n",
        "  print('matches:\\t', matches)\n",
        "  # matches[i] contains the index of the box in the previous frames\n",
        "  # corresponding to the box with index i in the current frame\n",
        "\n",
        "  #print('prev_tracks:', prev_tracks)\n",
        "  frame_tracks = get_frame_tracks(matches, prev_tracks, next_track_id)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (len(np.unique(frame_tracks)), len(frame_tracks))\n",
        "  video_tracks.append(frame_tracks)\n",
        "  all_prev_boxes.append(cur_boxes)\n",
        "  if len(all_prev_boxes) > T:\n",
        "    all_prev_boxes = all_prev_boxes[1:]\n",
        "\n",
        "  if frame_id < 3 or frame_id >= len(frames) - 3 or any([match == -1 for match in matches]):\n",
        "    vis_frame = visualize_predictions(frame, predictions)\n",
        "    for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "      cv2.putText(vis_frame, str(frame_track), (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]+1), int(box[1]+1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "    cv2_imshow(vis_frame)\n",
        "\n",
        "# TODO: filter out large position jumps that immediately return after one frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY5VMB7sggpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# TODO: smoothing (e.g. https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L600)\n",
        "# TODO: in each frame, compute distances between every pair of wrists\n",
        "# between different instances\n",
        "DISTANCE_THRESHOLD = 20\n",
        "print(len(all_predictions), 'frames')\n",
        "target_joint_names = [\n",
        "  'left_wrist',\n",
        "  'right_wrist'\n",
        "]\n",
        "infected_tracks = set(\n",
        "  #[np.random.randint(0, max_num_instances)]\n",
        "  [3]\n",
        ")\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "print('len(video_tracks):', len(video_tracks))\n",
        "assert(len(all_keypoints) == len(video_tracks))\n",
        "for frame_id, (keypoints, frame_tracks) in enumerate(zip(all_keypoints, video_tracks)):\n",
        "  print('*' * 40)\n",
        "  print('frame_id:', frame_id)\n",
        "  print('infected_tracks:', infected_tracks)\n",
        "  print('frame_tracks:', frame_tracks)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (len(np.unique(frame_tracks)), len(frame_tracks))\n",
        "  target_joint_vals = []\n",
        "  target_joint_probs = []\n",
        "  for keypoints_per_instance in keypoints:\n",
        "    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py#L703\n",
        "    # a tensor of shape (K, 3), where K is the number of keypoints\n",
        "    # and the last dimension corresponds to (x, y, probability).\n",
        "\n",
        "    for joint_name in target_joint_names:\n",
        "      joint_idx = JOINT_NAMES.index(joint_name)\n",
        "      joint_vals = keypoints_per_instance[joint_idx]\n",
        "      x, y, prob = joint_vals\n",
        "      # print(\n",
        "      #   'joint_name:', joint_name,\n",
        "      #   'joint_vals:', joint_vals,\n",
        "      #   'prob:', prob\n",
        "      # )\n",
        "      target_joint_vals.append([x, y])\n",
        "      target_joint_probs.append(prob)\n",
        "\n",
        "  target_joint_vals = np.array(target_joint_vals)\n",
        "  #print('target_joint_vals:', target_joint_vals.shape)\n",
        "\n",
        "  distances = squareform(pdist(target_joint_vals))\n",
        "  #print('distances.shape:', distances.shape)\n",
        "\n",
        "  '''\n",
        "  import sys\n",
        "  numpy.set_printoptions(\n",
        "    threshold=sys.maxsize,\n",
        "    formatter={'float': lambda x: \"{0:0.0f}\".format(x)}\n",
        "  )\n",
        "  print(distances)\n",
        "  '''\n",
        "  \n",
        "  hit_mask = distances < DISTANCE_THRESHOLD\n",
        "  #print('hit_mask.shape:', hit_mask.shape)\n",
        "  frame_infected_tracks = set()\n",
        "  frame_tracks = np.array(frame_tracks)\n",
        "  for infected_track in infected_tracks:\n",
        "    #print('-' * 20)\n",
        "    #print('infected_track:', infected_track)\n",
        "    infected_idx = np.where(frame_tracks == infected_track)[0]\n",
        "    #print('infected_idx:', infected_idx)\n",
        "    if not infected_idx.size:\n",
        "      #print('track', infected_track, 'no longer in frame')\n",
        "      continue\n",
        "    infected_idx = infected_idx[0]\n",
        "    \n",
        "    for i_joint in range(len(target_joint_names)):\n",
        "      #print('. ' * 20)\n",
        "      #print('i_joint:', i_joint)\n",
        "      row = hit_mask[infected_idx * len(target_joint_names) + i_joint]\n",
        "      #print('row:', [1 if v else 0 for v in row ])\n",
        "      #print('row.shape:', row.shape)\n",
        "      hit_idxs = np.where(row)[0]\n",
        "      hit_idxs = np.array([idx // len(target_joint_names) for idx in hit_idxs])\n",
        "      #print('hit_idxs:', hit_idxs)\n",
        "      #print('hit_idxs.shape:', hit_idxs.shape)\n",
        "      #print('frame_tracks:', frame_tracks)\n",
        "      #print('frame_tracks.shape:', frame_tracks.shape)\n",
        "      hit_tracks = frame_tracks[hit_idxs]\n",
        "      #print('hit_tracks:', hit_tracks)\n",
        "      # ignore self\n",
        "      hit_tracks = set(hit_tracks) - set([infected_track])\n",
        "      #print('hit_tracks:', hit_tracks)\n",
        "      frame_infected_tracks |= hit_tracks\n",
        "\n",
        "  # vis if new hit\n",
        "  new_infected_tracks = frame_infected_tracks - infected_tracks\n",
        "  if new_infected_tracks:\n",
        "    print('new_infected_tracks:', new_infected_tracks)\n",
        "    frame = frames[frame_id]\n",
        "    predictions = all_predictions[frame_id]\n",
        "    instances = predictions['instances'].to(cpu_device)\n",
        "    cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "    vis_frame = visualize_predictions(frame, predictions)\n",
        "    for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "      cv2.putText(vis_frame, str(frame_track), (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]+1), int(box[1]+1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "\n",
        "    # TODO: highlight\n",
        "    '''\n",
        "    infected_box = cur_boxes[infected_idx]\n",
        "    #print('infected_box:', infected_box)\n",
        "    for new_infected_track in new_infected_tracks:\n",
        "      print('new_infected_track:', new_infected_track)\n",
        "      new_infected_idx = np.where(frame_tracks == new_infected_track)[0][0]\n",
        "      hit_box = cur_boxes[new_infected_idx]\n",
        "      #print('hit_box:', hit_box)\n",
        "    '''\n",
        "\n",
        "    cv2_imshow(vis_frame)\n",
        "\n",
        "  infected_tracks |= new_infected_tracks\n",
        "\n",
        "  #confident = prob >= _KEYPOINT_THRESHOLD\n",
        "\n",
        "  # if frame_id > 50:\n",
        "  #   break\n",
        "\n",
        "print('final infected_tracks:', infected_tracks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGJoVm7x5Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Download the results\n",
        "if 0:\n",
        "  from google.colab import files\n",
        "  files.download(output)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRKsYoVJVCF1",
        "colab_type": "text"
      },
      "source": [
        "Limitations / Future Work\n",
        "- 2D results in false positives (due to lack of depth information) and false negatives (due to occlusion)\n",
        "  - Lack of depth can be mitigated with depth estimation: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\n",
        "  - Occlusion can be mitigated by using mutiple cameras: https://arxiv.org/pdf/2003.03972v2.pdf\n",
        "- Only keypoints are considered, not semantic segmentation masks, which may contain more information about whether contact ocurred.\n",
        "- Doesn't track individuals between videos"
      ]
    }
  ]
}