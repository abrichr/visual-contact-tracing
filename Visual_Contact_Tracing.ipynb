{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Contact Tracing",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrichr/visual-contact-tracing/blob/master/Visual_Contact_Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwL2oBtAsD3",
        "colab_type": "text"
      },
      "source": [
        "## TODO\n",
        "\n",
        "* Interpolate missing keypoints between frames\n",
        "* Smooth keypoint positions across frames\n",
        "  * Savgol or Kalman\n",
        "  * Optical Flow: https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L600\n",
        "* Propagate keypoints through frames using optical flow and add to distance matrix: https://arxiv.org/pdf/1804.06208.pdf\n",
        "* Require minimum number of consecutive contact frames\n",
        "* Add bounding box area difference to cost?\n",
        "* Add legend to output video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nLayYgLb2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1 cython_bbox\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
        "# get configs\n",
        "!git clone https://github.com/facebookresearch/detectron2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# May need to restart your runtime prior to this to let installation take effect\n",
        "\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "#setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import requests\n",
        "#import subprocess as sp\n",
        "\n",
        "import imageio\n",
        "from cython_bbox import bbox_overlaps\n",
        "from skimage.color import rgba2rgb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from detectron2.utils import visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.video_visualizer import VideoVisualizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85G46fHoT2kd",
        "colab_type": "text"
      },
      "source": [
        "Upload a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO2km6rXT10I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrAFRjpUsr4",
        "colab_type": "text"
      },
      "source": [
        "Or load from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uebmGrqTUwEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def download_file_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=$file_id\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id\" -o $file_name\n",
        "  with open(file_name, 'rb') as f:\n",
        "    data = f.read()\n",
        "    print('downloaded', len(data), 'bytes to', video_filename)\n",
        "\n",
        "\n",
        "file_id = '0Bzf1l8WmTwu0eUluQ2h1NWZQRjQ'\n",
        "video_filename = 'salsa_cpp_cam4.avi'\n",
        "download_file_from_google_drive(file_id, video_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eqWJ2UYAc6",
        "colab_type": "text"
      },
      "source": [
        "Re-encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvcgfcI1YC5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RUN_CONFIGS = [\n",
        "  # start_time_seconds, duration_seconds, start_infected_track\n",
        "  (1, 10, 3),\n",
        "  (10, 9.5, 2)\n",
        "]\n",
        "run_config = RUN_CONFIGS[1]\n",
        "start_time_seconds, duration_seconds, start_infected_track = run_config\n",
        "\n",
        "\n",
        "ffmpeg = imageio.plugins.ffmpeg                                                \n",
        "try:                                                                           \n",
        "    ffmpeg.download()                                                          \n",
        "except:                                                                        \n",
        "    pass                                                                       \n",
        "ffmpeg_exe = ffmpeg.get_exe()\n",
        "\n",
        "video_path = video_filename\n",
        "video_filename_reenc = video_filename + '-reenc.avi'\n",
        "\n",
        "cmd_parts = [\n",
        "  ffmpeg_exe,\n",
        "  '-i', video_path,\n",
        "  '-vcodec', 'h264',\n",
        "  '-acodec', 'aac',\n",
        "  #'-c', 'copy',\n",
        "  '-strict',\n",
        "  '-2',\n",
        "  '-ss', str(start_time_seconds),\n",
        "  '-t', str(duration_seconds),\n",
        "  '-y',\n",
        "  '-loglevel', 'debug',\n",
        "  '-an',\n",
        "  video_filename_reenc\n",
        "]\n",
        "cmd = ' '.join(cmd_parts)\n",
        "print('Running cmd:\\n', cmd)                                \n",
        "! $cmd                                \n",
        "\n",
        "with open(video_filename_reenc, 'rb') as f:\n",
        "  data = f.read()\n",
        "  print('wrote', len(data), 'bytes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lX3kQNvr8iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDE_KEYPOINTS = False\n",
        "DEFAULT_CONFIG = 'detectron2/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'\n",
        "DEFAULT_CONF_THRESH = 0.7\n",
        "DEFAULT_OPTS = [\n",
        "  'MODEL.WEIGHTS',\n",
        "  model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"),\n",
        "]\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/blob/04958b93e1232935e126c2fd9e6ccd3f57c3a8f3/detectron2/utils/visualizer.py#L32\n",
        "KEYPOINT_THRESHOLD = 0.04\n",
        "visualizer._KEYPOINT_THRESHOLD = KEYPOINT_THRESHOLD\n",
        "\n",
        "\n",
        "default_args = [DEFAULT_CONFIG, DEFAULT_OPTS, DEFAULT_CONF_THRESH]\n",
        "\n",
        "\n",
        "def setup_cfg(config=DEFAULT_CONFIG, opts=DEFAULT_OPTS, conf_thresh=DEFAULT_CONF_THRESH):\n",
        "  # load config from file and arguments\n",
        "  cfg = get_cfg()\n",
        "  cfg.merge_from_file(config)\n",
        "  cfg.merge_from_list(opts)\n",
        "  # Set score_threshold for builtin models\n",
        "  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = conf_thresh\n",
        "  cfg.freeze()\n",
        "  return cfg\n",
        "\n",
        "\n",
        "setup_logger(name=\"fvcore\")\n",
        "logger = setup_logger()\n",
        "logger.info(\"Arguments: \" + str(default_args))\n",
        "\n",
        "cfg = setup_cfg()\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "video_input = video_filename_reenc\n",
        "print('video_input:', video_input)\n",
        "assert os.path.isfile(video_input)\n",
        "video = cv2.VideoCapture(video_input)\n",
        "print('video:', video)\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "print('frames_per_second:', frames_per_second)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print('num_frames:', num_frames)\n",
        "basename = os.path.basename(video_input)\n",
        "\n",
        "kp_output_fname = f'{video_filename_reenc}-kp.mp4'\n",
        "print('kp_output_fname:', kp_output_fname)\n",
        "kp_output_file = cv2.VideoWriter(\n",
        "  filename=kp_output_fname,\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "  fps=float(frames_per_second),\n",
        "  frameSize=(width, height),\n",
        "  isColor=True,\n",
        ")\n",
        "\n",
        "metadata = MetadataCatalog.get(\n",
        "  cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
        ")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "instance_mode = ColorMode.IMAGE\n",
        "video_visualizer = VideoVisualizer(metadata, instance_mode)\n",
        "\n",
        "def process_predictions(frame, predictions):\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "  if \"panoptic_seg\" in predictions:\n",
        "    panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
        "    vis_frame = video_visualizer.draw_panoptic_seg_predictions(\n",
        "      frame, panoptic_seg.to(cpu_device), segments_info\n",
        "    )\n",
        "  elif \"instances\" in predictions:\n",
        "    predictions = predictions[\"instances\"].to(cpu_device)\n",
        "    if HIDE_KEYPOINTS:\n",
        "      predictions.remove('pred_keypoints')\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
        "  elif \"sem_seg\" in predictions:\n",
        "    vis_frame = video_visualizer.draw_sem_seg(\n",
        "      frame, predictions[\"sem_seg\"].argmax(dim=0).to(cpu_device)\n",
        "    )\n",
        "\n",
        "  # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "  vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "  return vis_frame\n",
        "\n",
        "def _frame_from_video(video):\n",
        "  while video.isOpened():\n",
        "    success, frame = video.read()\n",
        "    if success:\n",
        "      yield frame\n",
        "    else:\n",
        "      break\n",
        "\n",
        "frame_gen = _frame_from_video(video)\n",
        "frames = []\n",
        "all_predictions = []\n",
        "SHOW_NUM_FRAMES = 1\n",
        "WRITE_KP_OUTPUT = False\n",
        "for i, frame in enumerate(tqdm(frame_gen, total=num_frames)):\n",
        "\n",
        "  frames.append(frame)\n",
        "  start = time.time()\n",
        "  predictions = predictor(frame)\n",
        "  pred_times.append(time.time() - start)\n",
        "  all_predictions.append(predictions)\n",
        "\n",
        "  if i < SHOW_NUM_FRAMES or WRITE_KP_OUTPUT:\n",
        "    vis_frame = process_predictions(frame, predictions)\n",
        "    if i < SHOW_NUM_FRAMES:\n",
        "      print('displaying frame', i)\n",
        "      cv2_imshow(vis_frame)\n",
        "    if WRITE_KP_OUTPUT:\n",
        "      kp_output_file.write(vis_frame)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "#print('all_predictions.shape:', all_predictions.shape)\n",
        "\n",
        "video.release()\n",
        "kp_output_file.release()\n",
        "\n",
        "if WRITE_KP_OUTPUT:\n",
        "  with open(kp_output_fname, 'rb') as f:\n",
        "    print('wrote', len(f.read()), 'bytes to', kp_output_fname)\n",
        "  files.download(kp_output_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu4ZnPXgk1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_HIDE_KEYPOINTS = False\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/issues/754#issuecomment-579463185\n",
        "JOINT_NAMES = [\n",
        "  \"nose\",\n",
        "  \"left_eye\", \"right_eye\",\n",
        "  \"left_ear\", \"right_ear\",\n",
        "  \"left_shoulder\", \"right_shoulder\",\n",
        "  \"left_elbow\", \"right_elbow\",\n",
        "  \"left_wrist\", \"right_wrist\",\n",
        "  \"left_hip\", \"right_hip\",\n",
        "  \"left_knee\", \"right_knee\",\n",
        "  \"left_ankle\", \"right_ankle\"\n",
        "]\n",
        "\n",
        "# TODO: smoothing e.g. kalman or savgol\n",
        "# https://stackoverflow.com/a/52450682/95989\n",
        "max_num_instances = 0\n",
        "all_keypoints = []\n",
        "all_boxes = []\n",
        "for predictions in all_predictions:\n",
        "  #predictions['instances'].get_fields().keys()\n",
        "  #'pred_boxes', 'scores', 'pred_classes', 'pred_keypoints'\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  #import pdb; pdb.set_trace()\n",
        "  keypoints = np.asarray(instances.pred_keypoints)\n",
        "  boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  #print('keypoints:', keypoints.shape)\n",
        "  all_keypoints.append(keypoints)\n",
        "  all_boxes.append(boxes)\n",
        "  num_instances = keypoints.shape[0]\n",
        "  max_num_instances = max(max_num_instances, num_instances)\n",
        "print('max_num_instances:', max_num_instances)\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106\n",
        "def compute_pairwise_iou(a, b):\n",
        "  \"\"\"\n",
        "  a, b (np.ndarray) of shape Nx4 and Mx4.\n",
        "  The output is NxM, for each combination of boxes.\n",
        "  \"\"\"\n",
        "\n",
        "  C = 1 - bbox_overlaps(\n",
        "    np.ascontiguousarray(a, dtype=np.float64),\n",
        "    np.ascontiguousarray(b, dtype=np.float64),\n",
        "  )\n",
        "  return C\n",
        "\n",
        "\n",
        "def compute_distance_matrix(prev_boxes, cur_boxes):\n",
        "  # TODO: consider keypoint distance?\n",
        "  # TODO: weigh cost further away in time more heavily\n",
        "  return compute_pairwise_iou(prev_boxes, cur_boxes)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C, prev_tracks):\n",
        "    \"\"\"\n",
        "    Computes the bipartite matching between the rows and columns, given the\n",
        "    cost matrix, C.\n",
        "    \"\"\"\n",
        "    C = C.copy()  # to avoid affecting the original matrix\n",
        "    prev_ids = []\n",
        "    cur_ids = []\n",
        "    while (C == np.inf).sum() != C.size:\n",
        "      #print('*' * 40)\n",
        "\n",
        "      # Find the lowest cost element\n",
        "      min_idx = C.argmin()\n",
        "      i, j = np.unravel_index(min_idx, C.shape)\n",
        "      min_val = C[i][j]\n",
        "      #print('min_idx:', min_idx, 'min_val:', min_val, 'i:', i, 'j:', j)\n",
        "\n",
        "      # Add to results\n",
        "      #print('adding to results:')\n",
        "      prev_ids.append(i)\n",
        "      cur_ids.append(j)\n",
        "      #print('prev_ids:', prev_ids)\n",
        "      #print('cur_ids:', cur_ids)\n",
        "      \n",
        "      # Remove from cost matrix\n",
        "      track = prev_tracks[i]\n",
        "      #print('track:', track)\n",
        "      track_idxs = [\n",
        "        idx for idx in range(len(prev_tracks))\n",
        "        if prev_tracks[idx] == track\n",
        "      ]\n",
        "      #print('track_idxs:', track_idxs)\n",
        "      C[:, j] = np.inf\n",
        "      for track_idx in track_idxs:\n",
        "        #print('removing track_idx:', track_idx)\n",
        "        C[track_idx, :] = np.inf\n",
        "      #num_removed_costs = (C == np.inf).sum()\n",
        "      #print('num_removed_costs:', num_infs)\n",
        "\n",
        "    return prev_ids, cur_ids\n",
        "\n",
        "\n",
        "def compute_matches(prev_boxes, cur_boxes, prev_tracks):\n",
        "  assert len(prev_boxes) == len(prev_tracks)\n",
        "  #matches = -np.ones((max_num_instances, ), dtype=np.int32)\n",
        "  matches = -np.ones((len(cur_boxes), ), dtype=np.int32)\n",
        "  if not prev_boxes.size:\n",
        "    return matches\n",
        "  C = compute_distance_matrix(prev_boxes, cur_boxes)\n",
        "  prev_inds, next_inds = bipartite_matching_greedy(C, prev_tracks)\n",
        "  #print('prev_inds:', prev_inds, len(prev_inds))\n",
        "  #print('next_inds:', next_inds, len(next_inds))\n",
        "  assert(len(prev_inds) == len(next_inds))\n",
        "  for i in range(len(prev_inds)):\n",
        "    #print('i:', i, 'next_inds[i]:', next_inds[i], 'prev_inds[i]', prev_inds[i])\n",
        "    matches[next_inds[i]] = prev_inds[i]\n",
        "    #print('matches:', matches)\n",
        "  return matches\n",
        "\n",
        "def get_frame_tracks(matches, prev_tracks, next_track_id):\n",
        "  frame_tracks = []\n",
        "  for i, m in enumerate(matches):\n",
        "    #print('i:', i, 'm:', m, 'len(prev_tracks):', len(prev_tracks ))\n",
        "    if m == -1 or m >= len(prev_tracks):  # didn't match to any\n",
        "      frame_tracks.append(next_track_id[0])\n",
        "      next_track_id[0] += 1\n",
        "      if next_track_id[0] >= MAX_TRACK_IDS:\n",
        "        # TODO: handle this\n",
        "        print('Exceeded max track ids')\n",
        "        next_track_id[0] %= MAX_TRACK_IDS\n",
        "    else:\n",
        "      frame_tracks.append(prev_tracks[m])\n",
        "  #print('prev_tracks:\\t', prev_tracks, 'len(prev_tracks):', len(prev_tracks))\n",
        "  #print('frame_tracks:\\t', frame_tracks, 'len(frame_tracks):', len(frame_tracks))\n",
        "  return frame_tracks\n",
        "\n",
        "def visualize_predictions(frame, predictions, hide_keypoints=DEFAULT_HIDE_KEYPOINTS):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    predictions = predictions[\"instances\"].to(cpu_device)\n",
        "    if hide_keypoints:\n",
        "      predictions.remove('pred_keypoints')\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
        "    # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "    return vis_frame\n",
        "\n",
        "# compute tracks, inspired by:\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L272\n",
        "MAX_TRACK_IDS = 999\n",
        "all_prev_boxes = []\n",
        "T = 60\n",
        "video_tracks = []\n",
        "next_track_id = [0]\n",
        "for frame_id, (frame, predictions) in enumerate(tqdm(zip(frames, all_predictions), total=len(frames))):\n",
        "  #print('\\nframe_id:', frame_id)\n",
        "\n",
        "  instances = predictions['instances'].to(cpu_device)  \n",
        "  cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  prev_boxes = np.vstack(all_prev_boxes) if all_prev_boxes else np.array([])\n",
        "  all_prev_tracks = video_tracks[\n",
        "    max(0, frame_id - len(all_prev_boxes)) :\n",
        "    max(0, frame_id)\n",
        "  ]\n",
        "  #print('len(all_prev_tracks):', len(all_prev_tracks))\n",
        "  prev_tracks = np.hstack(all_prev_tracks) if all_prev_tracks else np.array([])\n",
        "  #print('prev_boxes.shape:', prev_boxes.shape)\n",
        "  #print('prev_tracks.shape:', prev_tracks.shape)\n",
        "\n",
        "  matches = compute_matches(prev_boxes, cur_boxes, prev_tracks)\n",
        "  #print('matches:\\t', matches)\n",
        "  # matches[i] contains the index of the box in the previous frames\n",
        "  # corresponding to the box with index i in the current frame\n",
        "\n",
        "  #print('prev_tracks:', prev_tracks)\n",
        "  frame_tracks = get_frame_tracks(matches, prev_tracks, next_track_id)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (len(np.unique(frame_tracks)), len(frame_tracks))\n",
        "  video_tracks.append(frame_tracks)\n",
        "  all_prev_boxes.append(cur_boxes)\n",
        "  if len(all_prev_boxes) > T:\n",
        "    all_prev_boxes = all_prev_boxes[1:]\n",
        "\n",
        "  SHOW_FRAME_ON_NEW_TRACK = False\n",
        "  HAS_NEW_MATCH = any([match == -1 for match in matches])\n",
        "  if frame_id < 3 or frame_id >= len(frames) - 3 or (SHOW_FRAME_ON_NEW_TRACK and HAS_NEW_MATCH):\n",
        "    print('Visualizing frame_id:', frame_id)\n",
        "    if HAS_NEW_MATCH:\n",
        "      print('New match:')\n",
        "    vis_frame = visualize_predictions(frame, predictions)\n",
        "    for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "    cv2_imshow(vis_frame)\n",
        "\n",
        "# TODO: filter out large position jumps that immediately return after one frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFGsgg2if_cS",
        "colab_type": "text"
      },
      "source": [
        "## Detect Infections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY5VMB7sggpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DISTANCE_THRESHOLD = 20\n",
        "MIN_CONSECUTIVE_HIT_FRAMES = 1\n",
        "WRITE_INF_OUTPUT = True\n",
        "\n",
        "print('DISTANCE_THRESHOLD:', DISTANCE_THRESHOLD)\n",
        "print('MIN_CONSECUTIVE_HIT_FRAMES:', MIN_CONSECUTIVE_HIT_FRAMES)\n",
        "\n",
        "inf_output_fname = f'{video_filename_reenc}-inf.mp4'\n",
        "print('inf_output_fname:', inf_output_fname)\n",
        "inf_output_file = cv2.VideoWriter(\n",
        "    filename=inf_output_fname,\n",
        "    fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    fps=float(frames_per_second),\n",
        "    frameSize=(width, height),\n",
        "    isColor=True,\n",
        ")\n",
        "\n",
        "print(len(all_predictions), 'frames')\n",
        "target_joint_names = [\n",
        "  'left_wrist',\n",
        "  'right_wrist'\n",
        "]\n",
        "infected_tracks = set(\n",
        "  #[np.random.randint(0, max_num_instances)]\n",
        "  [start_infected_track]\n",
        ")\n",
        "vis_times = []\n",
        "inf_times = []\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "print('len(video_tracks):', len(video_tracks))\n",
        "assert(len(all_keypoints) == len(video_tracks))\n",
        "num_tracks = max(max(frame_tracks) for frame_tracks in video_tracks)\n",
        "print('num_tracks:', num_tracks)\n",
        "num_joints = len(target_joint_names)\n",
        "dim = num_tracks * num_joints\n",
        "consecutive_hits = np.zeros((dim, dim))\n",
        "for frame_id, (keypoints, frame_tracks) in enumerate(\n",
        "    tqdm(zip(all_keypoints, video_tracks), total=len(all_keypoints))\n",
        "):\n",
        "  #print('*' * 40)\n",
        "  #print('frame_id:', frame_id)\n",
        "  #print('infected_tracks:', infected_tracks)\n",
        "  #print('frame_tracks:', frame_tracks)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (len(np.unique(frame_tracks)), len(frame_tracks))\n",
        "  target_joint_vals = []\n",
        "  target_joint_probs = []\n",
        "  for keypoints_per_instance in keypoints:\n",
        "    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py#L703\n",
        "    # a tensor of shape (K, 3), where K is the number of keypoints\n",
        "    # and the last dimension corresponds to (x, y, probability).\n",
        "\n",
        "    for joint_name in target_joint_names:\n",
        "      joint_idx = JOINT_NAMES.index(joint_name)\n",
        "      joint_vals = keypoints_per_instance[joint_idx]\n",
        "      x, y, prob = joint_vals\n",
        "      # TODO: use confidence?\n",
        "      #confident = prob >= _KEYPOINT_THRESHOLD\n",
        "      # print(\n",
        "      #   'joint_name:', joint_name,\n",
        "      #   'joint_vals:', joint_vals,\n",
        "      #   'prob:', prob\n",
        "      # )\n",
        "      target_joint_vals.append([x, y])\n",
        "      target_joint_probs.append(prob)\n",
        "\n",
        "  target_joint_vals = np.array(target_joint_vals)\n",
        "  #print('target_joint_vals:', target_joint_vals.shape)\n",
        "\n",
        "  start = time.time()\n",
        "  distances = squareform(pdist(target_joint_vals))\n",
        "  #print('distances.shape:', distances.shape)\n",
        "\n",
        "  PRINT_DISTANCES = False\n",
        "  if PRINT_DISTANCES:\n",
        "    np.set_printoptions(\n",
        "      threshold=sys.maxsize,\n",
        "      formatter={'float': lambda x: \"{0:0.0f}\".format(x)}\n",
        "    )\n",
        "    print('distances:')\n",
        "    print(distances)\n",
        "    np.set_printoptions()\n",
        "  \n",
        "  hit_mask = distances < DISTANCE_THRESHOLD\n",
        "  prob_mask = np.array(target_joint_probs) > KEYPOINT_THRESHOLD\n",
        "  hit_mask[~prob_mask, :] = False\n",
        "  hit_mask[:, ~prob_mask] = False\n",
        "  #print('hit_mask.shape:', hit_mask.shape)\n",
        "  frame_infected_tracks = set()\n",
        "  frame_tracks = np.array(frame_tracks)\n",
        "  hit_count_by_track_joint_idx_tup = {}\n",
        "  joint_idx_by_track_joint_idx = {}\n",
        "  for infected_track in infected_tracks:\n",
        "    #print('-' * 20)\n",
        "    #print('infected_track:', infected_track)\n",
        "    infected_idx = np.where(frame_tracks == infected_track)[0]\n",
        "    if not infected_idx.size:\n",
        "      #print('track', infected_track, 'no longer in frame')\n",
        "      continue\n",
        "    infected_idx = infected_idx[0]\n",
        "    #print('infected_idx:', infected_idx)\n",
        "    \n",
        "    for i_joint in range(len(target_joint_names)):\n",
        "      #print('. ' * 20)\n",
        "      #print('i_joint:', i_joint)\n",
        "      infected_joint_idx = infected_idx * len(target_joint_names) + i_joint\n",
        "      row = hit_mask[infected_joint_idx]\n",
        "      #print('row:', [1 if v else 0 for v in row ])\n",
        "      #print('row.shape:', row.shape)\n",
        "      hit_joint_idxs = np.where(row)[0].tolist()\n",
        "      #print('hit_joint_idxs:', hit_joint_idxs)\n",
        "\n",
        "      for hit_joint_idx in hit_joint_idxs:\n",
        "        hit_track_idx = hit_joint_idx // num_joints\n",
        "        hit_track = frame_tracks[hit_track_idx]\n",
        "\n",
        "        infected_track_joint_idx = infected_track * num_joints + i_joint\n",
        "        hit_track_joint_idx = hit_track * num_joints + i_joint\n",
        "\n",
        "        if infected_track_joint_idx == hit_track_joint_idx:\n",
        "          continue\n",
        "\n",
        "        joint_idx_by_track_joint_idx[infected_track_joint_idx] = infected_joint_idx\n",
        "        joint_idx_by_track_joint_idx[hit_track_joint_idx] = hit_joint_idx\n",
        "\n",
        "        hit_count = 1 + (\n",
        "            consecutive_hits[infected_track_joint_idx][hit_track_joint_idx]\n",
        "        )\n",
        "        track_joint_idx_tup = (infected_track_joint_idx, hit_track_joint_idx)\n",
        "\n",
        "        # TODO: understand why this happens\n",
        "        #assert track_joint_idx_tup not in hit_count_by_track_joint_idx_tup, (\n",
        "        #    track_joint_idx_tup, hit_count_by_track_joint_idx_tup\n",
        "        #)\n",
        "        if track_joint_idx_tup in hit_count_by_track_joint_idx_tup:\n",
        "          print(\n",
        "            '!!! WARNING',\n",
        "            'track_joint_idx_tup was already in hit_count_by_track_joint_idx_tup:',\n",
        "            track_joint_idx_tup, hit_count_by_track_joint_idx_tup\n",
        "          )\n",
        "        hit_count_by_track_joint_idx_tup.setdefault(track_joint_idx_tup, 0)\n",
        "        hit_count_by_track_joint_idx_tup[track_joint_idx_tup] = hit_count\n",
        "  \n",
        "  # TODO: don't fill if missing joint_idx belongs to track that's not visible\n",
        "  # in this frame, or if it's within e.g. MIN_CONSECUTIVE_DEAD_KP_FRAMES frames\n",
        "  # (it might reappear)\n",
        "  consecutive_hits.fill(0)\n",
        "  for (a_idx, b_idx), hit_count in hit_count_by_track_joint_idx_tup.items():\n",
        "    consecutive_hits[a_idx][b_idx] = hit_count\n",
        "    consecutive_hits[b_idx][a_idx] = hit_count\n",
        "  PRINT_CONSECUTIVE_HITS = False\n",
        "  if PRINT_CONSECUTIVE_HITS:\n",
        "    np.set_printoptions(\n",
        "      threshold=sys.maxsize,\n",
        "      formatter={'float': lambda x: \"{0:0.0f}\".format(x)}\n",
        "    )\n",
        "    print('consecutive_hits:')\n",
        "    print(consecutive_hits)\n",
        "    np.set_printoptions()\n",
        "\n",
        "\n",
        "  hit_tracks = []\n",
        "  all_hit_track_joint_idx_tups = []\n",
        "  hit_track_joint_idx_tups = []\n",
        "  for track_joint_idx_tup, hit_count in hit_count_by_track_joint_idx_tup.items():\n",
        "    infected_track_joint_idx, hit_track_joint_idx = track_joint_idx_tup\n",
        "    infected_track = infected_track_joint_idx // num_joints\n",
        "    hit_track = hit_track_joint_idx // num_joints\n",
        "    if infected_track == hit_track:\n",
        "      continue\n",
        "\n",
        "    all_hit_track_joint_idx_tups.append(track_joint_idx_tup)\n",
        "    if hit_count < MIN_CONSECUTIVE_HIT_FRAMES:\n",
        "      continue\n",
        "    hit_track_joint_idx_tups.append(track_joint_idx_tup)\n",
        "    hit_track = hit_track_joint_idx // num_joints\n",
        "    hit_tracks.append(hit_track)\n",
        "\n",
        "  #print('hit_tracks:', hit_tracks)\n",
        "  frame_infected_tracks |= set(hit_tracks)\n",
        "  inf_times.append(time.time() - start)\n",
        "\n",
        "  start = time.time()\n",
        "  frame = frames[frame_id]\n",
        "  predictions = all_predictions[frame_id]\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  vis_frame = visualize_predictions(frame, predictions, hide_keypoints=True)\n",
        "  for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "    cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "    cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "\n",
        "  b_channel, g_channel, r_channel = cv2.split(vis_frame)\n",
        "  alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 64\n",
        "\n",
        "  # highlight infected\n",
        "  #print('frame_infected_tracks:', frame_infected_tracks)\n",
        "  #print('infected_tracks:', infected_tracks)\n",
        "  highlight_tracks = frame_infected_tracks | infected_tracks\n",
        "  #print('highlight_tracks:', highlight_tracks)\n",
        "  highlight_boxes = []\n",
        "  for track in highlight_tracks:\n",
        "    track_id = np.where(frame_tracks == track)[0]\n",
        "    if not track_id.size:\n",
        "      # track is not visible in this frame\n",
        "      continue\n",
        "    track_id = track_id[0]\n",
        "    #print('track_id:', track_id)\n",
        "    box = cur_boxes[track_id]\n",
        "    #print('box:', box)\n",
        "    highlight_boxes.append(box)\n",
        "  #print('highlight_boxes:', highlight_boxes)\n",
        "  for box in highlight_boxes:\n",
        "    #print('box:', box, 'box[0]:', box[0])\n",
        "    xA = int(box[0])\n",
        "    yA = int(box[1])\n",
        "    xB = int(box[2])\n",
        "    yB = int(box[3])\n",
        "    alpha_channel[yA:yB, xA:xB] = 128\n",
        "    alpha_channel[yA, xA:xB] = 255\n",
        "    alpha_channel[yB, xA:xB] = 255\n",
        "    alpha_channel[yA:yB, xA] = 255\n",
        "    alpha_channel[yA:yB, xB] = 255\n",
        "\n",
        "    for channel in b_channel, g_channel, r_channel:\n",
        "      channel[yA, xA:xB] = 0\n",
        "      channel[yB, xA:xB] = 0\n",
        "      channel[yA:yB, xA] = 0\n",
        "      channel[yA:yB, xB] = 0\n",
        "\n",
        "  # highlight hits\n",
        "  HIGHLIGHT_AFTER_MIN_CONSECUTIVE_ONLY = False\n",
        "  if HIGHLIGHT_AFTER_MIN_CONSECUTIVE_ONLY:\n",
        "    highlight_track_joint_idx_tups = hit_track_joint_idx_tups\n",
        "  else:\n",
        "    highlight_track_joint_idx_tups = all_hit_track_joint_idx_tups\n",
        "\n",
        "  r = 20\n",
        "  for track_joint_idx_tup in highlight_track_joint_idx_tups:\n",
        "    infected_track_joint_idx, hit_track_joint_idx = track_joint_idx_tup\n",
        "\n",
        "    infected_joint_idx = joint_idx_by_track_joint_idx[infected_track_joint_idx]\n",
        "    hit_joint_idx = joint_idx_by_track_joint_idx[hit_track_joint_idx]\n",
        "\n",
        "    infected_joint_vals = target_joint_vals[infected_joint_idx]\n",
        "    hit_joint_vals = target_joint_vals[hit_joint_idx]\n",
        "    xA, yA = infected_joint_vals\n",
        "    xB, yB = hit_joint_vals\n",
        "    cA = int(min(xA, xB)) - r\n",
        "    cB = int(max(xA, xB)) + r\n",
        "    rA = int(min(yA, yB)) - r\n",
        "    rB = int(max(yA, yB)) + r\n",
        "\n",
        "    # TODO: rename, this is confusing\n",
        "    infected_track = infected_track_joint_idx // num_joints\n",
        "    hit_track = hit_track_joint_idx // num_joints\n",
        "    sorted_tracks = sorted([infected_track, hit_track])\n",
        "    label = f'{sorted_tracks[0]}:{sorted_tracks[1]}'\n",
        "\n",
        "    alpha_channel[rA:rB, cA:cB] = 255\n",
        "    for channel in b_channel, g_channel, r_channel:\n",
        "      channel[rA, cA:cB] = 255\n",
        "      channel[rB, cA:cB] = 255\n",
        "      channel[rA:rB, cA] = 255\n",
        "      channel[rA:rB, cB] = 255\n",
        "      cv2.putText(channel, label, (cA-2, rA-2), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0))\n",
        "      cv2.putText(channel, label, (cA-1, rA-1), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255))\n",
        "\n",
        "    \n",
        "  img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
        "  vis_times.append(time.time() - start)\n",
        "\n",
        "  new_infected_tracks = frame_infected_tracks - infected_tracks\n",
        "  if new_infected_tracks:\n",
        "    print('new_infected_tracks:', new_infected_tracks)\n",
        "  \n",
        "  if highlight_track_joint_idx_tups:\n",
        "    print('frame_id:', frame_id)\n",
        "    cv2_imshow(img_BGRA)\n",
        "\n",
        "\n",
        "  def bgra_2_bgr(bgra):\n",
        "    # faster (?) but inverted alpha -> brightness\n",
        "    b, g, r, a = cv2.split(bgra)\n",
        "    bgr = cv2.merge((b, g, r))\n",
        "    a = 255 - np.stack([a, a, a], axis=-1)\n",
        "    bgr = cv2.addWeighted(np.float32(bgr) / 255, .5, np.float32(a) / 255, .5, 0)\n",
        "    bgr = (bgr * 255).astype(np.uint8)\n",
        "    '''\n",
        "    # slower but matches cv2_imshow\n",
        "    rgba = cv2.cvtColor(bgra, cv2.COLOR_BGRA2RGBA)\n",
        "    rgb = rgba2rgb(rgba)\n",
        "    rgb = (rgb * 255).astype(np.uint8)\n",
        "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "    '''\n",
        "    return bgr\n",
        "\n",
        "  if WRITE_INF_OUTPUT:\n",
        "    img_BGR = bgra_2_bgr(img_BGRA)\n",
        "    inf_output_file.write(img_BGR)\n",
        "\n",
        "  infected_tracks |= frame_infected_tracks\n",
        "\n",
        "  # if frame_id > 50:\n",
        "  #   break\n",
        "\n",
        "print('final infected_tracks:', infected_tracks)\n",
        "print('inf_time:', sum(inf_times))\n",
        "print('vis_time:', sum(vis_times))\n",
        "\n",
        "inf_output_file.release()\n",
        "\n",
        "if WRITE_INF_OUTPUT:\n",
        "  with open(inf_output_fname, 'rb') as f:\n",
        "    print('wrote', len(f.read()), 'bytes to', inf_output_fname)\n",
        "  ! ls -alh $inf_output_fname\n",
        "  # this produces \"MessageError: TypeError: Failed to fetch\"\n",
        "  #files.download(inf_output_fname)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGJoVm7x5Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  if WRITE_INF_OUTPUT:\n",
        "    ! ls -alh $inf_output_fname\n",
        "    files.download(inf_output_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRKsYoVJVCF1",
        "colab_type": "text"
      },
      "source": [
        "Limitations / Future Work\n",
        "- 2D results in false positives (due to lack of depth information) and false negatives (due to occlusion)\n",
        "  - Lack of depth can be mitigated with depth estimation: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\n",
        "  - Occlusion can be mitigated by using mutiple cameras: https://arxiv.org/pdf/2003.03972v2.pdf\n",
        "- Only keypoints are considered, not semantic segmentation masks, which may contain more information about whether contact ocurred\n",
        "- Doesn't track individuals between videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fYd1w-Exbhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('frames_per_second:', frames_per_second)\n",
        "print('num_frames:', num_frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}