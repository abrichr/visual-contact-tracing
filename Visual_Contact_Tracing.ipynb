{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Contact Tracing",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrichr/visual-contact-tracing/blob/master/Visual_Contact_Tracing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwL2oBtAsD3",
        "colab_type": "text"
      },
      "source": [
        "## TODO\n",
        "\n",
        "* Interpolate missing keypoints between frames\n",
        "* Smooth keypoint positions across frames with optical flow: https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L600\n",
        "* Propagate keypoints through frames using optical flow and add to distance matrix: https://arxiv.org/pdf/1804.06208.pdf\n",
        "* Add bounding box area difference to cost?\n",
        "* Add legend to output video\n",
        "* Hide subsequent hits\n",
        "* Add padding in time around hits\n",
        "* Hide numbers for non-infected tracks\n",
        "* Re-number infected tracks\n",
        "* Inter video tracking\n",
        "* REST API (POST video/upload, GET video/contacts)\n",
        "* improve highlight on padded hits (e.g. thick red border, animation)\n",
        "* detect and blur faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nLayYgLb2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
        "!pip install cython pyyaml==5.1 cython_bbox\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "# install detectron2:\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
        "# get configs\n",
        "# TODO: can we read this from site-packages?\n",
        "!git clone https://github.com/facebookresearch/detectron2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# May need to restart your runtime prior to this to let installation take effect\n",
        "\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "#setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import requests\n",
        "#import subprocess as sp\n",
        "\n",
        "import imageio\n",
        "from cython_bbox import bbox_overlaps\n",
        "from skimage.color import rgba2rgb\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from detectron2.utils import visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "import torch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85G46fHoT2kd",
        "colab_type": "text"
      },
      "source": [
        "Upload a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO2km6rXT10I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrAFRjpUsr4",
        "colab_type": "text"
      },
      "source": [
        "Or load from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uebmGrqTUwEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def download_file_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=$file_id\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id\" -o $file_name\n",
        "  with open(file_name, 'rb') as f:\n",
        "    data = f.read()\n",
        "    print('downloaded', len(data), 'bytes to', video_filename)\n",
        "\n",
        "\n",
        "file_id = '0Bzf1l8WmTwu0eUluQ2h1NWZQRjQ'\n",
        "video_filename = 'salsa_cpp_cam4.avi'\n",
        "download_file_from_google_drive(file_id, video_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-eqWJ2UYAc6",
        "colab_type": "text"
      },
      "source": [
        "Re-encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvcgfcI1YC5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RUN_CONFIGS = [\n",
        "  # start_time_seconds, duration_seconds, start_infected_track\n",
        "  (1, 10, 3),\n",
        "  (10, 9.5, 2),\n",
        "  (10, 14, 2),\n",
        "  (1, 19.75, 3),\n",
        "  (4, 5, 2)\n",
        "]\n",
        "run_config = RUN_CONFIGS[4]\n",
        "start_time_seconds, duration_seconds, start_infected_track = run_config\n",
        "\n",
        "\n",
        "ffmpeg = imageio.plugins.ffmpeg                                                \n",
        "try:                                                                           \n",
        "    ffmpeg.download()                                                          \n",
        "except:                                                                        \n",
        "    pass                                                                       \n",
        "ffmpeg_exe = ffmpeg.get_exe()\n",
        "\n",
        "video_path = video_filename\n",
        "video_filename_reenc = video_filename + '-reenc.avi'\n",
        "\n",
        "cmd_parts = [\n",
        "  ffmpeg_exe,\n",
        "  '-i', video_path,\n",
        "  '-vcodec', 'h264',\n",
        "  '-acodec', 'aac',\n",
        "  #'-c', 'copy',\n",
        "  '-strict',\n",
        "  '-2',\n",
        "  '-ss', str(start_time_seconds),\n",
        "  '-t', str(duration_seconds),\n",
        "  '-y',\n",
        "  '-loglevel', 'debug',\n",
        "  '-an',\n",
        "  video_filename_reenc\n",
        "]\n",
        "cmd = ' '.join(cmd_parts)\n",
        "print('Running cmd:\\n', cmd)                                \n",
        "! $cmd                                \n",
        "\n",
        "with open(video_filename_reenc, 'rb') as f:\n",
        "  data = f.read()\n",
        "  print('wrote', len(data), 'bytes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "908kf_5_hWiJ",
        "colab_type": "text"
      },
      "source": [
        "## Detectron Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lX3kQNvr8iZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SHOW_NUM_FRAMES = 0\n",
        "WRITE_KP_OUTPUT = False\n",
        "HIDE_KEYPOINTS = False\n",
        "DEFAULT_CONFIG = 'detectron2/configs/COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml'\n",
        "DEFAULT_CONF_THRESH = 0.7\n",
        "DEFAULT_OPTS = [\n",
        "  'MODEL.WEIGHTS',\n",
        "  model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"),\n",
        "]\n",
        "# https://github.com/facebookresearch/detectron2/blob/04958b93e1232935e126c2fd9e6ccd3f57c3a8f3/detectron2/utils/visualizer.py#L32\n",
        "KEYPOINT_THRESHOLD = 0.04\n",
        "visualizer._KEYPOINT_THRESHOLD = KEYPOINT_THRESHOLD\n",
        "\n",
        "\n",
        "def setup_cfg(config=DEFAULT_CONFIG, opts=DEFAULT_OPTS, conf_thresh=DEFAULT_CONF_THRESH):\n",
        "  # load config from file and arguments\n",
        "  cfg = get_cfg()\n",
        "\n",
        "\n",
        "  # XXX untested\n",
        "  if not torch.cuda.device_count():\n",
        "    print('Running on CPU')\n",
        "    cfg.MODEL.DEVICE = 'cpu'\n",
        "\n",
        "\n",
        "  cfg.merge_from_file(config)\n",
        "  cfg.merge_from_list(opts)\n",
        "  # Set score_threshold for builtin models\n",
        "  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = conf_thresh\n",
        "  cfg.freeze()\n",
        "  return cfg\n",
        "\n",
        "\n",
        "setup_logger(name='core')\n",
        "logger = setup_logger('app')\n",
        "logger.info(f'arguments: {[DEFAULT_CONFIG, DEFAULT_OPTS, DEFAULT_CONF_THRESH]}')\n",
        "\n",
        "cfg = setup_cfg()\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "video_input = video_filename_reenc\n",
        "print('video_input:', video_input)\n",
        "assert os.path.isfile(video_input)\n",
        "video = cv2.VideoCapture(video_input)\n",
        "print('video:', video)\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "print('frames_per_second:', frames_per_second)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print('num_frames:', num_frames)\n",
        "basename = os.path.basename(video_input)\n",
        "\n",
        "kp_output_fname = f'{video_filename_reenc}-kp.mp4'\n",
        "print('kp_output_fname:', kp_output_fname)\n",
        "kp_output_file = cv2.VideoWriter(\n",
        "  filename=kp_output_fname,\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "  fps=float(frames_per_second),\n",
        "  frameSize=(width, height),\n",
        "  isColor=True,\n",
        ")\n",
        "\n",
        "metadata = MetadataCatalog.get(\n",
        "  cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
        ")\n",
        "cpu_device = torch.device(\"cpu\")\n",
        "instance_mode = ColorMode.IMAGE\n",
        "video_visualizer = VideoVisualizer(metadata, instance_mode)\n",
        "\n",
        "def process_predictions(frame, predictions):\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "  if \"panoptic_seg\" in predictions:\n",
        "    panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
        "    vis_frame = video_visualizer.draw_panoptic_seg_predictions(\n",
        "      frame, panoptic_seg.to(cpu_device), segments_info\n",
        "    )\n",
        "  elif \"instances\" in predictions:\n",
        "    predictions = predictions[\"instances\"].to(cpu_device)\n",
        "    if HIDE_KEYPOINTS:\n",
        "      predictions.remove('pred_keypoints')\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
        "  elif \"sem_seg\" in predictions:\n",
        "    vis_frame = video_visualizer.draw_sem_seg(\n",
        "      frame, predictions[\"sem_seg\"].argmax(dim=0).to(cpu_device)\n",
        "    )\n",
        "\n",
        "  # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "  vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "  return vis_frame\n",
        "\n",
        "def _frame_from_video(video):\n",
        "  while video.isOpened():\n",
        "    success, frame = video.read()\n",
        "    if success:\n",
        "      yield frame\n",
        "    else:\n",
        "      break\n",
        "\n",
        "frame_gen = _frame_from_video(video)\n",
        "frames = []\n",
        "all_predictions = []\n",
        "for i, frame in enumerate(tqdm(frame_gen, total=num_frames)):\n",
        "\n",
        "  frames.append(frame)\n",
        "  start = time.time()\n",
        "  predictions = predictor(frame)\n",
        "  all_predictions.append(predictions)\n",
        "\n",
        "  if i < SHOW_NUM_FRAMES or WRITE_KP_OUTPUT:\n",
        "    vis_frame = process_predictions(frame, predictions)\n",
        "    if i < SHOW_NUM_FRAMES:\n",
        "      print('displaying frame', i)\n",
        "      cv2_imshow(vis_frame)\n",
        "    if WRITE_KP_OUTPUT:\n",
        "      kp_output_file.write(vis_frame)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "#print('all_predictions.shape:', all_predictions.shape)\n",
        "\n",
        "video.release()\n",
        "kp_output_file.release()\n",
        "\n",
        "if WRITE_KP_OUTPUT:\n",
        "  with open(kp_output_fname, 'rb') as f:\n",
        "    print('wrote', len(f.read()), 'bytes to', kp_output_fname)\n",
        "  files.download(kp_output_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln35YEOFC8YH",
        "colab_type": "text"
      },
      "source": [
        "## Create tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu4ZnPXgk1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_HIDE_KEYPOINTS = True\n",
        "DEFAULT_HIDE_BOXES = True\n",
        "MAX_TRACK_IDS = 999\n",
        "SHOW_START_END_FRAMES = 0\n",
        "T = 60\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/issues/754#issuecomment-579463185\n",
        "JOINT_NAMES = [\n",
        "  \"nose\",\n",
        "  \"left_eye\", \"right_eye\",\n",
        "  \"left_ear\", \"right_ear\",\n",
        "  \"left_shoulder\", \"right_shoulder\",\n",
        "  \"left_elbow\", \"right_elbow\",\n",
        "  \"left_wrist\", \"right_wrist\",\n",
        "  \"left_hip\", \"right_hip\",\n",
        "  \"left_knee\", \"right_knee\",\n",
        "  \"left_ankle\", \"right_ankle\"\n",
        "]\n",
        "\n",
        "# TODO: smoothing e.g. kalman or savgol\n",
        "# https://stackoverflow.com/a/52450682/95989\n",
        "max_num_instances = 0\n",
        "all_keypoints = []\n",
        "all_boxes = []\n",
        "for predictions in all_predictions:\n",
        "  #predictions['instances'].get_fields().keys()\n",
        "  #'pred_boxes', 'scores', 'pred_classes', 'pred_keypoints'\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  #import pdb; pdb.set_trace()\n",
        "  keypoints = np.asarray(instances.pred_keypoints)\n",
        "  boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  #print('keypoints:', keypoints.shape)\n",
        "  all_keypoints.append(keypoints)\n",
        "  all_boxes.append(boxes)\n",
        "  num_instances = keypoints.shape[0]\n",
        "  max_num_instances = max(max_num_instances, num_instances)\n",
        "print('max_num_instances:', max_num_instances)\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106\n",
        "def compute_pairwise_iou(a, b):\n",
        "  \"\"\"\n",
        "  a, b (np.ndarray) of shape Nx4 and Mx4.\n",
        "  The output is NxM, for each combination of boxes.\n",
        "  \"\"\"\n",
        "\n",
        "  C = 1 - bbox_overlaps(\n",
        "    np.ascontiguousarray(a, dtype=np.float64),\n",
        "    np.ascontiguousarray(b, dtype=np.float64),\n",
        "  )\n",
        "  return C\n",
        "\n",
        "\n",
        "def compute_distance_matrix(prev_boxes, cur_boxes):\n",
        "  # TODO: consider keypoint distance?\n",
        "  # TODO: weigh cost further away in time more heavily\n",
        "  return compute_pairwise_iou(prev_boxes, cur_boxes)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C, prev_tracks):\n",
        "    \"\"\"\n",
        "    Computes the bipartite matching between the rows and columns, given the\n",
        "    cost matrix, C.\n",
        "    \"\"\"\n",
        "    C = C.copy()  # to avoid affecting the original matrix\n",
        "    prev_ids = []\n",
        "    cur_ids = []\n",
        "    MAX_COST = 1\n",
        "    min_costs = []\n",
        "    while (C == np.inf).sum() != C.size:\n",
        "      #print('*' * 40)\n",
        "\n",
        "      # Find the lowest cost element\n",
        "      min_idx = C.argmin()\n",
        "      i, j = np.unravel_index(min_idx, C.shape)\n",
        "      min_val = C[i][j]\n",
        "      #print('min_idx:', min_idx, 'min_val:', min_val, 'i:', i, 'j:', j)\n",
        "\n",
        "      # Add to results\n",
        "      min_cost = C.min()\n",
        "      min_costs.append(min_cost)\n",
        "      #print('adding to results:')\n",
        "      prev_ids.append(i)\n",
        "      if (not MAX_COST) or (min_cost < MAX_COST):\n",
        "        cur_ids.append(j)\n",
        "      else:\n",
        "        cur_ids.append(-1)\n",
        "      #print('prev_ids:', prev_ids)\n",
        "      #print('cur_ids:', cur_ids)\n",
        "      \n",
        "      # Remove from cost matrix\n",
        "      track = prev_tracks[i]\n",
        "      #print('track:', track)\n",
        "      track_idxs = [\n",
        "        idx for idx in range(len(prev_tracks))\n",
        "        if prev_tracks[idx] == track\n",
        "      ]\n",
        "      #print('track_idxs:', track_idxs)\n",
        "      C[:, j] = np.inf\n",
        "      for track_idx in track_idxs:\n",
        "        #print('removing track_idx:', track_idx)\n",
        "        C[track_idx, :] = np.inf\n",
        "      #num_removed_costs = (C == np.inf).sum()\n",
        "      #print('num_removed_costs:', num_infs)\n",
        "\n",
        "    #mean_min_cost = np.mean(min_costs)\n",
        "    #print('mean_min_cost:', mean_min_cost)\n",
        "\n",
        "    #max_min_cost = np.max(min_costs)\n",
        "    #print('max_min_cost:', max_min_cost)\n",
        "\n",
        "    return prev_ids, cur_ids\n",
        "\n",
        "\n",
        "def compute_matches(prev_boxes, cur_boxes, prev_tracks):\n",
        "  assert len(prev_boxes) == len(prev_tracks)\n",
        "  #matches = -np.ones((max_num_instances, ), dtype=np.int32)\n",
        "  matches = -np.ones((len(cur_boxes), ), dtype=np.int32)\n",
        "  if not prev_boxes.size:\n",
        "    return matches\n",
        "  C = compute_distance_matrix(prev_boxes, cur_boxes)\n",
        "  prev_inds, next_inds = bipartite_matching_greedy(C, prev_tracks)\n",
        "  #print('prev_inds:', prev_inds, len(prev_inds))\n",
        "  #print('next_inds:', next_inds, len(next_inds))\n",
        "  assert(len(prev_inds) == len(next_inds))\n",
        "  for i in range(len(prev_inds)):\n",
        "    #print('i:', i, 'next_inds[i]:', next_inds[i], 'prev_inds[i]', prev_inds[i])\n",
        "    matches[next_inds[i]] = prev_inds[i]\n",
        "    #print('matches:', matches)\n",
        "  return matches\n",
        "\n",
        "def get_frame_tracks(matches, prev_tracks, next_track_id):\n",
        "  frame_tracks = []\n",
        "  for i, m in enumerate(matches):\n",
        "    #print('i:', i, 'm:', m, 'len(prev_tracks):', len(prev_tracks ))\n",
        "    if m == -1 or m >= len(prev_tracks):  # didn't match to any\n",
        "      frame_tracks.append(next_track_id[0])\n",
        "      next_track_id[0] += 1\n",
        "      if next_track_id[0] >= MAX_TRACK_IDS:\n",
        "        # TODO: handle this\n",
        "        print('Exceeded max track ids')\n",
        "        next_track_id[0] %= MAX_TRACK_IDS\n",
        "    else:\n",
        "      frame_tracks.append(prev_tracks[m])\n",
        "  #print('prev_tracks:\\t', prev_tracks, 'len(prev_tracks):', len(prev_tracks))\n",
        "  #print('frame_tracks:\\t', frame_tracks, 'len(frame_tracks):', len(frame_tracks))\n",
        "  return frame_tracks\n",
        "\n",
        "def visualize_predictions(frame, instances, hide_keypoints=DEFAULT_HIDE_KEYPOINTS, hide_boxes=DEFAULT_HIDE_BOXES):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "    if hide_keypoints or hide_boxes:\n",
        "      # make a copy\n",
        "      instances = instances.to(cpu_device)\n",
        "    if hide_keypoints:\n",
        "      instances.remove('pred_keypoints')\n",
        "    if hide_boxes:\n",
        "      instances.pred_boxes.tensor = torch.Tensor()\n",
        "    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/video_visualizer.py#L53\n",
        "    # https://github.com/facebookresearch/detectron2/blob/b6fe828a2f3b2133f24cb93c1d0d74cb59c6a15d/detectron2/utils/video_visualizer.py#L53\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, instances)\n",
        "    # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "    return vis_frame\n",
        "\n",
        "# compute tracks, inspired by:\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L272\n",
        "all_prev_boxes = []\n",
        "video_tracks = []\n",
        "next_track_id = [0]\n",
        "for frame_id, (frame, predictions) in enumerate(tqdm(zip(frames, all_predictions), total=len(frames))):\n",
        "  #print('\\nframe_id:', frame_id)\n",
        "\n",
        "  instances = predictions['instances'].to(cpu_device)  \n",
        "  cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  prev_boxes = np.vstack(all_prev_boxes) if all_prev_boxes else np.array([])\n",
        "  all_prev_tracks = video_tracks[\n",
        "    max(0, frame_id - len(all_prev_boxes)) :\n",
        "    max(0, frame_id)\n",
        "  ]\n",
        "  #print('len(all_prev_tracks):', len(all_prev_tracks))\n",
        "  prev_tracks = np.hstack(all_prev_tracks) if all_prev_tracks else np.array([])\n",
        "  #print('prev_boxes.shape:', prev_boxes.shape)\n",
        "  #print('prev_tracks.shape:', prev_tracks.shape)\n",
        "\n",
        "  matches = compute_matches(prev_boxes, cur_boxes, prev_tracks)\n",
        "  #print('matches:\\t', matches)\n",
        "  # matches[i] contains the index of the box in the previous frames\n",
        "  # corresponding to the box with index i in the current frame\n",
        "\n",
        "  #print('prev_tracks:', prev_tracks)\n",
        "  frame_tracks = get_frame_tracks(matches, prev_tracks, next_track_id)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (\n",
        "      len(np.unique(frame_tracks)), len(frame_tracks)\n",
        "  )\n",
        "  video_tracks.append(np.array(frame_tracks))\n",
        "  all_prev_boxes.append(cur_boxes)\n",
        "  if len(all_prev_boxes) > T:\n",
        "    all_prev_boxes = all_prev_boxes[1:]\n",
        "\n",
        "  SHOW_FRAME_ON_NEW_TRACK = False\n",
        "  HAS_NEW_MATCH = any([match == -1 for match in matches])\n",
        "  if frame_id < SHOW_START_END_FRAMES or frame_id >= len(frames) - SHOW_START_END_FRAMES or (\n",
        "      SHOW_FRAME_ON_NEW_TRACK and HAS_NEW_MATCH\n",
        "  ):\n",
        "    print('Visualizing frame_id:', frame_id)\n",
        "    if HAS_NEW_MATCH:\n",
        "      print('New match:')\n",
        "    vis_frame = visualize_predictions(frame, instances)\n",
        "    for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "      cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "    cv2_imshow(vis_frame)\n",
        "\n",
        "# TODO: filter out large position jumps that immediately return after one frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt7EIziBDCsf",
        "colab_type": "text"
      },
      "source": [
        "## Savgol Filter Keypoints over Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5RWVpZNrauN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DO_SAVGOL_FILTER = True\n",
        "FILTER_WINDOW_LENGTH = 31\n",
        "FILTER_POLY_ORDER = 2\n",
        "\n",
        "\n",
        "num_tracks = 1 + max(max(frame_tracks) for frame_tracks in video_tracks)\n",
        "print('num_tracks:', num_tracks)\n",
        "target_joint_names = [\n",
        "  'left_wrist',\n",
        "  'right_wrist'\n",
        "]\n",
        "num_joints = len(target_joint_names)\n",
        "print('num_joints:', num_joints)\n",
        "\n",
        "# for plotting\n",
        "vis_kp_positions_by_joint_track = {}\n",
        "\n",
        "# for tracking\n",
        "kp_positions_by_joint_track = {}\n",
        "boxes_by_track = {}\n",
        "for predictions, frame_tracks in zip(all_predictions, video_tracks):\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  keypoints = np.asarray(instances.pred_keypoints)\n",
        "  boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "  for keypoints_per_instance, box, track in zip(keypoints, boxes, frame_tracks):\n",
        "    for i_joint, joint_name in enumerate(target_joint_names):\n",
        "      joint_idx = JOINT_NAMES.index(joint_name)\n",
        "      joint_vals = keypoints_per_instance[joint_idx]\n",
        "      joint_track = track * num_joints + i_joint\n",
        "      x, y, prob = joint_vals\n",
        "      kp_positions_by_joint_track.setdefault(joint_track, [])\n",
        "      kp_positions_by_joint_track[joint_track].append((x, y))\n",
        "    \n",
        "    # for plotting only\n",
        "    for joint_idx, joint_name in enumerate(JOINT_NAMES):\n",
        "      # values in here don't necessarily correspond to similarly named values\n",
        "      # elsewhere\n",
        "      joint_vals = keypoints_per_instance[joint_idx]\n",
        "      _joint_track = track * len(JOINT_NAMES) + joint_idx\n",
        "      x, y, prob = joint_vals\n",
        "      vis_kp_positions_by_joint_track.setdefault(_joint_track, [])\n",
        "      vis_kp_positions_by_joint_track[_joint_track].append((x, y))\n",
        "\n",
        "    boxes_by_track.setdefault(track, [])\n",
        "    boxes_by_track[track].append(box)\n",
        "\n",
        "  # fill in with nans for missing tracks\n",
        "  all_tracks = set(range(num_tracks))\n",
        "  missing_tracks = all_tracks - set(frame_tracks)\n",
        "  #print('all_tracks:', all_tracks)\n",
        "  #print('frame_tracks:', frame_tracks)\n",
        "  #print('missing_tracks:', missing_tracks)\n",
        "  for missing_track in missing_tracks:\n",
        "    for i_joint, joint_name in enumerate(target_joint_names):\n",
        "      joint_track = missing_track * num_joints + i_joint\n",
        "      #print('joint_track:', joint_track)\n",
        "      kp_positions_by_joint_track.setdefault(joint_track, [])\n",
        "      kp_positions_by_joint_track[joint_track].append((np.nan, np.nan))\n",
        "    for i_joint in range(len(JOINT_NAMES)):\n",
        "      _joint_track = missing_track * len(JOINT_NAMES) + i_joint\n",
        "      #print('_joint_track:', _joint_track)\n",
        "      vis_kp_positions_by_joint_track.setdefault(_joint_track, [])\n",
        "      vis_kp_positions_by_joint_track[_joint_track].append((np.nan, np.nan))\n",
        "    #print('missing_track:', missing_track)\n",
        "    boxes_by_track.setdefault(missing_track, [])\n",
        "    boxes_by_track[missing_track].append([np.nan for _ in range(4)])\n",
        "\n",
        "  # all tracks should now be of the same length\n",
        "  for d in (kp_positions_by_joint_track, vis_kp_positions_by_joint_track, boxes_by_track):\n",
        "    lengths = [len(positions) for positions in d.values()]\n",
        "    #print('lengths:', len(lengths), lengths)\n",
        "    assert np.all(np.array(lengths) == lengths[0]), lengths\n",
        "\n",
        "filtered_boxes_by_track = {}\n",
        "PLOT_BOX_FILTERING = True\n",
        "for track, boxes in boxes_by_track.items():\n",
        "  boxes = np.array(boxes)\n",
        "  #print('boxes.shape:', boxes.shape)\n",
        "  if DO_SAVGOL_FILTER:\n",
        "    filtered_boxes = savgol_filter(\n",
        "        boxes,\n",
        "        FILTER_WINDOW_LENGTH,\n",
        "        FILTER_POLY_ORDER,\n",
        "        axis=0,\n",
        "        mode='nearest'\n",
        "    )\n",
        "    nan_mask = np.isnan(filtered_boxes)\n",
        "    filtered_boxes[nan_mask] = boxes[nan_mask]\n",
        "  else:\n",
        "    filtered_boxes = boxes\n",
        "  \n",
        "  # print('boxes:')\n",
        "  # print(boxes)\n",
        "  # print('filtered_boxes:')\n",
        "  # print(filtered_boxes)\n",
        "  assert len(filtered_boxes) == len(boxes), (\n",
        "      len(filtered_boxes), len(boxes)\n",
        "  )\n",
        "  filtered_boxes_by_track[track] = filtered_boxes\n",
        "\n",
        "  if PLOT_BOX_FILTERING:\n",
        "    plt.figure()\n",
        "    labels = ['x', 'y']\n",
        "    plt.plot(boxes, label='raw', linestyle=':')\n",
        "    plt.plot(filtered_boxes, label='filtered')\n",
        "    plt.title(f'track {track} box')\n",
        "    plt.xlabel('frame')\n",
        "    plt.ylabel('position')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# for visualization only\n",
        "filtered_vis_kp_positions_by_joint_track = {}\n",
        "for joint_track, vis_kp_positions in vis_kp_positions_by_joint_track.items():\n",
        "  # TODO: resolve possible naming inconsistency\n",
        "  track = joint_track // len(JOINT_NAMES)\n",
        "  joint_idx = joint_track % len(JOINT_NAMES) \n",
        "  vis_kp_positions = np.array(vis_kp_positions)\n",
        "  if DO_SAVGOL_FILTER:\n",
        "    filtered_vis_kp_positions = savgol_filter(\n",
        "        vis_kp_positions,\n",
        "        FILTER_WINDOW_LENGTH,\n",
        "        FILTER_POLY_ORDER,\n",
        "        axis=0,\n",
        "        mode='nearest'\n",
        "    )\n",
        "    nan_mask = np.isnan(filtered_vis_kp_positions)\n",
        "    filtered_vis_kp_positions[nan_mask] = vis_kp_positions[nan_mask]\n",
        "  else:\n",
        "    filtered_vis_kp_positions = vis_kp_positions\n",
        "\n",
        "  assert len(filtered_vis_kp_positions) == len(vis_kp_positions), (\n",
        "      len(filtered_vis_kp_positions), len(vis_kp_positions)\n",
        "  )\n",
        "  filtered_vis_kp_positions_by_joint_track[joint_track] = filtered_vis_kp_positions\n",
        "\n",
        "filtered_kp_positions_by_joint_track = {}\n",
        "PLOT_KP_FILTERING = True\n",
        "for joint_track, kp_positions in kp_positions_by_joint_track.items():\n",
        "  # TODO: resolve possible naming inconsistency\n",
        "  track = joint_track // num_joints\n",
        "  joint_idx = joint_track % num_joints \n",
        "\n",
        "  joint_name = target_joint_names[joint_idx]\n",
        "  kp_positions = np.array(kp_positions)\n",
        "  if DO_SAVGOL_FILTER:\n",
        "    filtered_kp_positions = savgol_filter(\n",
        "        kp_positions,\n",
        "        FILTER_WINDOW_LENGTH,\n",
        "        FILTER_POLY_ORDER,\n",
        "        axis=0,\n",
        "        mode='nearest'\n",
        "    )\n",
        "    nan_mask = np.isnan(filtered_kp_positions)\n",
        "    filtered_kp_positions[nan_mask] = kp_positions[nan_mask]\n",
        "  else:\n",
        "    filtered_kp_positions = kp_positions\n",
        "  assert len(filtered_kp_positions) == len(kp_positions), (\n",
        "      len(filtered_kp_positions), len(kp_positions)\n",
        "  )\n",
        "  filtered_kp_positions_by_joint_track[joint_track] = filtered_kp_positions\n",
        "\n",
        "  if PLOT_KP_FILTERING:\n",
        "    plt.figure()\n",
        "    labels = ['x', 'y']\n",
        "    plt.plot(kp_positions, label='raw', linestyle=':')\n",
        "    plt.plot(filtered_kp_positions, label='filtered')\n",
        "    plt.title(f'track {track} {joint_name}')\n",
        "    plt.xlabel('frame')\n",
        "    plt.ylabel('position')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqSNe1utQpfi",
        "colab_type": "text"
      },
      "source": [
        "## Lowpass Filter Instances over Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRM3fZbeQtVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_EMPTY_PADDING = 30\n",
        "MAX_CONSECUTIVE_NONEMPTY_TO_REMOVE = 20\n",
        "\n",
        "# for visualization\n",
        "#for joint_track, filtered_vis_kp_positions in filtered_vis_kp_positions_by_joint_track.items():\n",
        "#for track, filtered_boxes in filtered_boxes_by_track.items():\n",
        "\n",
        "# for tracking\n",
        "for joint_track, filtered_kp_positions in filtered_kp_positions_by_joint_track.items():\n",
        "  N = len(filtered_kp_positions)\n",
        "  for i in range(N):\n",
        "    start_pad_start_idx = i - MIN_EMPTY_PADDING\n",
        "    start_pad_end_idx = i - 1\n",
        "    mid_start_idx = i\n",
        "    mid_end_idx = i + MAX_CONSECUTIVE_NONEMPTY_TO_REMOVE - 1\n",
        "    end_pad_start_idx = mid_end_idx + 1\n",
        "    end_pad_end_idx = end_pad_start_idx + MIN_EMPTY_PADDING - 1\n",
        "    if start_pad_start_idx < 0 or end_pad_end_idx >= N:\n",
        "      continue\n",
        "    #print('start_pad_start_idx:', start_pad_start_idx)\n",
        "    #print('start_pad_end_idx:', start_pad_end_idx)\n",
        "    #print('mid_start_idx:', mid_start_idx)\n",
        "    #print('mid_end_idx:', mid_end_idx)\n",
        "    #print('end_pad_start_idx:', end_pad_start_idx)\n",
        "    #print('end_pad_end_idx:', end_pad_end_idx)\n",
        "\n",
        "    start_pad_vals = filtered_kp_positions[start_pad_start_idx : start_pad_end_idx + 1]\n",
        "    mid_vals = filtered_kp_positions[mid_start_idx : mid_end_idx + 1]\n",
        "    end_pad_vals = filtered_kp_positions[end_pad_start_idx : end_pad_end_idx + 1]\n",
        "\n",
        "    start_pad_mask = np.all(np.isnan(start_pad_vals), axis=1)\n",
        "    #print('mid_vals.shape:', mid_vals.shape)\n",
        "    mid_mask = np.all(np.isnan(mid_vals), axis=1)\n",
        "    #print('mid_mask.shape:', mid_mask.shape)\n",
        "    end_pad_mask = np.all(np.isnan(end_pad_vals), axis=1)\n",
        "\n",
        "    num_start_pad_empty = start_pad_mask.sum()\n",
        "    num_mid_nonempty = (~mid_mask).sum()\n",
        "    num_end_pad_empty = end_pad_mask.sum()\n",
        "    #print('i:', i, 'num_start_pad_empty:', num_start_pad_empty, 'num_mid_nonempty:', num_mid_nonempty, 'num_end_pad_empty:', num_end_pad_empty)\n",
        "    if (\n",
        "        num_start_pad_empty == MIN_EMPTY_PADDING and\n",
        "        num_mid_nonempty > 0 and\n",
        "        num_end_pad_empty == MIN_EMPTY_PADDING\n",
        "    ):\n",
        "      print('removing num_mid_nonempty:', num_mid_nonempty)\n",
        "      filtered_kp_positions[mid_start_idx : mid_end_idx + 1] = np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFGsgg2if_cS",
        "colab_type": "text"
      },
      "source": [
        "## Detect Infections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY5VMB7sggpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DISTANCE_THRESHOLD = 20\n",
        "MIN_CONSECUTIVE_HIT_FRAMES = 1\n",
        "USE_FILTERED_KEYPOINTS = DO_SAVGOL_FILTER and False\n",
        "PLOT_FILTERED_KEYPOINTS = DO_SAVGOL_FILTER and True\n",
        "\n",
        "print('DISTANCE_THRESHOLD:', DISTANCE_THRESHOLD)\n",
        "print('MIN_CONSECUTIVE_HIT_FRAMES:', MIN_CONSECUTIVE_HIT_FRAMES)\n",
        "\n",
        "print(len(all_predictions), 'frames')\n",
        "infected_tracks = set(\n",
        "  #[np.random.randint(0, max_num_instances)]\n",
        "  [start_infected_track]\n",
        ")\n",
        "start_time = time.time()\n",
        "print('len(all_keypoints):', len(all_keypoints))\n",
        "print('len(video_tracks):', len(video_tracks))\n",
        "assert(len(all_keypoints) == len(video_tracks))\n",
        "dim = num_tracks * num_joints\n",
        "consecutive_hits = np.zeros((dim, dim))\n",
        "mean_keypoint_deltas = []\n",
        "mean_box_deltas = []\n",
        "frame_tups = []\n",
        "print('Tracking infections...')\n",
        "for frame_id, (keypoints, frame_tracks) in enumerate(\n",
        "    tqdm(zip(all_keypoints, video_tracks), total=len(all_keypoints))\n",
        "):\n",
        "  #print('*' * 40)\n",
        "  #print('frame_id:', frame_id)\n",
        "  #print('infected_tracks:', infected_tracks)\n",
        "  #print('frame_tracks:', frame_tracks)\n",
        "  assert len(np.unique(frame_tracks)) == len(frame_tracks), (\n",
        "      len(np.unique(frame_tracks)), len(frame_tracks)\n",
        "  )\n",
        "\n",
        "  target_joint_vals = []\n",
        "  target_joint_probs = []\n",
        "\n",
        "  for instance_idx, keypoints_per_instance in enumerate(keypoints):\n",
        "    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py#L703\n",
        "    # a tensor of shape (K, 3), where K is the number of keypoints\n",
        "    # and the last dimension corresponds to (x, y, probability).\n",
        "\n",
        "    for i_joint, joint_name in enumerate(target_joint_names):\n",
        "      joint = JOINT_NAMES.index(joint_name)        \n",
        "      joint_vals = keypoints_per_instance[joint]\n",
        "      x, y, prob = joint_vals\n",
        "\n",
        "      if USE_FILTERED_KEYPOINTS:\n",
        "        track = frame_tracks[instance_idx]\n",
        "        joint_track = track * num_joints + i_joint\n",
        "        # TODO: resolve naming inconsistency: joint_vals <> kp_positions\n",
        "        try:\n",
        "          filtered_joint_vals = (\n",
        "              filtered_kp_positions_by_joint_track[joint_track][frame_id]\n",
        "          )\n",
        "        except Exception as exc:\n",
        "          # TODO: remove if no longer occuring, or understand why if it is\n",
        "          print(exc)\n",
        "          filtered_joint_vals = (\n",
        "              filtered_kp_positions_by_joint_track[joint_track][-1]\n",
        "          )\n",
        "        #print('before x, y:', x, y)\n",
        "        x, y = filtered_joint_vals \n",
        "        #print('after x, y:', x, y)\n",
        "\n",
        "      # TODO: use confidence?\n",
        "      #confident = prob >= _KEYPOINT_THRESHOLD\n",
        "\n",
        "      if 0:\n",
        "        print(\n",
        "          'joint_name:', joint_name,\n",
        "          'joint_vals:', joint_vals,\n",
        "          'prob:', prob\n",
        "        )\n",
        "      target_joint_vals.append([x, y])\n",
        "      target_joint_probs.append(prob)\n",
        "\n",
        "  target_joint_vals = np.array(target_joint_vals)\n",
        "  #print('target_joint_vals:', target_joint_vals.shape)\n",
        "\n",
        "  distances = squareform(pdist(target_joint_vals))\n",
        "  #print('distances.shape:', distances.shape)\n",
        "\n",
        "  PRINT_DISTANCES = False\n",
        "  if PRINT_DISTANCES:\n",
        "    np.set_printoptions(\n",
        "      threshold=sys.maxsize,\n",
        "      formatter={'float': lambda x: \"{0:0.0f}\".format(x)}\n",
        "    )\n",
        "    print('distances:')\n",
        "    print(distances)\n",
        "    np.set_printoptions()\n",
        "  \n",
        "  hit_mask = distances < DISTANCE_THRESHOLD\n",
        "  prob_mask = np.array(target_joint_probs) > KEYPOINT_THRESHOLD\n",
        "  hit_mask[~prob_mask, :] = False\n",
        "  hit_mask[:, ~prob_mask] = False\n",
        "  #print('hit_mask.shape:', hit_mask.shape)\n",
        "  frame_infected_tracks = set()\n",
        "  #frame_tracks = np.array(frame_tracks)\n",
        "  hit_count_by_joint_track_tup = {}\n",
        "  joint_idx_by_joint_track = {}\n",
        "  for infected_track in infected_tracks:\n",
        "    #print('-' * 20)\n",
        "    #print('infected_track:', infected_track)\n",
        "    infected_idx = np.where(frame_tracks == infected_track)[0]\n",
        "    if not infected_idx.size:\n",
        "      #print('track', infected_track, 'no longer in frame')\n",
        "      continue\n",
        "    infected_idx = infected_idx[0]\n",
        "    #print('infected_idx:', infected_idx)\n",
        "    \n",
        "    for i_joint in range(len(target_joint_names)):\n",
        "      #print('. ' * 20)\n",
        "      #print('i_joint:', i_joint)\n",
        "      infected_joint_idx = infected_idx * len(target_joint_names) + i_joint\n",
        "      row = hit_mask[infected_joint_idx]\n",
        "      #print('row:', [1 if v else 0 for v in row ])\n",
        "      #print('row.shape:', row.shape)\n",
        "      hit_joint_idxs = np.where(row)[0].tolist()\n",
        "      #print('hit_joint_idxs:', hit_joint_idxs)\n",
        "\n",
        "      for hit_joint_idx in hit_joint_idxs:\n",
        "        hit_track_idx = hit_joint_idx // num_joints\n",
        "        hit_track = frame_tracks[hit_track_idx]\n",
        "\n",
        "        infected_joint_track = infected_track * num_joints + i_joint\n",
        "        hit_joint_track = hit_track * num_joints + i_joint\n",
        "\n",
        "        if infected_joint_track == hit_joint_track:\n",
        "          continue\n",
        "\n",
        "        joint_idx_by_joint_track[infected_joint_track] = infected_joint_idx\n",
        "        joint_idx_by_joint_track[hit_joint_track] = hit_joint_idx\n",
        "\n",
        "        hit_count = 1 + (\n",
        "            consecutive_hits[infected_joint_track][hit_joint_track]\n",
        "        )\n",
        "        joint_track_tup = (infected_joint_track, hit_joint_track)\n",
        "\n",
        "        if joint_track_tup in hit_count_by_joint_track_tup:\n",
        "          # This can happen if more than one of an infected person's joints\n",
        "          # touch someone else's joint (e.g. double handshake).\n",
        "          # Doing nothing treats this as a single hit.\n",
        "          pass\n",
        "        hit_count_by_joint_track_tup[joint_track_tup] = hit_count\n",
        "  \n",
        "  # TODO: don't fill if missing joint_idx belongs to track that's not visible\n",
        "  # in this frame, or if it's within e.g. MIN_CONSECUTIVE_DEAD_KP_FRAMES frames\n",
        "  # (it might reappear)\n",
        "  consecutive_hits.fill(0)\n",
        "  for (a_idx, b_idx), hit_count in hit_count_by_joint_track_tup.items():\n",
        "    consecutive_hits[a_idx][b_idx] = hit_count\n",
        "    consecutive_hits[b_idx][a_idx] = hit_count\n",
        "  PRINT_CONSECUTIVE_HITS = False\n",
        "  if PRINT_CONSECUTIVE_HITS:\n",
        "    np.set_printoptions(\n",
        "      threshold=sys.maxsize,\n",
        "      formatter={'float': lambda x: \"{0:0.0f}\".format(x)}\n",
        "    )\n",
        "    print('consecutive_hits:')\n",
        "    print(consecutive_hits)\n",
        "    np.set_printoptions()\n",
        "\n",
        "  hit_tracks = []\n",
        "  all_hit_joint_track_tups = []\n",
        "  consecutive_hit_joint_track_tups = []\n",
        "  for joint_track_tup, hit_count in hit_count_by_joint_track_tup.items():\n",
        "    infected_joint_track, hit_joint_track = joint_track_tup\n",
        "    infected_track = infected_joint_track // num_joints\n",
        "    hit_track = hit_joint_track // num_joints\n",
        "    if infected_track == hit_track:\n",
        "      continue\n",
        "\n",
        "    all_hit_joint_track_tups.append(joint_track_tup)\n",
        "    if hit_count < MIN_CONSECUTIVE_HIT_FRAMES:\n",
        "      continue\n",
        "    consecutive_hit_joint_track_tups.append(joint_track_tup)\n",
        "    hit_track = hit_joint_track // num_joints\n",
        "    hit_tracks.append(hit_track)\n",
        "\n",
        "  #print('hit_tracks:', hit_tracks)\n",
        "  frame_infected_tracks |= set(hit_tracks)\n",
        "\n",
        "  frame = frames[frame_id]\n",
        "  predictions = all_predictions[frame_id]\n",
        "  instances = predictions['instances'].to(cpu_device)\n",
        "  cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "\n",
        "  if USE_FILTERED_KEYPOINTS or PLOT_FILTERED_KEYPOINTS:\n",
        "\n",
        "    # TODO: rename\n",
        "    pred_boxes = cur_boxes.copy()\n",
        "    #print('pred_boxes:')\n",
        "    #print(pred_boxes)\n",
        "    num_instances = pred_boxes.shape[0]\n",
        "    assert pred_boxes.shape == (num_instances, 4), (pred_boxes.shape, (num_instances, 4))\n",
        "    pred_keypoints = instances.pred_keypoints.numpy().copy()\n",
        "    #print('pred_keypoints:')\n",
        "    #print(pred_keypoints)\n",
        "    assert pred_keypoints.shape == (num_instances, len(JOINT_NAMES), 3), (pred_keypoints.shape, (num_instances, len(JOINT_NAMES), 3))\n",
        "    box_deltas = []\n",
        "    keypoint_deltas = []\n",
        "    for instance_idx in range(num_instances):\n",
        "      track = frame_tracks[instance_idx]\n",
        "      filtered_boxes = filtered_boxes_by_track[track]\n",
        "\n",
        "      track_box = filtered_boxes[frame_id]\n",
        "\n",
        "      #print('before:', pred_boxes[instance_idx, :])\n",
        "      box_delta = pred_boxes[instance_idx, :] - track_box\n",
        "      box_deltas.append(box_delta)\n",
        "      pred_boxes[instance_idx, :] = track_box\n",
        "      #print('after:', pred_boxes[instance_idx, :])\n",
        "\n",
        "      # filtered_vis_kp_positions_by_joint_track\n",
        "      for i_joint in range(len(JOINT_NAMES)):\n",
        "        joint_track = track * len(JOINT_NAMES) + i_joint\n",
        "        filtered_vis_kp_positions = filtered_vis_kp_positions_by_joint_track[joint_track]\n",
        "        track_positions = filtered_vis_kp_positions[frame_id]\n",
        "        \n",
        "        #print('before:', pred_keypoints[instance_idx][i_joint])\n",
        "        keypoint_delta = pred_keypoints[instance_idx][i_joint][:2] - track_positions\n",
        "        keypoint_deltas.append(keypoint_delta)\n",
        "        pred_keypoints[instance_idx][i_joint][:2] = track_positions\n",
        "        #print('after:', pred_keypoints[instance_idx][i_joint])\n",
        "\n",
        "    mean_box_delta = np.abs(np.array(box_deltas)).mean()\n",
        "    #print('mean_box_delta:', mean_box_delta)\n",
        "    assert DO_SAVGOL_FILTER == bool(mean_box_delta), mean_box_delta\n",
        "    mean_box_deltas.append(mean_box_delta)\n",
        "    \n",
        "    mean_keypoint_delta = np.abs(np.array(keypoint_deltas)).mean()\n",
        "    #print('mean_keypoint_delta:', mean_keypoint_delta)\n",
        "    assert DO_SAVGOL_FILTER == bool(mean_keypoint_delta), mean_keypoint_delta\n",
        "    mean_keypoint_deltas.append(mean_keypoint_delta)\n",
        "\n",
        "    before = instances.pred_boxes.tensor.numpy()\n",
        "    instances.pred_boxes.tensor = torch.from_numpy(pred_boxes)\n",
        "    after = instances.pred_boxes.tensor.numpy()\n",
        "    assert DO_SAVGOL_FILTER == bool((before - after).mean())\n",
        "\n",
        "    before = instances.pred_keypoints.numpy()\n",
        "    instances.set('pred_keypoints', torch.from_numpy(pred_keypoints))\n",
        "    after = instances.pred_keypoints.numpy()\n",
        "    assert DO_SAVGOL_FILTER == bool((before - after).mean())\n",
        "\n",
        "    cur_boxes = pred_boxes\n",
        "\n",
        "  frame_tups.append((\n",
        "    frame, instances, cur_boxes, frame_tracks, frame_infected_tracks,\n",
        "    set(infected_tracks), consecutive_hit_joint_track_tups,\n",
        "    all_hit_joint_track_tups, joint_idx_by_joint_track, target_joint_vals\n",
        "  ))\n",
        "\n",
        "  infected_tracks |= frame_infected_tracks\n",
        "\n",
        "print('final infected_tracks:', infected_tracks)\n",
        "duration = time.time() - start_time\n",
        "print('duration:', duration)\n",
        "mean_mean_box_delta = np.mean(mean_box_deltas)\n",
        "print('mean_mean_box_delta:', mean_mean_box_delta)\n",
        "mean_mean_keypoint_delta = np.mean(mean_keypoint_deltas)\n",
        "print('mean_mean_keypoint_delta:', mean_mean_keypoint_delta)\n",
        "\n",
        "'''\n",
        "filter window length 61\n",
        "mean_mean_box_delta: 1.9673217390508577\n",
        "mean_mean_keypoint_delta: 2.335863792205027\n",
        "\n",
        "filter window length 121\n",
        "mean_mean_box_delta: 2.1252719775349673\n",
        "mean_mean_keypoint_delta: 2.6383550710672163\n",
        "\n",
        "filter window length 251\n",
        "mean_mean_box_delta: 2.552904603603061\n",
        "mean_mean_keypoint_delta: 2.911478190218577\n",
        "'''\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DxKMI58fAVX",
        "colab_type": "text"
      },
      "source": [
        "## Pad hits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-NTVSyJfDeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XXX broken if PAD_HITS is False (see below)\n",
        "PAD_HITS = False\n",
        "HIGHLIGHT_AFTER_MIN_CONSECUTIVE_ONLY = False\n",
        "PADDING_SECONDS = 1\n",
        "# TODO: rename\n",
        "r = 20\n",
        "PADDING_FRAMES = int(PADDING_SECONDS * frames_per_second)\n",
        "print('PADDING_FRAMES:', PADDING_FRAMES)\n",
        "\n",
        "all_highlight_joint_track_tups = [set() for _ in range(len(frame_tups))]\n",
        "all_boundaries_by_joint_track_tup = [dict() for _ in range(len(frame_tups))]\n",
        "\n",
        "for frame_idx, (\n",
        "    _,#frame,\n",
        "    _,#instances,\n",
        "    _,#cur_boxes,\n",
        "    _,#frame_tracks,\n",
        "    _,#frame_infected_tracks,\n",
        "    _,#infected_tracks,\n",
        "    consecutive_hit_joint_track_tups,\n",
        "    all_hit_joint_track_tups,\n",
        "    joint_idx_by_joint_track,\n",
        "    target_joint_vals\n",
        ") in tqdm(enumerate(frame_tups)):\n",
        "\n",
        "  past_frame_idx = max(frame_idx - PADDING_FRAMES, 0)\n",
        "  future_frame_idx = min(frame_idx + PADDING_FRAMES, len(frame_tups) - 1)\n",
        "  if consecutive_hit_joint_track_tups:\n",
        "    print('Padding hits at frame_idx:', frame_idx)\n",
        "    # TODO: do this more efficiently\n",
        "    for tup in consecutive_hit_joint_track_tups:\n",
        "      if PAD_HITS:\n",
        "        for _frame_idx in range(past_frame_idx, future_frame_idx + 1):\n",
        "          print('_frame_idx:', _frame_idx, 'tup:', tup)\n",
        "\n",
        "          infected_joint_track, hit_joint_track = tup\n",
        "          infected_joint_idx = joint_idx_by_joint_track[infected_joint_track]\n",
        "          hit_joint_idx = joint_idx_by_joint_track[hit_joint_track]\n",
        "\n",
        "\n",
        "          all_highlight_joint_track_tups[_frame_idx].add(sorted_tup)  \n",
        "\n",
        "\n",
        "          # XXX do the same thing if PAD_HITS is False\n",
        "\n",
        "\n",
        "          infected_joint_vals = target_joint_vals[infected_joint_idx]\n",
        "          hit_joint_vals = target_joint_vals[hit_joint_idx]\n",
        "          xA, yA = infected_joint_vals\n",
        "          xB, yB = hit_joint_vals\n",
        "          cA = int(min(xA, xB)) - r\n",
        "          cB = int(max(xA, xB)) + r\n",
        "          rA = int(min(yA, yB)) - r\n",
        "          rB = int(max(yA, yB)) + r\n",
        "\n",
        "          # more than one pair of joints hits between the same pair of tracks\n",
        "          all_boundaries_by_joint_track_tup[_frame_idx].setdefault(sorted_tup, [])\n",
        "          all_boundaries_by_joint_track_tup[_frame_idx][sorted_tup].append((rA, rB, cA, cB))\n",
        "      else:\n",
        "        if HIGHLIGHT_AFTER_MIN_CONSECUTIVE_ONLY:\n",
        "          highlight_joint_track_tups = consecutive_hit_joint_track_tups\n",
        "        else:\n",
        "          highlight_joint_track_tups = all_hit_joint_track_tups\n",
        "        all_highlight_joint_track_tups[frame_idx] = highlight_joint_track_tups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF9QS120dzKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WRITE_INF_OUTPUT = True\n",
        "inf_output_fname = f'{video_filename_reenc}-inf.mp4'\n",
        "print('inf_output_fname:', inf_output_fname)\n",
        "inf_output_file = cv2.VideoWriter(\n",
        "    filename=inf_output_fname,\n",
        "    fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    fps=float(frames_per_second),\n",
        "    frameSize=(width, height),\n",
        "    isColor=True,\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "print('Visualizing...')\n",
        "for frame_id, ((\n",
        "    frame,\n",
        "    instances,\n",
        "    cur_boxes,\n",
        "    frame_tracks,\n",
        "    frame_infected_tracks,\n",
        "    infected_tracks,\n",
        "    consecutive_hit_joint_track_tups,\n",
        "    all_hit_joint_track_tups,\n",
        "    joint_idx_by_joint_track,\n",
        "    target_joint_vals\n",
        "), highlight_joint_track_tups, boundaries_by_joint_track_tup) in tqdm(\n",
        "    enumerate(zip(frame_tups, all_highlight_joint_track_tups, all_boundaries_by_joint_track_tup))\n",
        "):\n",
        "  print('*' * 40)\n",
        "  print('frame_id:', frame_id)\n",
        "\n",
        "  vis_frame = visualize_predictions(frame, instances, hide_keypoints=True, hide_boxes=True)\n",
        "  for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "    cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "    cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "\n",
        "  b_channel, g_channel, r_channel = cv2.split(vis_frame)\n",
        "  alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 64\n",
        "\n",
        "  # highlight infected\n",
        "  #print('frame_infected_tracks:', frame_infected_tracks)\n",
        "  #print('infected_tracks:', infected_tracks)\n",
        "  highlight_tracks = frame_infected_tracks | infected_tracks\n",
        "  #print('highlight_tracks:', highlight_tracks)\n",
        "  highlight_boxes = []\n",
        "  for track in highlight_tracks:\n",
        "    track_id = np.where(frame_tracks == track)[0]\n",
        "    if not track_id.size:\n",
        "      # track is not visible in this frame\n",
        "      continue\n",
        "    track_id = track_id[0]\n",
        "    #print('track_id:', track_id)\n",
        "    box = cur_boxes[track_id]\n",
        "    #print('box:', box)\n",
        "    highlight_boxes.append(box)\n",
        "  #print('highlight_boxes:', highlight_boxes)\n",
        "  for box in highlight_boxes:\n",
        "    #print('box:', box, 'box[0]:', box[0])\n",
        "    xA = int(box[0])\n",
        "    yA = int(box[1])\n",
        "    xB = int(box[2])\n",
        "    yB = int(box[3])\n",
        "    alpha_channel[yA:yB, xA:xB] = 128\n",
        "    alpha_channel[yA, xA:xB] = 255\n",
        "    alpha_channel[yB, xA:xB] = 255\n",
        "    alpha_channel[yA:yB, xA] = 255\n",
        "    alpha_channel[yA:yB, xB] = 255\n",
        "\n",
        "    for channel in b_channel, g_channel, r_channel:\n",
        "      channel[yA, xA:xB] = 0\n",
        "      channel[yB, xA:xB] = 0\n",
        "      channel[yA:yB, xA] = 0\n",
        "      channel[yA:yB, xB] = 0\n",
        "\n",
        "  print('highlight_joint_track_tups:', highlight_joint_track_tups)\n",
        "  # will not be true if pad_hits is false\n",
        "  # assert set(highlight_joint_track_tups) == set(boundaries_by_joint_track_tup.keys()), (\n",
        "  #     highlight_joint_track_tups, boundaries_by_joint_track_tup.keys()\n",
        "  # )\n",
        "  for joint_track_tup in highlight_joint_track_tups:\n",
        "    infected_joint_track, hit_joint_track = joint_track_tup\n",
        "    print('joint_track_tup:', joint_track_tup)\n",
        "\n",
        "    infected_track = infected_joint_track // num_joints\n",
        "    hit_track = hit_joint_track // num_joints\n",
        "    sorted_tracks = sorted([infected_track, hit_track])\n",
        "    sorted_joint_tracks = sorted([infected_joint_track, hit_joint_track])\n",
        "    label = f'{sorted_tracks[0]}:{sorted_tracks[1]}'\n",
        "    APPEND_JOINT_TRACKS_TO_LABEL = False\n",
        "    if APPEND_JOINT_TRACKS_TO_LABEL:\n",
        "      label +=  f';({sorted_joint_tracks[0]}:{sorted_joint_tracks[1]})'\n",
        "    print('label:', label)\n",
        "    \n",
        "    def draw_boundary(boundary):\n",
        "      print('draw_boundary() boundary:', boundary)\n",
        "      rA, rB, cA, cB = boundary\n",
        "      alpha_channel[rA:rB, cA:cB] = 255\n",
        "      for channel in (b_channel, g_channel, r_channel):\n",
        "        channel[rA, cA:cB] = 255\n",
        "        channel[rB, cA:cB] = 255\n",
        "        channel[rA:rB, cA] = 255\n",
        "        channel[rA:rB, cB] = 255\n",
        "        cv2.putText(channel, label, (cA-2, rA-2), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0))\n",
        "        cv2.putText(channel, label, (cA-1, rA-1), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255))\n",
        "\n",
        "\n",
        "    boundaries = boundaries_by_joint_track_tup.get(joint_track_tup)\n",
        "    print('boundaries:', boundaries)\n",
        "    if boundaries:\n",
        "      for boundary in boundaries:\n",
        "        draw_boundary(boundary)\n",
        "    else:\n",
        "      try:\n",
        "        infected_joint_idx = joint_idx_by_joint_track[infected_joint_track]\n",
        "        hit_joint_idx = joint_idx_by_joint_track[hit_joint_track]\n",
        "      except KeyError as exc:\n",
        "        print('exc:', exc)\n",
        "        # A joint in the recent past or future that doesn't exist in the current\n",
        "        # frame was involved in a hit\n",
        "        continue\n",
        "\n",
        "      infected_joint_vals = target_joint_vals[infected_joint_idx]\n",
        "      hit_joint_vals = target_joint_vals[hit_joint_idx]\n",
        "      xA, yA = infected_joint_vals\n",
        "      xB, yB = hit_joint_vals\n",
        "      cA = int(min(xA, xB)) - r\n",
        "      cB = int(max(xA, xB)) + r\n",
        "      rA = int(min(yA, yB)) - r\n",
        "      rB = int(max(yA, yB)) + r\n",
        "      boundary = (rA, rB, cA, cB)\n",
        "      draw_boundary(boundary)\n",
        "    \n",
        "  img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
        "\n",
        "  # TODO: move this to infection tracking cell\n",
        "  new_infected_tracks = frame_infected_tracks - infected_tracks\n",
        "  if new_infected_tracks:\n",
        "    print('new_infected_tracks:', new_infected_tracks)\n",
        "  \n",
        "  # TODO: resolve naming inconsistency wrt word \"all\"\n",
        "  # (elsewhere refers to frames, here refers to consecutive vs. not/all)\n",
        "  if highlight_joint_track_tups or all_hit_joint_track_tups:\n",
        "    print('frame_id:', frame_id, 'hit')\n",
        "    cv2_imshow(img_BGRA)\n",
        "\n",
        "\n",
        "  def bgra_2_bgr(bgra):\n",
        "    # faster (?) but inverted alpha -> brightness\n",
        "    b, g, r, a = cv2.split(bgra)\n",
        "    bgr = cv2.merge((b, g, r))\n",
        "    a = 255 - np.stack([a, a, a], axis=-1)\n",
        "    bgr = cv2.addWeighted(np.float32(bgr) / 255, .5, np.float32(a) / 255, .5, 0)\n",
        "    bgr = (bgr * 255).astype(np.uint8)\n",
        "    '''\n",
        "    # slower but matches cv2_imshow\n",
        "    rgba = cv2.cvtColor(bgra, cv2.COLOR_BGRA2RGBA)\n",
        "    rgb = rgba2rgb(rgba)\n",
        "    rgb = (rgb * 255).astype(np.uint8)\n",
        "    bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "    '''\n",
        "    return bgr\n",
        "\n",
        "  if WRITE_INF_OUTPUT:\n",
        "    img_BGR = bgra_2_bgr(img_BGRA)\n",
        "    inf_output_file.write(img_BGR)\n",
        "\n",
        "duration = time.time() - start_time\n",
        "print('duration:', duration)\n",
        "\n",
        "inf_output_file.release()\n",
        "\n",
        "if WRITE_INF_OUTPUT:\n",
        "  with open(inf_output_fname, 'rb') as f:\n",
        "    print('wrote', len(f.read()), 'bytes to', inf_output_fname)\n",
        "  ! ls -alh $inf_output_fname\n",
        "  # this produces \"MessageError: TypeError: Failed to fetch\"\n",
        "  #files.download(inf_output_fname)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGJoVm7x5Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  if WRITE_INF_OUTPUT:\n",
        "    ! ls -alh $inf_output_fname\n",
        "    files.download(inf_output_fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRKsYoVJVCF1",
        "colab_type": "text"
      },
      "source": [
        "Limitations / Future Work\n",
        "- 2D results in false positives (due to lack of depth information) and false negatives (due to occlusion)\n",
        "  - Lack of depth can be mitigated with depth estimation: https://roxanneluo.github.io/Consistent-Video-Depth-Estimation/\n",
        "  - Occlusion can be mitigated by using mutiple cameras: https://arxiv.org/pdf/2003.03972v2.pdf\n",
        "- Only keypoints are considered, not semantic segmentation masks, which may contain more information about whether contact ocurred\n",
        "- Doesn't track individuals between videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fYd1w-Exbhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('frames_per_second:', frames_per_second)\n",
        "print('num_frames:', num_frames)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}