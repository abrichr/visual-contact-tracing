{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subject-reid-prototype.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wMQH5icYjcnS",
        "TXCE9sYBDtwx",
        "TreVxMwREpyM",
        "ayLZEgmPle5K",
        "7MHxIdKWs_zf",
        "JapoJjRE8pWE",
        "x4qsb_fA9PVC",
        "PYs-mLZu81OD",
        "kWZnFx8QwMsC",
        "jBTa1sU2b0Hc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce2d7854282a41fa9d4f0c272e66f757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b9872e6d763942719804f03d537aa3ea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a52b71f6740f477ea35bb3f2acf3ffbc",
              "IPY_MODEL_1612d8410dd44959ab90bd7e76d82931"
            ]
          }
        },
        "b9872e6d763942719804f03d537aa3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a52b71f6740f477ea35bb3f2acf3ffbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d67b049929b04496b11bae4c39ff3bd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3fc90b8e6a0043b3b65933708daa480d"
          }
        },
        "1612d8410dd44959ab90bd7e76d82931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d247ce56bf04658a50001f295ec0495",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 251/? [00:45&lt;00:00,  5.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4875f4fc5ea94394bad15c1b6d9bfedc"
          }
        },
        "d67b049929b04496b11bae4c39ff3bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3fc90b8e6a0043b3b65933708daa480d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d247ce56bf04658a50001f295ec0495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4875f4fc5ea94394bad15c1b6d9bfedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3671c2dfb20545bb90c61b02d1be88c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35483cbf44994185819c8386df30ee31",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_94066ca0d5ab49998035eda4c3443544",
              "IPY_MODEL_7881136584d64d48a8cf101a5163e1fe"
            ]
          }
        },
        "35483cbf44994185819c8386df30ee31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94066ca0d5ab49998035eda4c3443544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_911a5cf0b31e487da4679d95fa65234b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12c19ef5055446c1aea78d0210950cec"
          }
        },
        "7881136584d64d48a8cf101a5163e1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e019fd342b564e84a8d8f946c8279f2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 251/? [00:48&lt;00:00,  5.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73bda247a81446ab98da739f4014db52"
          }
        },
        "911a5cf0b31e487da4679d95fa65234b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12c19ef5055446c1aea78d0210950cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e019fd342b564e84a8d8f946c8279f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73bda247a81446ab98da739f4014db52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9190a898af314a9296681bda0c4848aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e696f1b7075f48fa987a44a580fa341f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97c7eed807b74f2c95f264d36b2b0c96",
              "IPY_MODEL_3371079979d14f59ad4593162ff3c349"
            ]
          }
        },
        "e696f1b7075f48fa987a44a580fa341f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97c7eed807b74f2c95f264d36b2b0c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_efa04cbdfce94bfca2de2a352a5e93bd",
            "_dom_classes": [],
            "description": " 10%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 52,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_68cffd549e4e4bca876302f6663bac28"
          }
        },
        "3371079979d14f59ad4593162ff3c349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00e87fd4247b4ebda35b9726a897daea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 51/500 [00:09&lt;01:21,  5.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d09219578f64f5d883a7c723bc89075"
          }
        },
        "efa04cbdfce94bfca2de2a352a5e93bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "68cffd549e4e4bca876302f6663bac28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00e87fd4247b4ebda35b9726a897daea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d09219578f64f5d883a7c723bc89075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee45e578a7454f28a08c5b191ab60c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2fd2d5265bf43ccad43d4d704efd1ac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0cac64e5cf1749b1bbad14b411fab3dd",
              "IPY_MODEL_40d9f544c6a24096af5201e9f098efb8"
            ]
          }
        },
        "a2fd2d5265bf43ccad43d4d704efd1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cac64e5cf1749b1bbad14b411fab3dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7fbe18041aeb4ea796ae940db16fcec3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83b4a837630a4dec9f5a4da938a6ca57"
          }
        },
        "40d9f544c6a24096af5201e9f098efb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0bf25f113aa45ceb3f8501479a940b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 251/? [00:28&lt;00:00, 31.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e290c1842d414092b31883e6349ae60e"
          }
        },
        "7fbe18041aeb4ea796ae940db16fcec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83b4a837630a4dec9f5a4da938a6ca57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0bf25f113aa45ceb3f8501479a940b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e290c1842d414092b31883e6349ae60e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea054a8469344b40bb3a2c26859ac235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b5163be30ad24a41991801789d99303e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a95fc894144c4386bc7bb8e70312ac0e",
              "IPY_MODEL_dbd4c0521e81418eb24ae2375ec1154b"
            ]
          }
        },
        "b5163be30ad24a41991801789d99303e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a95fc894144c4386bc7bb8e70312ac0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74943ba99a1d4ee3afa863a9b936204f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_069c73792e364ac5a3ecc55f57a58fd0"
          }
        },
        "dbd4c0521e81418eb24ae2375ec1154b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ebb080fae4c422995fb2cdd5fa30374",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 251/? [00:28&lt;00:00, 33.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36286fcdaaf44a96bf9d136451ff134e"
          }
        },
        "74943ba99a1d4ee3afa863a9b936204f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "069c73792e364ac5a3ecc55f57a58fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ebb080fae4c422995fb2cdd5fa30374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36286fcdaaf44a96bf9d136451ff134e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrichr/visual-contact-tracing/blob/master/subject_reid_prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gzBsTR51N3i",
        "colab_type": "text"
      },
      "source": [
        "# Subject Re-ID Prototype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_IboeLJTFDx",
        "colab_type": "text"
      },
      "source": [
        "### Re-ID Information \n",
        "\n",
        "### Workflow\n",
        "1. Import video data\n",
        "2. Use Detectron to get bounding box predictions and frames\n",
        "3. [Not Implemented] Run NMS to consolidate similar bounding boxes\n",
        "4. Run object tracking using simple OpenCV-based centroid tracker\n",
        "5. [Not Implemented] Filter out irrelevant tracklets\n",
        "6. Save bounding boxes to dataset on disk\n",
        "---\n",
        "7. Run subject re-identification\n",
        "8. Match query and gallery results between 2 videos using greedy bipartite matching\n",
        "9. Produce demo video with color-coded IDs\n",
        "\n",
        "### Notes\n",
        "- Currently runs re-ID successfully for two videos (one query video, one gallery video).\n",
        "- Skip steps 1-6 if you already have detection and track data.\n",
        "- Ignore code sections labelled [NOT USED] - these are experiments or additional code (other tracking methods, for example) that are not necessary to the base re-ID functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__tdamJF1Tu2",
        "colab_type": "text"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-8vQLir2U3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1116cfc-68dc-43f9-d87d-ca8338957a0a"
      },
      "source": [
        "# INSTALLATIONS\n",
        "\n",
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.6 torchvision==0.7 -f https://download.pytorch.org/whl/cu101/torch_stable.html\n",
        "!pip install cython pyyaml==5.1 cython_bbox\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
        "!pip install pretrainedmodels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Requirement already up-to-date: torch==1.6 in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already up-to-date: torchvision==0.7 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7) (7.0.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n",
            "Collecting pyyaml==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 8.3MB/s \n",
            "\u001b[?25hCollecting cython_bbox\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/b9/fc7d60e8c3b29cc0ff24a3bb3c4b7457e10b7610fbb2893741b623487b34/cython_bbox-0.1.3.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml, cython-bbox\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp36-cp36m-linux_x86_64.whl size=44074 sha256=072fa631e28e4ac572318b60ee9e956e289001a0c6f8237f00125655164907e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "  Building wheel for cython-bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cython-bbox: filename=cython_bbox-0.1.3-cp36-cp36m-linux_x86_64.whl size=57171 sha256=409bb18f17fb5f55539b4b7b213320618a5d1f61abc4bfee4e52b2abc7c0ff90\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/31/b5/9246d5988e79ef89dc28b894835d2f305e23c1e5f4f80278ee\n",
            "Successfully built pyyaml cython-bbox\n",
            "Installing collected packages: pyyaml, cython-bbox\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed cython-bbox-0.1.3 pyyaml-5.1\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-brle0itz\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-brle0itz\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (0.29.21)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266458 sha256=1e182a10d951feaf912429f5e8aaae9d79c5a55b65626a518f0a267ce28f415b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9j1fswlc/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.1\n",
            "    Uninstalling pycocotools-2.0.1:\n",
            "      Successfully uninstalled pycocotools-2.0.1\n",
            "Successfully installed pycocotools-2.0\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
            "Collecting detectron2\n",
            "\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/detectron2-0.2.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.3.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Collecting mock\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.41.1)\n",
            "Collecting fvcore>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/81/ee/3709513b414c2b525e4f03c45215579f93f41dba797f0e5fe539e6bc92b7/fvcore-0.1.1.post20200716.tar.gz\n",
            "Collecting pycocotools>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/82/bcaf4d21d7027fe5165b88e3aef1910a36ed02c3e99d3385d1322ea0ba29/pycocotools-2.0.1.tar.gz\n",
            "Collecting Pillow>=7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/bf/92385b4262178ca22b34f82e0e09c2922eb351fe39f3cc7b8ba9ea555b41/Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (49.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.17.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.31.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2) (5.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.1->detectron2) (0.29.21)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot->detectron2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Building wheels for collected packages: fvcore, pycocotools\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.1.post20200716-cp36-none-any.whl size=42325 sha256=4a209ec278c77119b460376a986ac928b9b58207854e3faf133187e4565dd0a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/99/f4/42a6bef61c07b3d78dfe6d7ebff259444c4526504cf72378d7\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.1-cp36-cp36m-linux_x86_64.whl size=266500 sha256=f91db611360f15c689908b2d13becc5c7b4a078e9f80ff1c1696ab2776dd0d8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/19/08/49b25f258ead1f861c9ab2fc41f73636f2928859adbb0e9797\n",
            "Successfully built fvcore pycocotools\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: yacs, mock, portalocker, Pillow, fvcore, pycocotools, detectron2\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: pycocotools 2.0\n",
            "    Uninstalling pycocotools-2.0:\n",
            "      Successfully uninstalled pycocotools-2.0\n",
            "Successfully installed Pillow-7.2.0 detectron2-0.2.1+cu101 fvcore-0.1.1.post20200716 mock-4.0.2 portalocker-2.0.0 pycocotools-2.0.1 yacs-0.1.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.7.0+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=df8f0794a18ddc12cf5944a0680be2ccf4fc628c95338915b5c499383244580c\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obKo7-Jh1R5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTS\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from pprint import pprint, pformat\n",
        "import random\n",
        "import statistics\n",
        "import requests\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch, torchvision\n",
        "from scipy.spatial import distance as dist\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils import visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "\n",
        "import imageio\n",
        "from skimage.color import rgba2rgb\n",
        "from cython_bbox import bbox_overlaps\n",
        "\n",
        "from google.colab import drive, files\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHL2yjV1a-p",
        "colab_type": "text"
      },
      "source": [
        "### Upload Input Video Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8zm5gHS1da0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "17c821e4-e19e-40c6-e402-88af313b2f78"
      },
      "source": [
        "# get input video files by uploading or from google drive\n",
        "# if uploading videos, upload at least two videos - uploading these may take a while\n",
        "\n",
        "input_video_files = []\n",
        "UPLOAD_FILES = False\n",
        "\n",
        "if UPLOAD_FILES:\n",
        "  uploaded = files.upload()\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn]))\n",
        "    )\n",
        "\n",
        "  for video_filename in list(uploaded.keys()):\n",
        "    print('video_filename:', video_filename)\n",
        "    input_video_files.append(video_filename)\n",
        "  \n",
        "  sample_video_dir = \"/content/\"\n",
        "else:\n",
        "  # mount google drive\n",
        "  drive.mount('/content/drive')\n",
        "  \n",
        "  # change this directory to gdrive directory with input videos\n",
        "  sample_video_dir = \"/content/drive/My Drive/reid-sample-videos\"\n",
        "  for video in os.listdir(sample_video_dir):\n",
        "    input_video_files.append(os.path.join(sample_video_dir, video))\n",
        "\n",
        "# view all input videos\n",
        "for i, video in enumerate(input_video_files):\n",
        "  print(i, video)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "0 /content/drive/My Drive/reid-sample-videos/54bfc8cfb1847b64a397902fb0067b6d4abaf250_2020-05-26-21-35-31.mp4\n",
            "1 /content/drive/My Drive/reid-sample-videos/a5a6fb92d838ff6a19b570d471b209a295bf7f2c_2020-05-26-21-35-26.mp4\n",
            "2 /content/drive/My Drive/reid-sample-videos/c38e16fd0f532f597fb3c63dfc3ae19e6ce1170e_2020-05-26-21-36-05.mp4\n",
            "3 /content/drive/My Drive/reid-sample-videos/cashier_20200526T155130-0500--20200526T155330-0500.mp4\n",
            "4 /content/drive/My Drive/reid-sample-videos/POS Station_20200526T155000-0500--20200526T155159-0500.mp4\n",
            "5 /content/drive/My Drive/reid-sample-videos/Store Front_20200526T155006-0500--20200526T155205-0500.mp4\n",
            "6 /content/drive/My Drive/reid-sample-videos/Store Back_20200526T154925-0500--20200526T155124-0500.mp4\n",
            "7 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000002.mp4\n",
            "8 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000101.mp4\n",
            "9 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000101_final.mp4\n",
            "10 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000002_final.mp4\n",
            "11 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000101_final_detectron_only.mp4\n",
            "12 /content/drive/My Drive/reid-sample-videos/VIRAT_S_000002_final_detectron_only.mp4\n",
            "13 /content/drive/My Drive/reid-sample-videos/a5a6fb92d838ff6a19b570d471b209a295bf7f2c_2020-05-26-21-35-26_output.mp4\n",
            "14 /content/drive/My Drive/reid-sample-videos/c38e16fd0f532f597fb3c63dfc3ae19e6ce1170e_2020-05-26-21-36-05_output.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGLnQsXjguqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select two input videos to use for re-ID using video index in input_video_files list\n",
        "\n",
        "# CHANGE THESE VALUES TO CHANGE INPUT VIDEOS\n",
        "QUERY_VIDEO_IDX = 9\n",
        "GALLERY_VIDEO_IDX = 10\n",
        "\n",
        "curr_query_video = input_video_files[QUERY_VIDEO_IDX]\n",
        "curr_gallery_video = input_video_files[GALLERY_VIDEO_IDX]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJZxpnbY1ey9",
        "colab_type": "text"
      },
      "source": [
        "### Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IB3ABHkt2zp",
        "colab_type": "text"
      },
      "source": [
        "#### Detectron Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZOPeS7Ht8na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d9da1fe-0db7-49a4-8bf5-d903afbe2f34"
      },
      "source": [
        "# get detectron configs, default model, metadata\n",
        "SCORE_THRESHOLD = 0.9\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = SCORE_THRESHOLD \n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
        "metadata = MetadataCatalog.get(\n",
        "  cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
        ")\n",
        "cpu_device = torch.device('cpu')\n",
        "instance_mode = ColorMode.IMAGE\n",
        "\n",
        "# initialize detectron predictor\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_final_a6e10b.pkl: 237MB [00:23, 10.2MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyJMoIFo1ln8",
        "colab_type": "text"
      },
      "source": [
        "#### Multi Object Tracking (Centroid Tracking)\n",
        "\n",
        "Code from https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxXAx8JK1kyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CentroidTrackerModified():\n",
        "\tdef __init__(self, maxDisappeared=50):\n",
        "\t\t# initialize the next unique object ID along with two ordered\n",
        "\t\t# dictionaries used to keep track of mapping a given object\n",
        "\t\t# ID to its centroid and number of consecutive frames it has\n",
        "\t\t# been marked as \"disappeared\", respectively\n",
        "\t\tself.nextObjectID = 0\n",
        "\t\tself.objects = OrderedDict()\n",
        "\t\tself.objects_to_last_bbox = OrderedDict()\n",
        "\t\tself.disappeared = OrderedDict()\n",
        "\n",
        "\t\t# store the number of maximum consecutive frames a given\n",
        "\t\t# object is allowed to be marked as \"disappeared\" until we\n",
        "\t\t# need to deregister the object from tracking\n",
        "\t\tself.maxDisappeared = maxDisappeared\n",
        "\n",
        "\tdef register(self, centroid, bbox):\n",
        "\t\t# when registering an object we use the next available object\n",
        "\t\t# ID to store the centroid\n",
        "\t\tself.objects[self.nextObjectID] = centroid\n",
        "\t\tself.disappeared[self.nextObjectID] = 0\n",
        "\t\tself.objects_to_last_bbox[self.nextObjectID] = bbox\n",
        "\t\tself.nextObjectID += 1\n",
        "\n",
        "\tdef deregister(self, objectID):\n",
        "\t\t# to deregister an object ID we delete the object ID from\n",
        "\t\t# both of our respective dictionaries\n",
        "\t\tdel self.objects[objectID]\n",
        "\t\tdel self.disappeared[objectID]\n",
        "\t\tdel self.objects_to_last_bbox[objectID]\n",
        "\n",
        "\tdef update(self, rects):\n",
        "\t\t# check to see if the list of input bounding box rectangles is empty\n",
        "\t\tif len(rects) == 0:\n",
        "\t\t\t# loop over any existing tracked objects and mark them as disappeared\n",
        "\t\t\tfor objectID in list(self.disappeared.keys()):\n",
        "\t\t\t\tself.disappeared[objectID] += 1\n",
        "\n",
        "\t\t\t\t# if we have reached a maximum number of consecutive\n",
        "\t\t\t\t# frames where a given object has been marked as\n",
        "\t\t\t\t# missing, deregister it\n",
        "\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n",
        "\t\t\t\t\tself.deregister(objectID)\n",
        "\n",
        "\t\t\t# return early as there are no centroids or tracking info\n",
        "\t\t\t# to update\n",
        "\t\t\treturn (self.objects, self.objects_to_last_bbox)\n",
        "\n",
        "\t\t# initialize an array of input centroids for the current frame\n",
        "\t\tinputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "\n",
        "\t\t# loop over the bounding box rectangles\n",
        "\t\tfor (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
        "\t\t\t# use the bounding box coordinates to derive the centroid\n",
        "\t\t\tcX = int((startX + endX) / 2.0)\n",
        "\t\t\tcY = int((startY + endY) / 2.0)\n",
        "\t\t\tinputCentroids[i] = (cX, cY)\n",
        "\n",
        "\t\t# if we are currently not tracking any objects take the input\n",
        "\t\t# centroids and register each of them\n",
        "\t\tif len(self.objects) == 0:\n",
        "\t\t\t# for i in range(0, len(inputCentroids)):\n",
        "\t\t\t# \tself.register(inputCentroids[i])\n",
        "\n",
        "\t\t\tfor inputCentroid, bbox in zip(inputCentroids, rects):\n",
        "\t\t\t\tself.register(inputCentroid, bbox)\n",
        "\n",
        "\t\t# otherwise, are are currently tracking objects so we need to\n",
        "\t\t# try to match the input centroids to existing object\n",
        "\t\t# centroids\n",
        "\t\telse:\n",
        "\t\t\t# grab the set of object IDs and corresponding centroids\n",
        "\t\t\tobjectIDs = list(self.objects.keys())\n",
        "\t\t\tobjectCentroids = list(self.objects.values())\n",
        "\n",
        "\t\t\t# compute the distance between each pair of object\n",
        "\t\t\t# centroids and input centroids, respectively -- our\n",
        "\t\t\t# goal will be to match an input centroid to an existing\n",
        "\t\t\t# object centroid\n",
        "\t\t\tD = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
        "\n",
        "\t\t\t# in order to perform this matching we must (1) find the\n",
        "\t\t\t# smallest value in each row and then (2) sort the row\n",
        "\t\t\t# indexes based on their minimum values so that the row\n",
        "\t\t\t# with the smallest value as at the *front* of the index\n",
        "\t\t\t# list\n",
        "\t\t\trows = D.min(axis=1).argsort()\n",
        "\n",
        "\t\t\t# next, we perform a similar process on the columns by\n",
        "\t\t\t# finding the smallest value in each column and then\n",
        "\t\t\t# sorting using the previously computed row index list\n",
        "\t\t\tcols = D.argmin(axis=1)[rows]\n",
        "\n",
        "\t\t\t# in order to determine if we need to update, register,\n",
        "\t\t\t# or deregister an object we need to keep track of which\n",
        "\t\t\t# of the rows and column indexes we have already examined\n",
        "\t\t\tusedRows = set()\n",
        "\t\t\tusedCols = set()\n",
        "\n",
        "\t\t\t# loop over the combination of the (row, column) index\n",
        "\t\t\t# tuples\n",
        "\t\t\tfor (row, col) in zip(rows, cols):\n",
        "\t\t\t\t# if we have already examined either the row or\n",
        "\t\t\t\t# column value before, ignore it\n",
        "\t\t\t\t# val\n",
        "\t\t\t\tif row in usedRows or col in usedCols:\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t# otherwise, grab the object ID for the current row,\n",
        "\t\t\t\t# set its new centroid, and reset the disappeared counter\n",
        "\t\t\t\tobjectID = objectIDs[row]\n",
        "\t\t\t\tself.objects[objectID] = inputCentroids[col]\n",
        "\t\t\t\tself.disappeared[objectID] = 0\n",
        "\t\t\t\tself.objects_to_last_bbox[objectID] = rects[col]\n",
        "\n",
        "\t\t\t\t# indicate that we have examined each of the row and\n",
        "\t\t\t\t# column indexes, respectively\n",
        "\n",
        "\t\t\t\tusedRows.add(row)\n",
        "\t\t\t\tusedCols.add(col)\n",
        "\n",
        "\t\t\t# compute both the row and column index we have NOT yet examined\n",
        "\n",
        "\t\t\tunusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "\t\t\tunusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "\n",
        "\t\t\t# in the event that the number of object centroids is\n",
        "\t\t\t# equal or greater than the number of input centroids\n",
        "\t\t\t# we need to check and see if some of these objects have\n",
        "\t\t\t# potentially disappeared\n",
        "\t\t\tif D.shape[0] >= D.shape[1]:\n",
        "\t\t\t\t# loop over the unused row indexes\n",
        "\t\t\t\tfor row in unusedRows:\n",
        "\t\t\t\t\t# grab the object ID for the corresponding row\n",
        "\t\t\t\t\t# index and increment the disappeared counter\n",
        "\t\t\t\t\tobjectID = objectIDs[row]\n",
        "\t\t\t\t\tself.disappeared[objectID] += 1\n",
        "\n",
        "\t\t\t\t\t# check to see if the number of consecutive\n",
        "\t\t\t\t\t# frames the object has been marked \"disappeared\"\n",
        "\t\t\t\t\t# for warrants deregistering the object\n",
        "\t\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n",
        "\t\t\t\t\t\tself.deregister(objectID)\n",
        "\n",
        "\t\t\t# otherwise, if the number of input centroids is greater\n",
        "\t\t\t# than the number of existing object centroids we need to\n",
        "\t\t\t# register each new input centroid as a trackable object\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor col in unusedCols:\n",
        "\t\t\t\t\tself.register(inputCentroids[col], rects[col])\n",
        "\n",
        "\t\t# return the set of trackable objects\n",
        "\t\treturn (self.objects, self.objects_to_last_bbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKxYTLaDWN-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_track_information(video_input, max_frames=1000, stride=1, GET_VIDEO_OUTPUT=False):\n",
        "  \"\"\"\n",
        "  Returns track information after running keypoint detection and simple centroid tracking.\n",
        "\n",
        "  video_input -- full absolute path of input video\n",
        "  max_frames -- upper limit of frames to process\n",
        "  stride -- run detection and tracking at every x # of frames\n",
        "  GET_VIDEO_OUTPUT -- set true if video output is required to analyze effectiveness of detectron and tracking\n",
        "  \"\"\"\n",
        "\n",
        "  # configure input video\n",
        "  video_filename = video_input[:-4]\n",
        "  vs = cv2.VideoCapture(video_input)\n",
        "  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  frames_per_second = vs.get(cv2.CAP_PROP_FPS)\n",
        "  num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  print(video_input)\n",
        "  print(num_frames)\n",
        "\n",
        "  # initialize centroid tracker and settings\n",
        "  ct = CentroidTrackerModified()\n",
        "  frame_counter = 0\n",
        "  num_frames_to_output = max_frames\n",
        "  pbar = tqdm(total = num_frames_to_output)\n",
        "\n",
        "  if GET_VIDEO_OUTPUT:\n",
        "    output_fname = f'{vid}-output.mp4'\n",
        "    output_file = cv2.VideoWriter(\n",
        "      filename=output_fname,\n",
        "      fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "      fps=float(frames_per_second),\n",
        "      frameSize=(width, height),\n",
        "      isColor=True,\n",
        "    )\n",
        "\n",
        "  # video data needed for re-id matching\n",
        "  object_id_to_images = {}\n",
        "  object_id_to_bbox = {}\n",
        "  frame_to_bbox = {}\n",
        "\n",
        "  while True:\n",
        "    frame = vs.read()[1]\n",
        "\n",
        "    if frame is None or frame_counter > num_frames_to_output:\n",
        "      break\n",
        "\n",
        "    # store images only every few frames\n",
        "    if frame_counter % stride == 0:\n",
        "      outputs = predictor(frame)\n",
        "      predictions = outputs[\"instances\"].to(cpu_device)\n",
        "      predictions.remove(\"pred_keypoints\")\n",
        "      tensors = predictions.pred_boxes.tensor.numpy()\n",
        "\n",
        "      rects = []\n",
        "      for idx, tensor in enumerate(tensors):\n",
        "        box = [int(v) for v in tensor]\n",
        "        rects.append(box)\n",
        "\n",
        "      # update centroid tracker using bounding boxes\n",
        "      (object_to_centroids, object_to_bboxes) = ct.update(rects)\n",
        "\n",
        "      # draw over objects with information, loop over tracked objects\n",
        "      for (objectID, centroid) in object_to_centroids.items():\n",
        "        # save track information\n",
        "        (startX, startY, endX, endY) = box = object_to_bboxes[objectID]\n",
        "        imCrop = frame[startY:endY, startX:endX]\n",
        "\n",
        "        if GET_VIDEO_OUTPUT:\n",
        "          text = \"ID {}\".format(objectID)\n",
        "          cv2.rectangle(frame, (startX, startY), \n",
        "                        (endX, endY), (0, 255, 0), 1)\n",
        "          cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "        # save object tracks and other information\n",
        "        if objectID in object_id_to_images:\n",
        "          object_id_to_images[objectID].append(imCrop)\n",
        "        else:\n",
        "          object_id_to_images[objectID] = [imCrop]\n",
        "\n",
        "        if objectID in object_id_to_bbox:\n",
        "          object_id_to_bbox[objectID].append(box)\n",
        "        else:\n",
        "          object_id_to_bbox[objectID] = [box]\n",
        "\n",
        "        if frame_counter in frame_to_bbox:\n",
        "          frame_to_bbox[frame_counter].append(box)\n",
        "        else:\n",
        "          frame_to_bbox[frame_counter] = [tuple(box)]\n",
        "\n",
        "    if GET_VIDEO_OUTPUT:\n",
        "      output_file.write(frame)\n",
        "\n",
        "    frame_counter += 1\n",
        "    pbar.update(1)\n",
        "\n",
        "  vs.release()\n",
        "\n",
        "  if GET_VIDEO_OUTPUT:\n",
        "    output_file.release()\n",
        "    files.download(output_fname)\n",
        "\n",
        "  # print statistics about people detected\n",
        "  print(\"number of people detected: \", len(object_id_to_images))\n",
        "  for (person_id, images) in object_id_to_images.items():\n",
        "    print(\"person ID: \", person_id, \", \", \"number of images: \", len(images))\n",
        "\n",
        "  return object_id_to_images, object_id_to_bbox, frame_to_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdXxorSnIUCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557,
          "referenced_widgets": [
            "ce2d7854282a41fa9d4f0c272e66f757",
            "b9872e6d763942719804f03d537aa3ea",
            "a52b71f6740f477ea35bb3f2acf3ffbc",
            "1612d8410dd44959ab90bd7e76d82931",
            "d67b049929b04496b11bae4c39ff3bd8",
            "3fc90b8e6a0043b3b65933708daa480d",
            "2d247ce56bf04658a50001f295ec0495",
            "4875f4fc5ea94394bad15c1b6d9bfedc",
            "3671c2dfb20545bb90c61b02d1be88c5",
            "35483cbf44994185819c8386df30ee31",
            "94066ca0d5ab49998035eda4c3443544",
            "7881136584d64d48a8cf101a5163e1fe",
            "911a5cf0b31e487da4679d95fa65234b",
            "12c19ef5055446c1aea78d0210950cec",
            "e019fd342b564e84a8d8f946c8279f2c",
            "73bda247a81446ab98da739f4014db52"
          ]
        },
        "outputId": "78acec0f-2a34-4bc7-b788-c4d458892507"
      },
      "source": [
        "# important config values\n",
        "MAX_FRAMES = 250            # maximum number of frames to process\n",
        "STRIDE = 1                  # sample frames every # of frames\n",
        "GET_VIDEO_OUTPUT = False    # if video output of detection and tracking is needed\n",
        "\n",
        "# run detection and tracking for both videos\n",
        "id_to_images_1, id_to_bbox_1, frame_to_bbox_1 = get_track_information(curr_query_video, \n",
        "                                                           max_frames=MAX_FRAMES,\n",
        "                                                           stride=STRIDE, GET_VIDEO_OUTPUT=GET_VIDEO_OUTPUT)\n",
        "id_to_images_2, id_to_bbox_2, frame_to_bbox_2 = get_track_information(curr_gallery_video, \n",
        "                                                           max_frames=MAX_FRAMES, \n",
        "                                                           stride=STRIDE, GET_VIDEO_OUTPUT=GET_VIDEO_OUTPUT)\n",
        "\n",
        "# consolidate track information across videos\n",
        "id_to_images_list = [id_to_images_1, id_to_images_2]\n",
        "id_to_bbox_list = [id_to_bbox_1, id_to_bbox_2]\n",
        "frame_to_bbox_list = [frame_to_bbox_1, frame_to_bbox_2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/reid-sample-videos/VIRAT_S_000002_final.mp4\n",
            "1833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce2d7854282a41fa9d4f0c272e66f757",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/detectron2/layers/wrappers.py:226: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  return x.nonzero().unbind(1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of people detected:  11\n",
            "person ID:  0 ,  number of images:  154\n",
            "person ID:  1 ,  number of images:  251\n",
            "person ID:  2 ,  number of images:  139\n",
            "person ID:  3 ,  number of images:  237\n",
            "person ID:  4 ,  number of images:  251\n",
            "person ID:  5 ,  number of images:  249\n",
            "person ID:  6 ,  number of images:  213\n",
            "person ID:  7 ,  number of images:  96\n",
            "person ID:  8 ,  number of images:  87\n",
            "person ID:  9 ,  number of images:  39\n",
            "person ID:  10 ,  number of images:  39\n",
            "/content/drive/My Drive/reid-sample-videos/VIRAT_S_000101_final.mp4\n",
            "8873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3671c2dfb20545bb90c61b02d1be88c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "number of people detected:  6\n",
            "person ID:  0 ,  number of images:  251\n",
            "person ID:  1 ,  number of images:  251\n",
            "person ID:  2 ,  number of images:  251\n",
            "person ID:  3 ,  number of images:  251\n",
            "person ID:  4 ,  number of images:  251\n",
            "person ID:  5 ,  number of images:  251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMQH5icYjcnS",
        "colab_type": "text"
      },
      "source": [
        "### New Detection and Tracking [NOT USED]\n",
        "NMS + tracks + filtering tracks (from Visual Contact Tracing notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNmVlGT9DoON",
        "colab_type": "text"
      },
      "source": [
        "#### Setup and Get Frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpAP6Im8b9zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# detectron pre-trained model setup\n",
        "\n",
        "cpu_device = torch.device('cpu')\n",
        "cfg_name = \"keypoint_rcnn_R_50_FPN_3x.yaml\"\n",
        "DEFAULT_CONFIG = f'COCO-Keypoints/{cfg_name}'\n",
        "DEFAULT_CONF_THRESH = 0.1\n",
        "DEFAULT_OPTS = ['MODEL.WEIGHTS', model_zoo.get_checkpoint_url(f'COCO-Keypoints/{cfg_name}')]\n",
        "# https://github.com/facebookresearch/detectron2/blob/04958b93e1232935e126c2fd9e6ccd3f57c3a8f3/detectron2/utils/visualizer.py#L32\n",
        "\n",
        "def setup_cfg(config=DEFAULT_CONFIG, opts=DEFAULT_OPTS, conf_thresh=DEFAULT_CONF_THRESH):\n",
        "  \"\"\"Set up config values for Detectron pretrained model\"\"\"\n",
        "\n",
        "  # load config from file and arguments\n",
        "  cfg = get_cfg()\n",
        "  if not torch.cuda.device_count():\n",
        "    print('Running on CPU')\n",
        "    cfg.MODEL.DEVICE = 'cpu'\n",
        "  cfg.merge_from_file(model_zoo.get_config_file(config))\n",
        "  cfg.merge_from_list(opts)\n",
        "\n",
        "  # set score_threshold for builtin models\n",
        "  cfg.MODEL.RETINANET.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = conf_thresh\n",
        "  cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = conf_thresh\n",
        "  cfg.freeze()\n",
        "  return cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI2_azg79gWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_video(video_input, max_frames, stride=1):\n",
        "  \"\"\"Returns frames and detectron predictions at a specified stride value.\"\"\"\n",
        "\n",
        "  video_filename = video_input.split(\".\")[-1]\n",
        "  vs = cv2.VideoCapture(video_input)\n",
        "  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  frames_per_second = vs.get(cv2.CAP_PROP_FPS)\n",
        "  num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  # set max frames to process to limit time\n",
        "  if max_frames:\n",
        "    max_frames = max_frames\n",
        "  else:\n",
        "    max_frames = num_frames\n",
        "\n",
        "  frame_counter = 0\n",
        "  pbar = tqdm(total = max_frames)\n",
        "\n",
        "  frames = []\n",
        "  all_predictions = []\n",
        "\n",
        "  while True:\n",
        "    frame = vs.read()[1]\n",
        "\n",
        "    if frame is None or frame_counter >= max_frames:\n",
        "      break\n",
        "    \n",
        "    if frame_counter % stride == 0:\n",
        "      frames.append(frame)\n",
        "      predictions = predictor(frame)\n",
        "      all_predictions.append(predictions)\n",
        "\n",
        "    frame_counter += 1\n",
        "    pbar.update(1)\n",
        "\n",
        "  vs.release()\n",
        "\n",
        "  return frames, all_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWC_S8nDJHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up config and predictor for Detectron\n",
        "cfg = setup_cfg()\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# choose input video files from list\n",
        "input_video_file_1 = input_video_files[12]\n",
        "input_video_file_2 = input_video_files[13]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_m_6FAWrTcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9190a898af314a9296681bda0c4848aa",
            "e696f1b7075f48fa987a44a580fa341f",
            "97c7eed807b74f2c95f264d36b2b0c96",
            "3371079979d14f59ad4593162ff3c349",
            "efa04cbdfce94bfca2de2a352a5e93bd",
            "68cffd549e4e4bca876302f6663bac28",
            "00e87fd4247b4ebda35b9726a897daea",
            "4d09219578f64f5d883a7c723bc89075"
          ]
        },
        "outputId": "2a66ecc3-3021-49ee-a66d-b5317d0f67a1"
      },
      "source": [
        "# get frames and predictions\n",
        "frames_1, all_predictions_1 = process_video(input_video_file_1, max_frames=500, stride=1)\n",
        "frames_2, all_predictions_2 = process_video(input_video_file_2, max_frames=500, stride=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9190a898af314a9296681bda0c4848aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXCE9sYBDtwx",
        "colab_type": "text"
      },
      "source": [
        "#### Non-Maximum Suppression (not currently used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i15ydnFGfUHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxxtwMvDDfb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# indicies = nms.nms.nms_rboxes(rrects, scores)\n",
        "# boxes in all_flowed_boxes are (xA, xB, yA, yB, prob)\n",
        "# we need boxes as (x, y, w, h) and scores as (prob)\n",
        "\n",
        "from nms import nms\n",
        "\n",
        "# copied from nms, their version prints to stdout verbosely\n",
        "def rboxes(rrects, scores, nms_algorithm=nms.default_algorithm, **kwargs):\n",
        "    \"\"\"\n",
        "    Non Maxima Suppression for rotated rectangles\n",
        "\n",
        "    :param rrects: a list of polygons, each described by ((cx, cy), (w,h), deg)\n",
        "    :type rrects: list\n",
        "    :param scores: a list of the scores associated with the rects\n",
        "    :type scores: list\n",
        "    :param nms_algorithm: the NMS comparison function to use, kwargs will be passed to this function. Defaults to :func:`nms.malisiewicz.NMS`\n",
        "    :type nms_algorithm: function\n",
        "    :returns: an array of indicies of the best rrects\n",
        "    \"\"\"\n",
        "\n",
        "    # convert the rrects to polys\n",
        "    polys = []\n",
        "    for rrect in rrects:\n",
        "        r = cv2.boxPoints(rrect)\n",
        "        polys.append(r)\n",
        "    return nms.polygons(polys, scores, nms_algorithm, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohBGSO7zEHi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_nms(all_predictions):\n",
        "  all_augmented_boxes = []\n",
        "  all_indices = []\n",
        "\n",
        "  for frame_idx, predictions in enumerate(tqdm(all_predictions)):\n",
        "    #'pred_boxes', 'scores', 'pred_classes', 'pred_keypoints'\n",
        "    instances = predictions['instances'].to(cpu_device)\n",
        "    boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "    boxes_scores = np.asarray(instances.scores)\n",
        "    boxes_scores = np.expand_dims(boxes_scores, axis=1)\n",
        "    boxes = np.append(boxes, boxes_scores, axis=1)\n",
        "\n",
        "    rrects = []\n",
        "    scores = []\n",
        "    combined_boxes = boxes\n",
        "\n",
        "    for box in combined_boxes:\n",
        "      xA, yA, xB, yB, prob = box\n",
        "      assert xA < xB, (xA, xB)\n",
        "      assert yA < yB, (yA, yB)\n",
        "      xC = (xB - xA) / 2\n",
        "      yC = (yB - yA) / 2\n",
        "      w = xB - xA\n",
        "      h = yB - yA\n",
        "      # ((cx, cy), (w,h), deg)\n",
        "      rrects.append(((xA, yA), (w, h), 0))\n",
        "      scores.append(prob)\n",
        "    \n",
        "    indices = rboxes(rrects, scores)\n",
        "    assert len(indices) <= len(rrects), (len(indices), len(rrects))\n",
        "\n",
        "    augmented_boxes = [box[:4] for i, box in enumerate(combined_boxes) if i in indices]\n",
        "    all_augmented_boxes.append(augmented_boxes)\n",
        "    all_indices.append(indices)\n",
        "\n",
        "  return all_augmented_boxes, all_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuCzpnuWnyBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_augmented_boxes_1, all_indices_1 = run_nms(all_predictions_1)\n",
        "all_augmented_boxes_2, all_indices_2 = run_nms(all_predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TreVxMwREpyM",
        "colab_type": "text"
      },
      "source": [
        "#### Tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFpLGIZKjq_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRACKING VARIABLES\n",
        "\n",
        "DEFAULT_HIDE_KEYPOINTS = True\n",
        "DEFAULT_HIDE_BOXES = False\n",
        "SHOW_START_END_FRAMES = 0\n",
        "T = 51\n",
        "MAX_COST = .99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxvv0AfXAhIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRACKING HELPER FUNCTIONS\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106\n",
        "def compute_pairwise_iou(a, b):\n",
        "  \"\"\"\n",
        "  a, b (np.ndarray) of shape Nx4 and Mx4.\n",
        "  The output is NxM, for each combination of boxes.\n",
        "  \"\"\"\n",
        "\n",
        "  C = 1 - bbox_overlaps(\n",
        "    np.ascontiguousarray(a, dtype=np.float64),\n",
        "    np.ascontiguousarray(b, dtype=np.float64),\n",
        "  )\n",
        "  return C\n",
        "\n",
        "\n",
        "def compute_distance_matrix(prev_boxes, cur_boxes):\n",
        "  return compute_pairwise_iou(prev_boxes, cur_boxes)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C, prev_tracks, next_track_id):\n",
        "    \"\"\"\n",
        "    Computes the bipartite matching between the rows and columns, given the\n",
        "    cost matrix, C.\n",
        "    \"\"\"\n",
        "    C = C.copy()  # to avoid affecting the original matrix\n",
        "    prev_ids = []\n",
        "    cur_ids = []\n",
        "    min_costs = []\n",
        "    while (C == np.inf).sum() != C.size:\n",
        "      # Find the lowest cost element\n",
        "      min_idx = C.argmin()\n",
        "      i, j = np.unravel_index(min_idx, C.shape)\n",
        "      min_val = C[i][j]\n",
        "      #print('min_idx:', min_idx, 'min_val:', min_val, 'i:', i, 'j:', j)\n",
        "\n",
        "      # Add to results\n",
        "      min_cost = C.min()\n",
        "      min_costs.append(min_cost)\n",
        "      prev_ids.append(i)\n",
        "      if (not MAX_COST) or (min_cost < MAX_COST):\n",
        "        cur_ids.append(j)\n",
        "      else:\n",
        "        cur_ids.append(-1)\n",
        "\n",
        "      # Remove from cost matrix\n",
        "      track = prev_tracks[i]\n",
        "      track_idxs = [\n",
        "        idx for idx in range(len(prev_tracks))\n",
        "        if prev_tracks[idx] == track\n",
        "      ]\n",
        "\n",
        "      C[:, j] = np.inf\n",
        "      for track_idx in track_idxs:\n",
        "        C[track_idx, :] = np.inf\n",
        "      #num_removed_costs = (C == np.inf).sum()\n",
        "\n",
        "    mean_min_cost = np.mean(min_costs)\n",
        "    mean_min_cost_idx = np.argmin(min_costs)\n",
        "    mean_min_cost_track = cur_ids[mean_min_cost_idx]\n",
        "    if mean_min_cost_track == -1:\n",
        "      max_min_cost_track = next_track_id[0]\n",
        "    mean_min_costs.append((mean_min_cost, mean_min_cost_track))\n",
        "\n",
        "    max_min_cost = np.max(min_costs)\n",
        "    max_min_cost_idx = np.argmax(min_costs)\n",
        "    max_min_cost_track = cur_ids[max_min_cost_idx]\n",
        "    if max_min_cost_track == -1:\n",
        "      max_min_cost_track = next_track_id[0]\n",
        "    max_min_costs.append((max_min_cost, max_min_cost_track))\n",
        "\n",
        "    return prev_ids, cur_ids\n",
        "\n",
        "\n",
        "def compute_matches(prev_boxes, cur_boxes, prev_tracks, next_track_id):\n",
        "  assert len(prev_boxes) == len(prev_tracks)\n",
        "  matches = -np.ones((len(cur_boxes)), dtype=np.int32)\n",
        "  if not prev_boxes.size:\n",
        "    return matches\n",
        "  \n",
        "  C = compute_distance_matrix(prev_boxes, cur_boxes)\n",
        "  prev_inds, next_inds = bipartite_matching_greedy(C, prev_tracks, next_track_id)\n",
        "  assert(len(prev_inds) == len(next_inds))\n",
        "  for i in range(len(prev_inds)):\n",
        "    matches[next_inds[i]] = prev_inds[i]\n",
        "  return matches\n",
        "\n",
        "\n",
        "def get_frame_tracks(matches, prev_tracks, next_track_id):\n",
        "  frame_tracks = []\n",
        "  for i, m in enumerate(matches):\n",
        "    if m == -1 or m >= len(prev_tracks): \n",
        "      frame_tracks.append(next_track_id[0])\n",
        "      next_track_id[0] += 1\n",
        "    else:\n",
        "      frame_tracks.append(prev_tracks[m])\n",
        "  return frame_tracks\n",
        "\n",
        "\n",
        "def visualize_predictions(frame, instances, hide_keypoints=DEFAULT_HIDE_KEYPOINTS, hide_boxes=DEFAULT_HIDE_BOXES):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    if hide_keypoints or hide_boxes:\n",
        "      instances = instances.to(cpu_device)\n",
        "    if hide_keypoints:\n",
        "      instances.remove('pred_keypoints')\n",
        "    if hide_boxes:\n",
        "      instances.pred_boxes.tensor = torch.Tensor()\n",
        "    \n",
        "    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/video_visualizer.py#L53\n",
        "    # https://github.com/facebookresearch/detectron2/blob/b6fe828a2f3b2133f24cb93c1d0d74cb59c6a15d/detectron2/utils/video_visualizer.py#L53\n",
        "    vis_frame = video_visualizer.draw_instance_predictions(frame, instances)\n",
        "    # Converts Matplotlib RGB format to OpenCV BGR format\n",
        "    vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    return vis_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0inWWC2At7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_tracks(frames, all_predictions, all_augmented_boxes=None, USE_AUGMENTED=False):\n",
        "  # compute tracks, inspired by:\n",
        "  # https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L272\n",
        "  all_prev_boxes = []\n",
        "  video_tracks = []\n",
        "  next_track_id = [0]\n",
        "  for frame_id, (frame, predictions) in enumerate(tqdm(zip(frames, all_predictions), total=len(frames))):\n",
        "    instances = predictions['instances'].to(cpu_device)  \n",
        "    if USE_AUGMENTED:\n",
        "      cur_boxes = all_augmented_boxes[frame_id]\n",
        "    else:\n",
        "      cur_boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "\n",
        "    prev_boxes = np.vstack(all_prev_boxes) if all_prev_boxes else np.array([])\n",
        "    all_prev_tracks = video_tracks[\n",
        "      max(0, frame_id - len(all_prev_boxes)) :\n",
        "      max(0, frame_id)\n",
        "    ]\n",
        "\n",
        "    prev_tracks = np.hstack(all_prev_tracks) if all_prev_tracks else np.array([])\n",
        "\n",
        "    if len(cur_boxes) == 0:\n",
        "      matches = []\n",
        "    else:\n",
        "      matches = compute_matches(prev_boxes, cur_boxes, prev_tracks, next_track_id)\n",
        "    # matches[i] contains the index of the box in the previous frames\n",
        "    # corresponding to the box with index i in the current frame\n",
        "\n",
        "    frame_tracks = get_frame_tracks(matches, prev_tracks, next_track_id)\n",
        "    assert len(np.unique(frame_tracks)) == len(frame_tracks), (\n",
        "        len(np.unique(frame_tracks)), len(frame_tracks)\n",
        "    )\n",
        "    video_tracks.append(np.array(frame_tracks))\n",
        "    all_prev_boxes.append(cur_boxes)\n",
        "    # if len(all_prev_boxes) > T:\n",
        "    #   all_prev_boxes = all_prev_boxes[1:]\n",
        "\n",
        "    SHOW_FRAME_ON_NEW_TRACK = False\n",
        "    HAS_NEW_MATCH = any([match == -1 for match in matches])\n",
        "    if frame_id < SHOW_START_END_FRAMES or frame_id >= len(frames) - SHOW_START_END_FRAMES or (\n",
        "        SHOW_FRAME_ON_NEW_TRACK and HAS_NEW_MATCH\n",
        "    ):\n",
        "      print('Visualizing frame_id:', frame_id)\n",
        "      if HAS_NEW_MATCH:\n",
        "        print('New match:')\n",
        "      vis_frame = visualize_predictions(frame, instances)\n",
        "      for box, frame_track in zip(cur_boxes, frame_tracks):\n",
        "        cv2.putText(vis_frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "        cv2.putText(vis_frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "\n",
        "      cv2_imshow(vis_frame)\n",
        "\n",
        "  # TODO: filter out large position jumps that immediately return after one frame\n",
        "  mean_min_costs.sort(key=lambda tup: tup[0])\n",
        "  max_min_costs.sort(key=lambda tup: tup[0])\n",
        "\n",
        "  return video_tracks, all_prev_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEbIGv4zocGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_min_costs = []\n",
        "max_min_costs = []\n",
        "video_tracks_1, all_prev_boxes_1 = compute_tracks(frames_1, all_predictions_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohhfItY5rZE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_min_costs = []\n",
        "max_min_costs = []\n",
        "video_tracks_2, all_prev_boxes_2 = compute_tracks(frames_2, all_predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayLZEgmPle5K",
        "colab_type": "text"
      },
      "source": [
        "#### Filtering Tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH-TYLu31zp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for tracking\n",
        "def setup_filtering(video_tracks, all_predictions, all_augmented_boxes=None, USE_AUGMENTED=False):\n",
        "  num_tracks = 1 + max(max(frame_tracks) for frame_tracks in video_tracks)\n",
        "  print('num_tracks:', num_tracks)\n",
        "\n",
        "  boxes_by_track = {}\n",
        "  for frame_idx, (predictions, frame_tracks) in enumerate(zip(all_predictions, video_tracks)):\n",
        "    instances = predictions['instances'].to(cpu_device)\n",
        "    if USE_AUGMENTED:\n",
        "      boxes = np.asarray(all_augmented_boxes[frame_idx])\n",
        "    else:\n",
        "      boxes = np.asarray(instances.pred_boxes.tensor)\n",
        "    \n",
        "    for box, track in zip(boxes, frame_tracks):\n",
        "      boxes_by_track.setdefault(track, [])\n",
        "      boxes_by_track[track].append(box)\n",
        "\n",
        "    # fill in with nans for missing tracks\n",
        "    all_tracks = set(range(num_tracks))\n",
        "    missing_tracks = all_tracks - set(frame_tracks)\n",
        "\n",
        "    for missing_track in missing_tracks:\n",
        "      boxes_by_track.setdefault(missing_track, [])\n",
        "      boxes_by_track[missing_track].append([np.nan for _ in range(4)])\n",
        "\n",
        "  filtered_boxes_by_track = {}\n",
        "  for track, boxes in boxes_by_track.items():\n",
        "    filtered_boxes = np.array(boxes)\n",
        "    \n",
        "    assert len(filtered_boxes) == len(boxes), (\n",
        "        len(filtered_boxes), len(boxes)\n",
        "    )\n",
        "    filtered_boxes_by_track[track] = filtered_boxes\n",
        "\n",
        "  return filtered_boxes_by_track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLg11M9NpJLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_boxes_by_track_1 = setup_filtering(video_tracks_1, all_predictions_1)\n",
        "filtered_boxes_by_track_2 = setup_filtering(video_tracks_2, all_predictions_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZENbvhty6Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# at least this ratio must be missing on both sides in order to remove\n",
        "REFERENCE_MIN_EMPTY_RATIO = 1\n",
        "FILTER_ENDS = True\n",
        "FILTER_TUPS = [\n",
        "  # (a, b)\n",
        "  # a: reference_radius\n",
        "  # b: max_consecutive_nonempty_to_remove\n",
        "  (3, 5),\n",
        "  (10, 20),\n",
        "  (5, 2),\n",
        "]\n",
        "\n",
        "def filter_tracks(frames, filtered_boxes_by_track):\n",
        "  # filtering\n",
        "  for filter_tup in FILTER_TUPS:\n",
        "    print('filter_tup:', filter_tup)\n",
        "    reference_radius, max_consecutive_nonempty_to_remove = filter_tup\n",
        "\n",
        "    # for tracking\n",
        "    for track, filtered_boxes in filtered_boxes_by_track.items():\n",
        "      N = len(filtered_boxes)\n",
        "      for i in range(N):\n",
        "        start_ref_start_idx = max(0, i - reference_radius)\n",
        "        start_ref_end_idx = i - 1\n",
        "        mid_start_idx = i\n",
        "        mid_end_idx = i + max_consecutive_nonempty_to_remove - 1\n",
        "        end_ref_start_idx = mid_end_idx + 1\n",
        "        end_ref_end_idx = min(end_ref_start_idx + reference_radius - 1, N - 1)\n",
        "\n",
        "        start_ref_vals = filtered_boxes[start_ref_start_idx : start_ref_end_idx + 1]\n",
        "        mid_vals = filtered_boxes[mid_start_idx : mid_end_idx + 1]\n",
        "        end_ref_vals = filtered_boxes[end_ref_start_idx : end_ref_end_idx + 1]\n",
        "        \n",
        "        start_ref_mask = np.all(np.isnan(start_ref_vals), axis=1)\n",
        "        mid_mask = np.all(np.isnan(mid_vals), axis=1)\n",
        "        end_ref_mask = np.all(np.isnan(end_ref_vals), axis=1)\n",
        "\n",
        "        num_mid_nonempty = (~mid_mask).sum()\n",
        "        mid_nonempty_idxs = [i + j for j in np.where(~mid_mask)[0]]\n",
        "\n",
        "        start_ref_empty_ratio = start_ref_mask.mean()\n",
        "        end_ref_empty_ratio = end_ref_mask.mean()\n",
        "\n",
        "        start_condition = start_ref_empty_ratio >= REFERENCE_MIN_EMPTY_RATIO\n",
        "        end_condition = end_ref_empty_ratio >= REFERENCE_MIN_EMPTY_RATIO\n",
        "        if FILTER_ENDS:\n",
        "          start_condition = start_condition or np.isnan(start_ref_empty_ratio)\n",
        "          end_condition = end_condition or np.isnan(end_ref_empty_ratio)\n",
        "        mid_condition = num_mid_nonempty > 0\n",
        "\n",
        "        if start_condition and mid_condition and end_condition:\n",
        "          print('removing num_mid_nonempty:', num_mid_nonempty)\n",
        "          print('mid_nonempty_idxs:', mid_nonempty_idxs)\n",
        "          filtered_boxes[mid_start_idx : mid_end_idx + 1] = np.nan\n",
        "\n",
        "  # getting consumable results\n",
        "  object_id_to_images = {}\n",
        "  object_id_to_bbox = {}\n",
        "  frame_to_bbox = {}\n",
        "\n",
        "  for frame_number, frame in tqdm(enumerate(frames)):\n",
        "    for frame_track, boxes in filtered_boxes_by_track.items():\n",
        "      # get data\n",
        "      box = boxes[frame_number]\n",
        "\n",
        "      if not np.isnan(box).any():\n",
        "        box = [int(v) for v in box]\n",
        "        (startX, startY, endX, endY) = box\n",
        "        imCrop = frame[startY:endY, startX:endX]\n",
        "\n",
        "        # save data\n",
        "        if frame_track in object_id_to_images:\n",
        "          object_id_to_images[int(frame_track)].append(imCrop)\n",
        "        else:\n",
        "          object_id_to_images[int(frame_track)] = [imCrop]\n",
        "\n",
        "        if frame_track in object_id_to_bbox:\n",
        "          object_id_to_bbox[int(frame_track)].append(tuple(box))\n",
        "        else:\n",
        "          object_id_to_bbox[int(frame_track)] = [tuple(box)]\n",
        "        \n",
        "        if frame_number in frame_to_bbox:\n",
        "          frame_to_bbox[frame_number].append(box)\n",
        "        else:\n",
        "          frame_to_bbox[frame_number] = [tuple(box)]\n",
        "\n",
        "  return object_id_to_images, object_id_to_bbox, frame_to_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPXMKaVYpu9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "object_id_to_images_1, object_id_to_bbox_1, frame_to_bbox_1 = filter_tracks(frames_1, filtered_boxes_by_track_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2buZZNHIrNzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "object_id_to_images_2, object_id_to_bbox_2, frame_to_bbox_2 = filter_tracks(frames_2, filtered_boxes_by_track_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv-h3UIOs_bC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oiti_list = [object_id_to_images_1, object_id_to_images_2]\n",
        "oitb_list = [object_id_to_bbox_1, object_id_to_bbox_2]\n",
        "ftbb_list = [frame_to_bbox_1, frame_to_bbox_2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJwzROvv8re-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oiti_list_rev = [object_id_to_images_2, object_id_to_images_1]\n",
        "oitb_list_rev = [object_id_to_bbox_2, object_id_to_bbox_1]\n",
        "ftbb_list_rev = [frame_to_bbox_2, frame_to_bbox_1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MHxIdKWs_zf",
        "colab_type": "text"
      },
      "source": [
        "##### Random Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPo0eht77wcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output():  \n",
        "  PRINT_ON_FRAME = True\n",
        "\n",
        "  vs = cv2.VideoCapture(input_video_files[7])\n",
        "  width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "  out = cv2.VideoWriter('outpy.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 10, (width, height))\n",
        "\n",
        "  for frame_number in range(0, len(frames)):\n",
        "    frame = frames[frame_number]\n",
        "    # boxes = filtered_boxes_by_track[frame_number]\n",
        "    # frame_tracks = video_tracks[frame_number]\n",
        "    # print(len(boxes) == len(frame_tracks))\n",
        "\n",
        "    if frame_number % 3 == 0:\n",
        "      for track, boxes in filtered_boxes_by_track.items():\n",
        "        box = boxes[frame_number]\n",
        "        if not np.isnan(box).any():\n",
        "          box = [int(v) for v in box]\n",
        "          (startX, startY, endX, endY) = box\n",
        "\n",
        "          if PRINT_ON_FRAME:\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 1)\n",
        "            cv2.putText(frame, str(track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "            cv2.putText(frame, str(track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "  out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op_6BRCWFsKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OLD CODE\n",
        "def get_data_from_tracking(frames, video_tracks, all_prev_boxes):\n",
        "  PRINT_ON_FRAME = False\n",
        "\n",
        "  object_id_to_images = {}\n",
        "  object_id_to_bbox = {}\n",
        "  frame_to_bbox = {}\n",
        "\n",
        "  for frame_number in range(0, len(frames)):\n",
        "    frame = frames[frame_number]\n",
        "    boxes = all_prev_boxes[frame_number]\n",
        "    frame_tracks = video_tracks[frame_number]\n",
        "\n",
        "    # print(len(boxes) == len(frame_tracks))\n",
        "\n",
        "    for box, frame_track in zip(boxes, frame_tracks):\n",
        "      box = [int(v) for v in box]\n",
        "      (startX, startY, endX, endY) = box\n",
        "      imCrop = frame[startY:endY, startX:endX]\n",
        "\n",
        "      if PRINT_ON_FRAME:\n",
        "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 1)\n",
        "        cv2.putText(frame, str(frame_track), (int(box[0]-5), int(box[1]-5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0))\n",
        "        cv2.putText(frame, str(frame_track), (int(box[0]-4), int(box[1]-4)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255))\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "      # save data\n",
        "      if frame_track in object_id_to_images:\n",
        "        object_id_to_images[int(frame_track)].append(imCrop)\n",
        "      else:\n",
        "        object_id_to_images[int(frame_track)] = [imCrop]\n",
        "\n",
        "      if frame_track in object_id_to_bbox:\n",
        "        object_id_to_bbox[int(frame_track)].append(tuple(box))\n",
        "      else:\n",
        "        object_id_to_bbox[int(frame_track)] = [tuple(box)]\n",
        "      \n",
        "      if frame_number in frame_to_bbox:\n",
        "        frame_to_bbox[frame_number].append(box)\n",
        "      else:\n",
        "        frame_to_bbox[frame_number] = [tuple(box)]\n",
        "\n",
        "  return object_id_to_images, object_id_to_bbox, frame_to_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NjfGewYXQBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXWUSmG3TXvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frames_7, all_predictions_7 = process_video(input_video_files, 7, stride=1)\n",
        "# video_tracks_7, all_prev_boxes_7 = compute_tracks(frames_7, all_predictions_7)\n",
        "# object_id_to_images_7, object_id_to_bbox_7, frame_to_bbox_7 = get_data_from_tracking(frames_7, video_tracks_7, all_prev_boxes_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE5ejOWHloI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_ids = len(object_id_to_images_11)\n",
        "for idx in range(num_ids):\n",
        "  print(idx)\n",
        "  print(len(object_id_to_images_11[idx]))\n",
        "  cv2_imshow(object_id_to_images_11[idx][6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8LWWJQnsEzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "object_id_to_images_11.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXBzhbtTT-Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames_11, all_predictions_11 = process_video(input_video_files, 11, stride=2)\n",
        "video_tracks_11, all_prev_boxes_11 = compute_tracks(frames_11, all_predictions_11)\n",
        "object_id_to_images_11, object_id_to_bbox_11, frame_to_bbox_11 = get_data_from_tracking(frames_11, video_tracks_11, all_prev_boxes_11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIailn8WsLtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_ids = len(object_id_to_images_11)\n",
        "for idx in range(0, num_ids):\n",
        "  print(len(object_id_to_images_11[idx]))\n",
        "  cv2_imshow(object_id_to_images_11[idx][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMPyFdnsTuok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print statistics\n",
        "# print(\"# frames: \", len(frames))\n",
        "# print(\"# video tracks: \", len(video_tracks))\n",
        "# print(\"# boxes: \", len(all_prev_boxes))\n",
        "# print(\"# object_id_to_images: \", len(object_id_to_images))\n",
        "# print(\"# frame_to_bbox: \", len(frame_to_bbox))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JapoJjRE8pWE",
        "colab_type": "text"
      },
      "source": [
        "### Towards Realtime MOT [NOT USED]\n",
        "Experiment with using Realtime MOT library (https://github.com/Zhongdao/Towards-Realtime-MOT) for e2e object detection and tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4qsb_fA9PVC",
        "colab_type": "text"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWMTa3EO9AqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install pytest==3.8\n",
        "!pip install motmetrics\n",
        "!pip install lap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT837ZEp-mun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MOT_DIR = \"Towards-Realtime-MOT/\"\n",
        "MOT_DIR_FULL = \"/content/drive/My Drive/Towards-Realtime-MOT/\"\n",
        "%cd $MOT_DIR_FULL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYs-mLZu81OD",
        "colab_type": "text"
      },
      "source": [
        "#### Testing with default demo.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5sRE7X29L2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_input_video = \"../reid-sample-videos/VIRAT_S_000002_final.mp4\"\n",
        "path_to_output_root = \"../reid-output-videos/\"\n",
        "path_to_model_weights = \"jde.1088x608.uncertainty.pt\"\n",
        "path_to_demo_script = \"demo.py\"\n",
        "\n",
        "print(\"Input video: \", path_to_input_video)\n",
        "print(\"Output root: \", path_to_output_root)\n",
        "print(\"Model weights: \", path_to_model_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0WZpDeDCDb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python $path_to_demo_script --input-video $path_to_input_video --weights $path_to_model_weights --output-format video --output-root $path_to_output_root --conf-thres 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWZnFx8QwMsC",
        "colab_type": "text"
      },
      "source": [
        "#### Experimental: One Video at a Time\n",
        "Good for testing other approaches (ex. real time MOT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989V8oor0RVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# configure input video\n",
        "VID_NUMBER = 0\n",
        "\n",
        "video_filename = input_video_files[VID_NUMBER][:-4]\n",
        "video_input = input_video_files[VID_NUMBER]\n",
        "vs = cv2.VideoCapture(video_input)\n",
        "\n",
        "width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = vs.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "print(video_filename)\n",
        "\n",
        "# configure keypoint output video\n",
        "output_fname = f'{video_filename}-output.mp4'\n",
        "output_file = cv2.VideoWriter(\n",
        "  filename=output_fname,\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "  fps=float(frames_per_second),\n",
        "  frameSize=(width, height),\n",
        "  isColor=True,\n",
        ")\n",
        "\n",
        "num_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6StTRbO200S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUNNING KP DETECTION AND CENTROID TRACKING\n",
        "\n",
        "# initialize centroid tracker\n",
        "ct = CentroidTrackerModified()\n",
        "\n",
        "# settings\n",
        "GET_VIDEO_OUTPUT = False\n",
        "frame_counter = 0\n",
        "num_frames_to_output = 750\n",
        "padding = 5\n",
        "pbar = tqdm(total = num_frames_to_output)\n",
        "\n",
        "# to store cropped images\n",
        "object_id_to_images = {}\n",
        "\n",
        "# read frames\n",
        "while True:\n",
        "  frame = vs.read()[1]\n",
        "\n",
        "  if frame is None or frame_counter > num_frames_to_output:\n",
        "    break\n",
        "  \n",
        "  outputs = predictor(frame)\n",
        "  predictions = outputs[\"instances\"].to(\"cpu\")\n",
        "  predictions.remove(\"pred_keypoints\")\n",
        "  tensors = predictions.pred_boxes.tensor.numpy()\n",
        "\n",
        "  rects = []\n",
        "  cropped_images = []\n",
        "\n",
        "  for idx, tensor in enumerate(tensors):\n",
        "    box = [int(v) for v in tensor]\n",
        "    rects.append(box)\n",
        "\n",
        "    # print bbox output on frame\n",
        "    if GET_VIDEO_OUTPUT:\n",
        "      (startX, startY, endX, endY) = box\n",
        "      cv2.rectangle(frame, (startX, startY), \n",
        "                    (endX, endY), (0, 255, 0), 1)\n",
        "\n",
        "\n",
        "  # update centroid tracker using bounding boxes\n",
        "  (object_to_centroids, object_to_bboxes) = ct.update(rects)\n",
        "\n",
        "  # draw over objects with information, loop over tracked objects\n",
        "  for (objectID, centroid) in object_to_centroids.items():\n",
        "    # draw ID and centroids of object on output frame\n",
        "    if GET_VIDEO_OUTPUT:\n",
        "      text = \"ID {}\".format(objectID)\n",
        "      cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "      cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
        "\n",
        "    # store images only every 25 frames\n",
        "    if frame_counter % 25 == 0:\n",
        "      (startX, startY, endX, endY) = box = object_to_bboxes[objectID]\n",
        "      imCrop = frame[startY:endY, startX:endX]\n",
        "\n",
        "      if objectID in object_id_to_images:\n",
        "        object_id_to_images[objectID].append(imCrop)\n",
        "      else:\n",
        "        object_id_to_images[objectID] = [imCrop]\n",
        "\n",
        "  if GET_VIDEO_OUTPUT:\n",
        "    output_file.write(frame)\n",
        "\n",
        "  frame_counter += 1\n",
        "  pbar.update(1)\n",
        "\n",
        "# release input video and output video services\n",
        "vs.release()\n",
        "\n",
        "if GET_VIDEO_OUTPUT:\n",
        "  output_file.release()\n",
        "\n",
        "oiti_list.append(object_id_to_images)\n",
        "\n",
        "# statistics about processed video\n",
        "print(\"number of people detected: \", len(object_id_to_images))\n",
        "\n",
        "for (person_id, images) in object_id_to_images.items():\n",
        "  print(\"person ID: \", person_id, \", \", \"number of images: \", len(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FS9eJ9o2942",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write to dataset\n",
        "oiti_dataset = \"oiti-dataset-\" + str(VID_NUMBER)\n",
        "if not os.path.isdir(oiti_dataset):\n",
        "  os.mkdir(oiti_dataset)\n",
        "\n",
        "for idx, (key, val) in enumerate(object_id_to_images.items()):\n",
        "  os.mkdir(os.path.join(oiti_dataset, str(idx)))\n",
        "  for imgidx, img in enumerate(val):\n",
        "    # new_img = imutils.resize(img, width=200) # resize makes for a lot of issues\n",
        "    filename = \"img_\" + str(imgidx) + \".jpg\"\n",
        "    cv2.imwrite(os.path.join(oiti_dataset, str(idx), filename), img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvIpsykvAqtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPqr1G-oTror",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBTa1sU2b0Hc",
        "colab_type": "text"
      },
      "source": [
        "#### Save dataset for torchreid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oso7osNy4QPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run once for each new dataset\n",
        "dataset_dir = \"new_dataset\"\n",
        "if not os.path.isdir(dataset_dir):\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "SHOT = \"multi_shot\"\n",
        "if not os.path.isdir(os.path.join(dataset_dir, SHOT)):\n",
        "  os.mkdir(os.path.join(dataset_dir, SHOT))\n",
        "\n",
        "camera_number = input_video_files.index(video_input)\n",
        "\n",
        "def save_images_to_dataset(camera_num, object_id_to_images):\n",
        "  # create camera directory\n",
        "  camera_dir = \"camera_\" + str(camera_num)\n",
        "  os.mkdir(os.path.join(dataset_dir, SHOT, camera_dir))\n",
        "  os.mkdir(camera_dir)\n",
        "\n",
        "  num_digits_to_pad = len(str(len(object_id_to_images)))\n",
        "\n",
        "  for (person_id, images) in object_id_to_images.items():\n",
        "    # create person directory\n",
        "    person_dir = \"person_\" + str(person_id).zfill(num_digits_to_pad)\n",
        "    os.mkdir(os.path.join(dataset_dir, SHOT, camera_dir, person_dir))\n",
        "\n",
        "    pad = len(str(len(images)))\n",
        "    for idx, img in enumerate(images):\n",
        "      # save image to person directory\n",
        "      filename = \"img_\" + str(idx).zfill(pad) + \".png\"\n",
        "      cv2.imwrite(os.path.join(dataset_dir, SHOT, camera_dir, person_dir, filename), img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sx_lYe9md-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_images_to_dataset(camera_number, object_id_to_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYMmnjXDAlv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# repeat steps for all cameras in dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiFPk_UnDbtn",
        "colab_type": "text"
      },
      "source": [
        "## Subject Re-ID\n",
        "- Based on Person_reID_baseline_pytorch (prbp) library (https://github.com/layumi/Person_reID_baseline_pytorch)\n",
        "- Code modified to suit CTAI purposes => can be cloned from MLDSAI repository or taken from Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTlnmN8uFknh",
        "colab_type": "text"
      },
      "source": [
        "Library script breakdown:\n",
        "- prepare.py = prepare certain datasets (UNUSED)\n",
        "- train.py = training (UNUSED)\n",
        "- test.py = test only, inference + extracting features\n",
        "- evaluate.py = get results (Rank@1, Rank@5, Rank@10 and mAP), plot CMC curves\n",
        "- demo.py = retrieve results and display visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6QfgZA2DnaS",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYDYudytDqo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "37283526-f349-459f-dae3-3e1e45d15026"
      },
      "source": [
        "# set this to True if cloning from MLDSAI repository, False if from Google Drive\n",
        "CLONE_PRBP = True\n",
        "prbp_dir = \"Person_reID_baseline_pytorch\"\n",
        "\n",
        "\n",
        "# download model from google drive to colab\n",
        "# https://drive.google.com/file/d/1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE/view\n",
        "def download_file_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=$file_id\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=$confirm_text&id=$file_id\" -o $file_name\n",
        "  with open(file_name, 'rb') as f:\n",
        "    data = f.read()\n",
        "    print('downloaded', len(data), 'bytes')\n",
        "\n",
        "\n",
        "if CLONE_PRBP:\n",
        "  %cd \"/content\"\n",
        "\n",
        "  if os.path.isdir(prbp_dir):\n",
        "    prbp_dir = \"/content/\" + prbp_dir\n",
        "    %cd $prbp_dir\n",
        "  else:\n",
        "    # change credentials to access MLDSAI person re-ID repository\n",
        "    username = \"<your github username here>\"\n",
        "    password = \"<your github password here>\"\n",
        "    repo_url = f\"https://{username}:{password}@github.com/MLDSAI/Person_reID_baseline_pytorch.git\"\n",
        "    !git clone {repo_url}\n",
        "\n",
        "    prbp_dir = \"/content/\" + prbp_dir\n",
        "    %cd $prbp_dir\n",
        "\n",
        "    # download and unzip pretrained model\n",
        "    model_zip_name = \"model.zip\"\n",
        "    download_file_from_google_drive(\"1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE\", model_zip_name)\n",
        "    !unzip $model_zip_name -d \"model_folder\"\n",
        "    !mv \"model_folder/model\" .\n",
        "    !rm -rf \"model_folder\"\n",
        "else: \n",
        "  # use google drive\n",
        "  prbp_dir = \"/content/drive/My Drive/\" + prbp_dir\n",
        "  %cd $prbp_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Person_reID_baseline_pytorch'...\n",
            "remote: Enumerating objects: 4015, done.\u001b[K\n",
            "remote: Total 4015 (delta 0), reused 0 (delta 0), pack-reused 4015\u001b[K\n",
            "Receiving objects: 100% (4015/4015), 123.78 MiB | 5.84 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n",
            "/content/Person_reID_baseline_pytorch\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   1952      0 --:--:-- --:--:-- --:--:--  1952\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  329M    0  329M    0     0   120M      0 --:--:--  0:00:02 --:--:--  172M\n",
            "downloaded 345900662 bytes\n",
            "Archive:  model.zip\n",
            "   creating: model_folder/model/\n",
            "   creating: model_folder/model/ft_ResNet50/\n",
            "  inflating: model_folder/model/ft_ResNet50/train.jpg  \n",
            "  inflating: model_folder/model/ft_ResNet50/model.py  \n",
            "  inflating: model_folder/model/ft_ResNet50/_DS_Store  \n",
            "  inflating: model_folder/model/ft_ResNet50/opts.yaml  \n",
            "  inflating: model_folder/model/ft_ResNet50/train.py  \n",
            "  inflating: model_folder/model/ft_ResNet50/net_last.pth  \n",
            " extracting: model_folder/model/_gitkeep  \n",
            "  inflating: model_folder/model/_DS_Store  \n",
            " extracting: model_folder/model/.gitkeep  \n",
            "   creating: model_folder/model/fp16/\n",
            "  inflating: model_folder/model/fp16/train.jpg  \n",
            "  inflating: model_folder/model/fp16/model.py  \n",
            "  inflating: model_folder/model/fp16/opts.yaml  \n",
            "  inflating: model_folder/model/fp16/train.py  \n",
            "  inflating: model_folder/model/fp16/net_last.pth  \n",
            "   creating: model_folder/model/ft_net_dense/\n",
            "  inflating: model_folder/model/ft_net_dense/train.jpg  \n",
            "  inflating: model_folder/model/ft_net_dense/model.py  \n",
            "  inflating: model_folder/model/ft_net_dense/_DS_Store  \n",
            "  inflating: model_folder/model/ft_net_dense/opts.yaml  \n",
            "  inflating: model_folder/model/ft_net_dense/train.py  \n",
            "  inflating: model_folder/model/ft_net_dense/net_last.pth  \n",
            "   creating: model_folder/model/PCB/\n",
            "  inflating: model_folder/model/PCB/train.jpg  \n",
            "  inflating: model_folder/model/PCB/model.py  \n",
            "  inflating: model_folder/model/PCB/_DS_Store  \n",
            "  inflating: model_folder/model/PCB/opts.yaml  \n",
            "  inflating: model_folder/model/PCB/train.py  \n",
            "  inflating: model_folder/model/PCB/net_last.pth  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DPjbM6uJ5PC",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYIrx1AOKJMh",
        "colab_type": "text"
      },
      "source": [
        "#### Create or retrieve person re-id dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zvsbY8fJXvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_images_to_prbp_dataset(prbp_dir, query_path, gallery_path, query_images, gallery_images):\n",
        "  \"\"\"Creates and saves new dataset to disk for re-id library consumption.\n",
        "  \n",
        "  Structure of the new dataset is:\n",
        "  new_dataset\n",
        "      |___ query\n",
        "            |___ 001 (pid 1)\n",
        "            |___ 002 (pid 2)\n",
        "            |___ ...\n",
        "      |___ gallery\n",
        "            |___ 001 (pid 1)\n",
        "            |___ 002 (pid 2)\n",
        "            |___ ...\n",
        "\n",
        "  prbp_dir -- full path of the directory containing the prbp re-id library code\n",
        "  query_path -- full path of the query folder of the new dataset\n",
        "  gallery_path -- full path of the gallery folder of the new dataset\n",
        "  query_images -- dictionary of person IDs to images for query camera\n",
        "  gallery_images -- dictionary of person IDs to images for gallery_camera\n",
        "  \"\"\"\n",
        "  \n",
        "  # create query directory\n",
        "  for (person_id, images) in query_images.items():\n",
        "    # create person directory\n",
        "    person_dir = str(person_id).zfill(3)\n",
        "    os.mkdir(os.path.join(prbp_dir, query_path, person_dir))\n",
        "\n",
        "    # save images to person directory\n",
        "    for idx, img in enumerate(images):\n",
        "      filename = str(person_id).zfill(3) + \"_img_\" + str(idx).zfill(3) + \".jpg\"\n",
        "      cv2.imwrite(os.path.join(prbp_dir, query_path, person_dir, filename), img)\n",
        "\n",
        "  # create gallery directory\n",
        "  for (person_id, images) in gallery_images.items():\n",
        "    # create person directory\n",
        "    person_dir = str(person_id).zfill(3)\n",
        "    os.mkdir(os.path.join(prbp_dir, gallery_path, person_dir))\n",
        "\n",
        "    # save images to person directory\n",
        "    for idx, img in enumerate(images):\n",
        "      filename = str(person_id).zfill(3) + \"_img_\" + str(idx).zfill(3) + \".jpg\"\n",
        "      cv2.imwrite(os.path.join(prbp_dir, gallery_path, person_dir, filename), img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG036bwWJly0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(tracks_list, prbp_dir, dataset_name):\n",
        "  \"\"\"Driver function for creating and saving new dataset to disk for re-id consumption.\"\"\"\n",
        "\n",
        "  prbp_dataset_dir = os.path.join(\"/content\", prbp_dir, \"prbp_dataset\" + \"_\" + str(dataset_name))\n",
        "  query_path = os.path.join(prbp_dataset_dir, \"query\")\n",
        "  gallery_path = os.path.join(prbp_dataset_dir, \"gallery\")\n",
        "  \n",
        "  # create dataset if not created yet\n",
        "  if not os.path.isdir(prbp_dataset_dir):\n",
        "    os.mkdir(prbp_dataset_dir)\n",
        "    os.mkdir(query_path)\n",
        "    os.mkdir(gallery_path)\n",
        "\n",
        "    save_images_to_prbp_dataset(prbp_dir, query_path, gallery_path, tracks_list[0], tracks_list[1])\n",
        "  \n",
        "  return (prbp_dataset_dir, query_path, gallery_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q7GQTevXJ_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# True if creating a new dataset, False if retrieving an existing dataset\n",
        "CREATE_NEW_DATASET = True\n",
        "\n",
        "# if creating a new dataset: CHANGE THIS VARIABLE to create a new dataset (any name works)\n",
        "# if retrieving an existing dataset: CHANGE THIS VARIABLE to the name of the dataset you want to retrieve\n",
        "# ex. DATASET_DIR_NAME = \"demo_500\" for sample data in repo\n",
        "DATASET_DIR_NAME = f\"demo_{MAX_FRAMES}\"\n",
        "\n",
        "if CREATE_NEW_DATASET: # create new dataset\n",
        "  # save video information to files\n",
        "  id_to_bbox_file = f\"{DATASET_DIR_NAME}_id_to_bbox.npy\"\n",
        "  frame_to_bbox_file = f\"{DATASET_DIR_NAME}_frame_to_bbox.npy\"\n",
        "\n",
        "  with open(id_to_bbox_file, \"wb\") as f:\n",
        "    np.save(f, np.array(id_to_bbox_list))\n",
        "\n",
        "  with open(frame_to_bbox_file, \"wb\") as f:\n",
        "    np.save(f, np.array(frame_to_bbox_list))\n",
        "\n",
        "  # create and save dataset\n",
        "  (prbp_dataset_dir, query_path, gallery_path) = create_dataset(id_to_images_list, prbp_dir, DATASET_DIR_NAME)\n",
        "\n",
        "  # get test dataset name in the form of (prbp_dataset_[NEW_DATASET_NAME])\n",
        "  test_dataset = prbp_dataset_dir.split(\"/\")[-1]\n",
        "else: # retrieve existing dataset\n",
        "  prbp_dataset_dir = os.path.join(\"/content\", prbp_dir, \"prbp_dataset\" + \"_\" + str(DATASET_DIR_NAME))\n",
        "  test_dataset = \"prbp_dataset_\" + str(DATASET_DIR_NAME)\n",
        "  query_path = os.path.join(prbp_dataset_dir, \"query\")\n",
        "  gallery_path = os.path.join(prbp_dataset_dir, \"gallery\")\n",
        "\n",
        "  # get video information file names\n",
        "  id_to_bbox_file = f\"{DATASET_DIR_NAME}_id_to_bbox.npy\"\n",
        "  frame_to_bbox_file = f\"{DATASET_DIR_NAME}_frame_to_bbox.npy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S37kqu3AKZuL",
        "colab_type": "text"
      },
      "source": [
        "### Model + Test + Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quWtkmSrIufi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "998cd732-1728-4c0d-ea98-9152110a20d1"
      },
      "source": [
        "# conduct inference\n",
        "# this will take a while to complete for larger datasets\n",
        "# note: evaluation script call in test.py can be removed or commented out - rank and mAP scores will not be correct due to lack of ground truth\n",
        "\n",
        "!python test.py --gpu_ids 0 --name ft_ResNet50 --test_dir $test_dataset --batchsize 32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
            "test.py:48: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  config = yaml.load(stream)\n",
            "We use the scale: 1\n",
            "-----------test-----------\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 248MB/s]\n",
            "32\n",
            "64\n",
            "96\n",
            "128\n",
            "160\n",
            "192\n",
            "224\n",
            "256\n",
            "288\n",
            "320\n",
            "352\n",
            "384\n",
            "416\n",
            "448\n",
            "480\n",
            "512\n",
            "544\n",
            "576\n",
            "608\n",
            "640\n",
            "672\n",
            "704\n",
            "736\n",
            "768\n",
            "800\n",
            "832\n",
            "864\n",
            "896\n",
            "928\n",
            "960\n",
            "992\n",
            "1024\n",
            "1056\n",
            "1088\n",
            "1120\n",
            "1152\n",
            "1184\n",
            "1216\n",
            "1248\n",
            "1280\n",
            "1312\n",
            "1344\n",
            "1376\n",
            "1408\n",
            "1440\n",
            "1472\n",
            "1504\n",
            "1506\n",
            "32\n",
            "64\n",
            "96\n",
            "128\n",
            "160\n",
            "192\n",
            "224\n",
            "256\n",
            "288\n",
            "320\n",
            "352\n",
            "384\n",
            "416\n",
            "448\n",
            "480\n",
            "512\n",
            "544\n",
            "576\n",
            "608\n",
            "640\n",
            "672\n",
            "704\n",
            "736\n",
            "768\n",
            "800\n",
            "832\n",
            "864\n",
            "896\n",
            "928\n",
            "960\n",
            "992\n",
            "1024\n",
            "1056\n",
            "1088\n",
            "1120\n",
            "1152\n",
            "1184\n",
            "1216\n",
            "1248\n",
            "1280\n",
            "1312\n",
            "1344\n",
            "1376\n",
            "1408\n",
            "1440\n",
            "1472\n",
            "1504\n",
            "1536\n",
            "1568\n",
            "1600\n",
            "1632\n",
            "1664\n",
            "1696\n",
            "1728\n",
            "1755\n",
            "ft_ResNet50\n",
            "prbp_dataset_demo_250\n",
            "torch.Size([1755, 512])\n",
            "(1755,)\n",
            "Rank@1:0.068376 Rank@5:0.139601 Rank@10:0.182906 mAP:0.148018\n",
            "multi:  False\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhnCpXJm3oUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e7e80737-d803-4cc3-f91b-d9cd05cc4461"
      },
      "source": [
        "# EXAMPLE: get and output results for a single query image in test_dataset\n",
        "QUERY_NUMBER = 0\n",
        "!python demo.py --query_index $QUERY_NUMBER --test_dir $test_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prbp_dataset_demo_250/query/000/000_img_000.jpg\n",
            "Top 10 images are as follow:\n",
            "prbp_dataset_demo_250/gallery/004/004_img_246.jpg\n",
            "prbp_dataset_demo_250/gallery/004/004_img_245.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_224.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_220.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_221.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_223.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_227.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_234.jpg\n",
            "prbp_dataset_demo_250/gallery/002/002_img_225.jpg\n",
            "prbp_dataset_demo_250/gallery/004/004_img_247.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp81zmGGBwYm",
        "colab_type": "text"
      },
      "source": [
        "#### Modified demo.py to get aggregated re-id information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6uJovh42OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modified demo.py (from https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/demo.py)\n",
        "\n",
        "# imports\n",
        "import argparse\n",
        "import scipy.io\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import datasets\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#####################################################################\n",
        "# show result\n",
        "def imshow(path, title=None):\n",
        "  \"\"\"Imshow for Tensor.\"\"\"\n",
        "  im = plt.imread(path)\n",
        "  plt.imshow(im)\n",
        "  if title is not None:\n",
        "      plt.title(title)\n",
        "  plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "#######################################################################\n",
        "def sort_img(qf, ql, qc, gf, gl, gc, image_datasets):\n",
        "  \"\"\"Sort gallery images based on score.\"\"\"\n",
        "  query = qf.view(-1,1)\n",
        "  score = torch.mm(gf,query)\n",
        "  score = score.squeeze(1).cpu()\n",
        "  score = score.numpy()\n",
        "  # predict index\n",
        "  index = np.argsort(score)  # from small to large\n",
        "  index = index[::-1]\n",
        "  # index = index[0:2000]\n",
        "  # good index\n",
        "  query_index = np.argwhere(gl==ql)\n",
        "  # same camera\n",
        "  camera_index = np.argwhere(gc==qc)\n",
        "\n",
        "  # good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
        "  junk_index1 = np.argwhere(gl==-1)\n",
        "  junk_index2 = np.intersect1d(query_index, camera_index)\n",
        "  junk_index = np.append(junk_index2, junk_index1) \n",
        "\n",
        "  mask = np.in1d(index, junk_index, invert=True)\n",
        "  index = index[mask]\n",
        "  \n",
        "  return (index, sorted(score, reverse=True))\n",
        "\n",
        "########################################################################\n",
        "# visualize the rank result and returns the top images\n",
        "def visualize_result(image_datasets, query_index, query_label, index, gallery_label, \n",
        "                     num_top_images=10, PRINT_OUTPUT=False, SAVE_IMAGE=False):\n",
        "  \"\"\"Return top images and produce visualization of query image with top matching gallery images.\"\"\"\n",
        "  query_path, _ = image_datasets['query'].imgs[query_index]\n",
        "  query_label = query_label[query_index]\n",
        "\n",
        "  if PRINT_OUTPUT:\n",
        "    print(query_path)\n",
        "    print(f'Top {num_top_images} images are as follow:')\n",
        "\n",
        "  top_images = []\n",
        "  try: \n",
        "      # Visualize Ranking Result - Graphical User Interface is needed\n",
        "      if SAVE_IMAGE:\n",
        "        fig = plt.figure(figsize=(16,4))\n",
        "        ax = plt.subplot(1,11,1)\n",
        "        ax.axis('off')\n",
        "        imshow(query_path,'query')\n",
        "      for i in range(num_top_images):\n",
        "          img_path, _ = image_datasets['gallery'].imgs[index[i]]\n",
        "          label = gallery_label[index[i]]\n",
        "\n",
        "          if SAVE_IMAGE:\n",
        "            ax = plt.subplot(1,11,i+2)\n",
        "            ax.axis('off')\n",
        "            imshow(img_path)\n",
        "            if label == query_label:\n",
        "                ax.set_title('%d'%(i+1), color='green')\n",
        "            else:\n",
        "                ax.set_title('%d'%(i+1), color='red')\n",
        "          \n",
        "          if PRINT_OUTPUT:\n",
        "            print(img_path)\n",
        "\n",
        "          top_images.append(img_path)\n",
        "\n",
        "  except RuntimeError:\n",
        "      for i in range(10):\n",
        "          img_path = image_datasets.imgs[index[i]]\n",
        "          if PRINT_OUTPUT:\n",
        "            print(img_path[0])\n",
        "          top_images.append(img_path)\n",
        "      print('If you want to see the visualization of the ranking result, graphical user interface is needed.')\n",
        "\n",
        "  # saves query image and top 10 gallery images to a png file\n",
        "  fig.savefig(\"result.png\")\n",
        "  return (query_path, top_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzDS9k0lJqGM",
        "colab_type": "text"
      },
      "source": [
        "###### New Query to Gallery Matching (Maximum Bipartite Matching)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDmhJE27RzDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_ids(pytorch_result_filename):\n",
        "  \"\"\"Returns all unique query and gallery IDs.\"\"\"\n",
        "  result = scipy.io.loadmat(pytorch_result_filename)\n",
        "  query_label = result['query_label'][0]\n",
        "  gallery_label = result['gallery_label'][0]\n",
        "  qpids = np.unique(np.array(query_label))\n",
        "  gpids = np.unique(np.array(gallery_label))\n",
        "  return qpids, gpids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xayLcyoI3N8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gallery_probs(image_datasets, gallery_label, index, scores, score_threshold=0.5):\n",
        "  \"\"\"\n",
        "  Returns numpy array of aggregated scores for all gallery IDs for a given query image,\n",
        "  assuming the scores are above given score threshold.\n",
        "  \n",
        "  image_datasets -- torchvision image datasets created from dataset on disk\n",
        "  gallery_label -- numpy array of integer person ID labels corresponding to each gallery image\n",
        "  index -- numpy array of gallery image indices, sorted in order of best match to query image\n",
        "  scores -- numpy array of gallery image scores, sorted in order of best score to query image\n",
        "  score_threshold -- minimum score floor for being included in aggregated gallery results\n",
        "  \"\"\"\n",
        "\n",
        "  gpids = np.unique(np.array(gallery_label))\n",
        "  id_to_prob = { gpid: 0 for gpid in gpids }\n",
        "\n",
        "  for i, (idx, score) in enumerate(zip(index, scores)):\n",
        "    img_path, _ = image_datasets['gallery'].imgs[idx]\n",
        "    label = gallery_label[idx]\n",
        "    score = scores[i]\n",
        "\n",
        "    if score < score_threshold:\n",
        "      break\n",
        "\n",
        "    id_to_prob[label] += score\n",
        "\n",
        "  return np.array(list(id_to_prob.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad5o-aOhI1IE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C):\n",
        "    \"\"\"\n",
        "    Computes the bipartite matching between the rows and columns given a\n",
        "    cost matrix, C.\n",
        "    \"\"\"\n",
        "    C = C.copy()  # to avoid affecting the original matrix\n",
        "    prev_ids = []\n",
        "    cur_ids = []\n",
        "    min_costs = []\n",
        "    while (C == np.inf).sum() != C.size:\n",
        "      # find the lowest cost element\n",
        "      min_idx = C.argmin()\n",
        "      i, j = np.unravel_index(min_idx, C.shape)\n",
        "      min_val = C[i][j]\n",
        "\n",
        "      # add to results\n",
        "      min_cost = C.min()\n",
        "      min_costs.append(min_cost)\n",
        "      prev_ids.append(i)\n",
        "      cur_ids.append(j)\n",
        "      \n",
        "      # remove from cost matrix\n",
        "      C[:, j] = np.inf\n",
        "      C[i, :] = np.inf\n",
        "\n",
        "    return prev_ids, cur_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRklw3IwLZjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match_query_to_gallery(test_dataset, pytorch_result_filename, query_path):\n",
        "  \"\"\"\n",
        "  Returns re-ID bipartite matching in the form of a dictionary of query IDs to gallery IDs.\n",
        "  \n",
        "  test_dataset -- full name of test dataset in the form of (prbp_dataset_[DATASET_DIR_NAME])\n",
        "  pytorch_result_filename -- filename of pytorch results from running inference on dataset\n",
        "  query_path -- path to query folder in dataset\n",
        "  \"\"\"\n",
        "\n",
        "  result = scipy.io.loadmat(pytorch_result_filename)\n",
        "  qpaths = result['qpaths']\n",
        "  gpaths = result['gpaths']\n",
        "  qpaths_to_qid = dict(zip(qpaths, range(0, len(qpaths))))\n",
        "  gpaths_to_gid = dict(zip(gpaths, range(0, len(gpaths))))\n",
        "\n",
        "  image_datasets = {x: datasets.ImageFolder(os.path.join(test_dataset, x)) for x in ['gallery','query']}\n",
        "\n",
        "  query_feature = torch.FloatTensor(result['query_f'])\n",
        "  query_cam = result['query_cam']\n",
        "  query_label = result['query_label'][0]\n",
        "  query_feature = query_feature.cuda()\n",
        "\n",
        "  gallery_feature = torch.FloatTensor(result['gallery_f'])\n",
        "  gallery_cam = result['gallery_cam'][0]\n",
        "  gallery_label = result['gallery_label'][0]\n",
        "  gallery_feature = gallery_feature.cuda()\n",
        "\n",
        "  qpids = np.unique(np.array(query_label))\n",
        "  gpids = np.unique(np.array(gallery_label))\n",
        "\n",
        "  all_combined_weights = []\n",
        "\n",
        "  for pid in sorted(os.listdir(query_path)):\n",
        "    combined_weights = np.zeros(len(gpids))\n",
        "\n",
        "    for img in os.listdir(os.path.join(query_path, pid)):\n",
        "      qpath = os.path.join(test_dataset, \"query\", pid, img)\n",
        "\n",
        "      # get sorted results\n",
        "      query_index = qpaths_to_qid[qpath]\n",
        "      (index, scores) = sort_img(query_feature[query_index], query_label[query_index], query_cam[query_index], \n",
        "                                 gallery_feature, gallery_label, gallery_cam, image_datasets)\n",
        "      \n",
        "      # aggregate gallery weights\n",
        "      weights = compute_gallery_probs(image_datasets, gallery_label, index, scores, score_threshold=SCORE_THRESHOLD)\n",
        "      combined_weights += weights\n",
        "\n",
        "    all_combined_weights.append(combined_weights)\n",
        "\n",
        "  # run greedy bipartite matching using combined weights\n",
        "  all_combined_weights = np.array(all_combined_weights)\n",
        "  assigned_qpids, assigned_gpids = bipartite_matching_greedy(1 / all_combined_weights) # invert matrix values\n",
        "  query_to_gallery = dict(zip(assigned_qpids, assigned_gpids))\n",
        "\n",
        "  return query_to_gallery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAUMlaeuQ8Z0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c4e2c57-8439-4966-b642-ebafb2d49115"
      },
      "source": [
        "# adjust score_threshold if needed based on how lenient the re-ID matching should be\n",
        "SCORE_THRESHOLD = 0.5\n",
        "\n",
        "pytorch_result_filename = f'pytorch_result_{test_dataset}.mat'\n",
        "all_qpids, all_gpids = get_all_ids(pytorch_result_filename)\n",
        "query_to_gallery = match_query_to_gallery(test_dataset, pytorch_result_filename, query_path)\n",
        "gallery_to_query = { v: k for k, v in query_to_gallery.items() }\n",
        "\n",
        "print(\"Query to gallery matching: \", query_to_gallery)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query to gallery matching:  {5: 2, 4: 1, 3: 5, 1: 4, 8: 3, 6: 0}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-hEcO-1Kmue",
        "colab_type": "text"
      },
      "source": [
        "###### Assign colors and new IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-MnmBwsuE9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 16 colors, accessible for 99% of the population\n",
        "# https://sashamaps.net/docs/tools/20-colors/\n",
        "\n",
        "MAX_COLORS = None\n",
        "\n",
        "class ColorPicker():\n",
        "  \"\"\"A class to retrieve unique colors from a list of accessible colors.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.colors = [(230, 25, 75), (60, 180, 75), (255, 225, 25), (0, 130, 200), \n",
        "                   (245, 130, 48), (70, 240, 240), (240, 50, 230), (250, 190, 212), \n",
        "                   (0, 128, 128), (220, 190, 255), (170, 110, 40), (255, 250, 200), \n",
        "                   (128, 0, 0), (170, 255, 195), (0, 0, 128), (128, 128, 128)]\n",
        "    self.curr_max_index = len(self.colors) - 1\n",
        "    np.random.seed(42)\n",
        "    \n",
        "  def get_new_color(self):\n",
        "    \"\"\"Returns an unseen color from the 16 accessible colors listed above in (R, G, B) tuple format.\"\"\"\n",
        "    if self.curr_max_index < 0:\n",
        "      raise Exception('No more colors to choose from.')\n",
        "\n",
        "    if self.curr_max_index == 0:\n",
        "      self.curr_max_index -= 1\n",
        "      return self.colors[0]\n",
        "\n",
        "    idx = np.random.randint(self.curr_max_index)\n",
        "    color = self.colors[idx]\n",
        "\n",
        "    self.colors[idx] = self.colors[self.curr_max_index]\n",
        "    self.colors[self.curr_max_index] = color\n",
        "    self.curr_max_index -= 1\n",
        "\n",
        "    return color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4iRWUkCwEHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_color():\n",
        "  \"\"\"Generates a random color in (R, G, B) tuple format.\"\"\"\n",
        "  color = list(np.random.choice(range(256), size=3))\n",
        "  color = tuple([int(val) for val in color])\n",
        "  return color\n",
        "\n",
        "def assign_id_colors(query_to_gallery):\n",
        "  \"\"\"Assign ID colors given a bipartite matching from query IDs to gallery IDs.\"\"\"\n",
        "  cp = ColorPicker()\n",
        "\n",
        "  seen_gpid_colors = {}\n",
        "  query_colors = {}\n",
        "  gallery_colors = {}\n",
        "\n",
        "  for qpid, gpid in query_to_gallery.items():\n",
        "    qpid_color = cp.get_new_color()\n",
        "\n",
        "    if gpid in seen_gpid_colors: # if there is a match already seen\n",
        "      query_colors[qpid] = seen_gpid_colors[gpid]\n",
        "    else:\n",
        "      query_colors[qpid] = qpid_color\n",
        "      gallery_colors[gpid] = qpid_color\n",
        "      seen_gpid_colors[gpid] = qpid_color\n",
        "\n",
        "  return query_colors, gallery_colors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQUf7tu7LN8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reassign_to_new_ids(query_to_gallery):\n",
        "  \"\"\"\n",
        "  Return new matching IDs for original query IDs and gallery IDs across camera views.\n",
        "  \"\"\"\n",
        "  curr_id = 0\n",
        "  new_qpids = {}\n",
        "  new_gpids = {}\n",
        "\n",
        "  # handle query IDs, including duplicates\n",
        "  for qpid, gpid in query_to_gallery.items():\n",
        "    if gpid == None: # unique query ID\n",
        "      unique_qpids.append(qpid)\n",
        "    else: # new query ID\n",
        "      new_qpids[qpid] = curr_id\n",
        "      new_gpids[gpid] = curr_id\n",
        "      curr_id += 1\n",
        "\n",
        "  return new_qpids, new_gpids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM2V9K0uMzcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_colors, gallery_colors = assign_id_colors(query_to_gallery)\n",
        "new_qpids, new_gpids = reassign_to_new_ids(query_to_gallery)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnvjjUOaNyBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "79548356-7a4c-41f9-e28f-000a2ff9101b"
      },
      "source": [
        "print(\"Query ID colors: \", query_colors)\n",
        "print(\"Gallery ID colors: \", gallery_colors)\n",
        "print(\"New query IDs: \", new_qpids)\n",
        "print(\"New gallery IDs: \", new_gpids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query ID colors:  {5: (240, 50, 230), 4: (0, 130, 200), 3: (128, 0, 0), 1: (170, 110, 40), 8: (250, 190, 212), 6: (245, 130, 48)}\n",
            "Gallery ID colors:  {2: (240, 50, 230), 1: (0, 130, 200), 5: (128, 0, 0), 4: (170, 110, 40), 3: (250, 190, 212), 0: (245, 130, 48)}\n",
            "New query IDs:  {5: 0, 4: 1, 3: 2, 1: 3, 8: 4, 6: 5}\n",
            "New gallery IDs:  {2: 0, 1: 1, 5: 2, 4: 3, 3: 4, 0: 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLEXuU-4QAvc",
        "colab_type": "text"
      },
      "source": [
        "#### Generate Demo Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "383TZMXR-Nf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72d9c288-c367-4a2c-d0d3-4b90d7841f89"
      },
      "source": [
        "# specific videos only if using the sample videos\n",
        "# from https://github.com/MLDSAI/Person_reID_baseline_pytorch.git\n",
        "\n",
        "query_video_file = curr_query_video if \"curr_query_video\" in locals() else \"VIRAT_S_000101_final.mp4\"\n",
        "gallery_video_file = curr_gallery_video if \"curr_gallery_video\" in locals() else \"VIRAT_S_000002_final.mp4\"\n",
        "print(\"Query video: \", query_video_file)\n",
        "print(\"Gallery video: \", gallery_video_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query video:  /content/drive/My Drive/reid-sample-videos/VIRAT_S_000002_final.mp4\n",
            "Gallery video:  /content/drive/My Drive/reid-sample-videos/VIRAT_S_000101_final.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UdopFXVC0d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_video_name = query_video_file.split(\".\")[0]\n",
        "gallery_video_name = gallery_video_file.split(\".\")[0]\n",
        "\n",
        "# configure query video input\n",
        "assert os.path.isfile(query_video_file)\n",
        "qvideo = cv2.VideoCapture(query_video_file)\n",
        "qwidth = int(qvideo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "qheight = int(qvideo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "qframes_per_second = qvideo.get(cv2.CAP_PROP_FPS)\n",
        "qnum_frames = int(qvideo.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "query_output_fname = f'{query_video_name}_output.mp4'\n",
        "query_output_file = cv2.VideoWriter(\n",
        "  filename=query_output_fname,\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "  fps=float(qframes_per_second),\n",
        "  frameSize=(qwidth, qheight),\n",
        "  isColor=True,\n",
        ")\n",
        "\n",
        "# configure gallery video input\n",
        "assert os.path.isfile(gallery_video_file)\n",
        "gvideo = cv2.VideoCapture(gallery_video_file)\n",
        "gwidth = int(gvideo.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "gheight = int(gvideo.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "gframes_per_second = gvideo.get(cv2.CAP_PROP_FPS)\n",
        "gnum_frames = int(gvideo.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "gallery_output_fname = f'{gallery_video_name}_output.mp4'\n",
        "gallery_output_file = cv2.VideoWriter(\n",
        "  filename=gallery_output_fname,\n",
        "  fourcc=cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "  fps=float(gframes_per_second),\n",
        "  frameSize=(gwidth, gheight),\n",
        "  isColor=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5nOPXzaYLME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def invert_oitb(oitb):\n",
        "  \"\"\"Returns bounding box to person ID mapping for use in creating demo video.\"\"\"\n",
        "  inverted_oitb = {}\n",
        "  for pid, bboxes in oitb.items():\n",
        "    for bbox in bboxes:\n",
        "      inverted_oitb[tuple(bbox)] = pid\n",
        "  return inverted_oitb\n",
        "\n",
        "# get information from files\n",
        "with open(id_to_bbox_file, \"rb\") as f:\n",
        "  id_to_bbox_loaded = np.load(f, allow_pickle=True)\n",
        "  query_oitb, gallery_oitb = id_to_bbox_loaded\n",
        "  query_bbox_to_id = invert_oitb(query_oitb)\n",
        "  gallery_bbox_to_id = invert_oitb(gallery_oitb)\n",
        "\n",
        "with open(frame_to_bbox_file, \"rb\") as f:\n",
        "  frame_to_bbox_loaded = np.load(f, allow_pickle=True)\n",
        "  query_frame_to_bbox, gallery_frame_to_bbox = frame_to_bbox_loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZQznZmVVXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set variables for printing multi-line legend on final re-ID video\n",
        "TEXT_SIZE, _ = cv2.getTextSize(\"sample text\", cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "LINE_HEIGHT = TEXT_SIZE[1] + 5\n",
        "SHOW_LEGEND = False\n",
        "SHOW_UNMATCHED_IDS = False\n",
        "\n",
        "def create_output_video(video, output, frame_to_bbox, bbox_to_id, id_dict, \n",
        "                        all_pids, new_pids, colors, num_frames_to_output=1000):\n",
        "  \"\"\"Produce output video with color-coded tracks based on re-identification results.\"\"\"\n",
        "\n",
        "  frame_counter = 0\n",
        "  pbar = tqdm(total = num_frames_to_output) \n",
        "\n",
        "  if SHOW_LEGEND:\n",
        "    legend_text = \"THIS ID, OTHER ID\" + \"\\n\"\n",
        "    for pid1, pid2 in sorted(id_dict.items()):\n",
        "      legend_text += f\"ID: {pid1}, \"\n",
        "      if pid2:\n",
        "        legend_text += f\"Other ID: {pid2}\" + \"\\n\"\n",
        "      else:\n",
        "        legend_text += \"No Match\" + \"\\n\"\n",
        "    \n",
        "    legend_text_lines = legend_text.split(\"\\n\")\n",
        "\n",
        "  while True:\n",
        "    frame = video.read()[1]\n",
        "\n",
        "    if frame is None or frame_counter > num_frames_to_output:\n",
        "      break\n",
        "\n",
        "    # draw legend in corner of frame\n",
        "    if SHOW_LEGEND:\n",
        "      curr_y = 40\n",
        "      for line in legend_text_lines:\n",
        "        cv2.putText(frame, line, (10, curr_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "        cv2.putText(frame, line, (11, curr_y+1), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        curr_y += LINE_HEIGHT\n",
        "\n",
        "    # draw bounding boxes around people at the given stride\n",
        "    if frame_counter in frame_to_bbox:\n",
        "      bboxes = frame_to_bbox[frame_counter]\n",
        "\n",
        "      for idx, bbox in enumerate(bboxes):\n",
        "        (startX, startY, endX, endY) = bbox\n",
        "        pid = bbox_to_id[tuple(bbox)]\n",
        "\n",
        "        if pid in colors: # assigned a color\n",
        "          color = colors[pid]\n",
        "          cv2.rectangle(frame, (startX, startY), (endX, endY), \n",
        "                        (color[2], color[1], color[0]), 2)\n",
        "          \n",
        "          text = f\"ID: {new_pids[pid]} \"\n",
        "          cv2.putText(frame, text, (startX - 11, startY - 11), \n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "          cv2.putText(frame, text, (startX - 10, startY - 10), \n",
        "                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (color[2], color[1], color[0]), 2)\n",
        "        else:\n",
        "          if SHOW_UNMATCHED_IDS:\n",
        "            text = \"Unmatched ID\"\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), \n",
        "                          (128, 128, 128), 2)\n",
        "            cv2.putText(frame, text, (startX - 11, startY - 11), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "            cv2.putText(frame, text, (startX - 10, startY - 10), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
        "\n",
        "    output.write(frame)\n",
        "    frame_counter += 1\n",
        "    pbar.update(1)\n",
        "\n",
        "  video.release()\n",
        "  output.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMwyRvdJhO8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "ee45e578a7454f28a08c5b191ab60c51",
            "a2fd2d5265bf43ccad43d4d704efd1ac",
            "0cac64e5cf1749b1bbad14b411fab3dd",
            "40d9f544c6a24096af5201e9f098efb8",
            "7fbe18041aeb4ea796ae940db16fcec3",
            "83b4a837630a4dec9f5a4da938a6ca57",
            "a0bf25f113aa45ceb3f8501479a940b8",
            "e290c1842d414092b31883e6349ae60e",
            "ea054a8469344b40bb3a2c26859ac235",
            "b5163be30ad24a41991801789d99303e",
            "a95fc894144c4386bc7bb8e70312ac0e",
            "dbd4c0521e81418eb24ae2375ec1154b",
            "74943ba99a1d4ee3afa863a9b936204f",
            "069c73792e364ac5a3ecc55f57a58fd0",
            "3ebb080fae4c422995fb2cdd5fa30374",
            "36286fcdaaf44a96bf9d136451ff134e"
          ]
        },
        "outputId": "36a037b6-819d-4644-d9c0-755f5ee41d6e"
      },
      "source": [
        "# number of frames to include in the re-ID output video\n",
        "# change this depending on how many frames were processed when the input videos were processed\n",
        "num_frames_to_output = MAX_FRAMES if \"MAX_FRAMES\" in locals() else 1000\n",
        "print(\"Maximum number of frames processed: \", MAX_FRAMES)\n",
        "\n",
        "# get output videos\n",
        "create_output_video(qvideo, query_output_file, query_frame_to_bbox, query_bbox_to_id, \n",
        "                    query_to_gallery, all_qpids, new_qpids, query_colors, num_frames_to_output)\n",
        "\n",
        "create_output_video(gvideo, gallery_output_file, gallery_frame_to_bbox, gallery_bbox_to_id, \n",
        "                    gallery_to_query, all_gpids, new_gpids, gallery_colors, num_frames_to_output)\n",
        "\n",
        "# download output videos\n",
        "files.download(query_output_fname)\n",
        "files.download(gallery_output_fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum number of frames processed:  250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee45e578a7454f28a08c5b191ab60c51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea054a8469344b40bb3a2c26859ac235",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dbc0b24a-2f1f-4d9f-8ab7-befa71e8156f\", \"VIRAT_S_000002_final_output.mp4\", 15288010)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b8b36879-ad73-4cba-a5e0-5c547a6a383f\", \"VIRAT_S_000101_final_output.mp4\", 10857328)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQM5ieznRCe4",
        "colab_type": "text"
      },
      "source": [
        "### TO DO\n",
        "\n",
        "1. Improve object detection and tracking\n",
        "2. Discuss intra-camera re-id techniques\n",
        "3. Make additional visualization improvements\n",
        "4. Build in human input (via ourselves or Amazon Mechanical Turk)"
      ]
    }
  ]
}