{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tracking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f721bacdf884fdb843ca28c9dcbb6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f85868bfee944d52b3f1f6b9388b1c3e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad1eb9302b9f400e91d03e236bfbd930",
              "IPY_MODEL_f6e3570952794999928c489f34ae170a"
            ]
          }
        },
        "f85868bfee944d52b3f1f6b9388b1c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad1eb9302b9f400e91d03e236bfbd930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d43d0baec6de428abac3ce8688e8a551",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 810,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 810,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff91a83786254af9a7a777f6a3a04c12"
          }
        },
        "f6e3570952794999928c489f34ae170a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c21b5f9fbf9409299cb4963e703c304",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 810/810 [00:19&lt;00:00, 40.85it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec6330fb0f52458491afa016054839cf"
          }
        },
        "d43d0baec6de428abac3ce8688e8a551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff91a83786254af9a7a777f6a3a04c12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c21b5f9fbf9409299cb4963e703c304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec6330fb0f52458491afa016054839cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrichr/visual-contact-tracing/blob/master/Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsont4Rck80H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fe4403a1-f838-41b2-821c-072d5e6ac926"
      },
      "source": [
        "!pip install cython pyyaml==5.1 cython_bbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n",
            "Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.6/dist-packages (5.1)\n",
            "Requirement already satisfied: cython_bbox in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsK7FDiuyl88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f891ef7e-6017-414f-86c4-08a5fd4d6a95"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import scipy\n",
        "import scipy.spatial\n",
        "import scipy.linalg\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "\n",
        "from cython_bbox import bbox_overlaps\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "import time\n",
        "from enum import Enum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zi_6KjJyyJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "  files.upload() # all_boxes.pickle all_keypoints.pickle\n",
        "\n",
        "\"\"\"\n",
        "  Detections produced by detectron should be pickled\n",
        "where all_keypoints and all_boxes are lists of arrays storing\n",
        "the keypoints and boxes for each frame.\n",
        "\n",
        "  Entries of all_boxes are of shape Nx4. Each of the N boxes are in format\n",
        "[x1, y1, x2, y2], where (x1, y1) is the top left corner of the box,\n",
        "and (x2, y2) is the bottom right corner of the box.\n",
        "\n",
        "  Keypoints are of shape Nx17x3. Each of the N keypoint arrays contains 17\n",
        "keypoints which are in format [x, y, score]. The 17 keypoints are the joints of\n",
        "the individual. See JOINT_NAMES for details.\n",
        "\"\"\"\n",
        "LOAD_DETECTIONS = True\n",
        "\n",
        "if LOAD_DETECTIONS:\n",
        "  with open('all_boxes.pickle', mode='rb') as f:\n",
        "    all_boxes = pickle.load(f)\n",
        "  \n",
        "  with open('all_keypoints.pickle', mode='rb') as f:\n",
        "    all_keypoints = pickle.load(f)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/detectron2/issues/754#issuecomment-579463185\n",
        "JOINT_NAMES = [\n",
        "  \"nose\",\n",
        "  \"left_eye\", \"right_eye\",\n",
        "  \"left_ear\", \"right_ear\",\n",
        "  \"left_shoulder\", \"right_shoulder\",\n",
        "  \"left_elbow\", \"right_elbow\",\n",
        "  \"left_wrist\", \"right_wrist\",\n",
        "  \"left_hip\", \"right_hip\",\n",
        "  \"left_knee\", \"right_knee\",\n",
        "  \"left_ankle\", \"right_ankle\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szU0tXRxD4Y_",
        "colab_type": "text"
      },
      "source": [
        "## Kalman Filter\n",
        "Taken directly from https://github.com/nwojke/deep_sort/blob/master/deep_sort/kalman_filter.py\n",
        "\n",
        "Can put this in a different file once it's off of google colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmKlUs7uECIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Table for the 0.95 quantile of the chi-square distribution with N degrees of\n",
        "freedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\n",
        "function and used as Mahalanobis gating threshold.\n",
        "\"\"\"\n",
        "chi2inv95 = {\n",
        "  1: 3.8415,\n",
        "  2: 5.9915,\n",
        "  3: 7.8147,\n",
        "  4: 9.4877,\n",
        "  5: 11.070,\n",
        "  6: 12.592,\n",
        "  7: 14.067,\n",
        "  8: 15.507,\n",
        "  9: 16.919}\n",
        "\n",
        "\n",
        "class KalmanFilter:\n",
        "  \"\"\"\n",
        "  A simple Kalman filter for tracking bounding boxes in image space.\n",
        "\n",
        "  The 8-dimensional state space\n",
        "\n",
        "    x, y, a, h, vx, vy, va, vh\n",
        "\n",
        "  contains the bounding box center position (x, y), aspect ratio a, height h,\n",
        "  and their respective velocities.\n",
        "\n",
        "  Object motion follows a constant velocity model. The bounding box location\n",
        "  (x, y, a, h) is taken as direct observation of the state space (linear\n",
        "  observation model).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    ndim, dt = 4, 1.\n",
        "\n",
        "    # Create Kalman filter model matrices.\n",
        "    self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n",
        "    for i in range(ndim):\n",
        "      self._motion_mat[i, ndim + i] = dt\n",
        "    self._update_mat = np.eye(ndim, 2 * ndim)\n",
        "\n",
        "    # Motion and observation uncertainty are chosen relative to the current\n",
        "    # state estimate. These weights control the amount of uncertainty in\n",
        "    # the model. This is a bit hacky.\n",
        "    self._std_weight_position = 1. / 20\n",
        "    self._std_weight_velocity = 1. / 160\n",
        "\n",
        "  def initiate(self, measurement):\n",
        "    \"\"\"Create track from unassociated measurement.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    measurement : ndarray\n",
        "      Bounding box coordinates (x, y, a, h) with center position (x, y),\n",
        "      aspect ratio a, and height h.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (ndarray, ndarray)\n",
        "      Returns the mean vector (8 dimensional) and covariance matrix (8x8\n",
        "      dimensional) of the new track. Unobserved velocities are initialized\n",
        "      to 0 mean.\n",
        "\n",
        "    \"\"\"\n",
        "    mean_pos = measurement\n",
        "    mean_vel = np.zeros_like(mean_pos)\n",
        "    mean = np.r_[mean_pos, mean_vel]\n",
        "\n",
        "    std = [\n",
        "      2 * self._std_weight_position * measurement[3],\n",
        "      2 * self._std_weight_position * measurement[3],\n",
        "      1e-2,\n",
        "      2 * self._std_weight_position * measurement[3],\n",
        "      10 * self._std_weight_velocity * measurement[3],\n",
        "      10 * self._std_weight_velocity * measurement[3],\n",
        "      1e-5,\n",
        "      10 * self._std_weight_velocity * measurement[3]]\n",
        "    covariance = np.diag(np.square(std))\n",
        "    return mean, covariance\n",
        "\n",
        "  def predict(self, mean, covariance):\n",
        "    \"\"\"Run Kalman filter prediction step.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "      The 8 dimensional mean vector of the object state at the previous\n",
        "      time step.\n",
        "    covariance : ndarray\n",
        "      The 8x8 dimensional covariance matrix of the object state at the\n",
        "      previous time step.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (ndarray, ndarray)\n",
        "      Returns the mean vector and covariance matrix of the predicted\n",
        "      state. Unobserved velocities are initialized to 0 mean.\n",
        "\n",
        "    \"\"\"\n",
        "    std_pos = [\n",
        "      self._std_weight_position * mean[3],\n",
        "      self._std_weight_position * mean[3],\n",
        "      1e-2,\n",
        "      self._std_weight_position * mean[3]]\n",
        "    std_vel = [\n",
        "      self._std_weight_velocity * mean[3],\n",
        "      self._std_weight_velocity * mean[3],\n",
        "      1e-5,\n",
        "      self._std_weight_velocity * mean[3]]\n",
        "    motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n",
        "\n",
        "    mean = np.dot(self._motion_mat, mean)\n",
        "    covariance = np.linalg.multi_dot((\n",
        "      self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n",
        "\n",
        "    return mean, covariance\n",
        "\n",
        "  def project(self, mean, covariance):\n",
        "    \"\"\"Project state distribution to measurement space.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "      The state's mean vector (8 dimensional array).\n",
        "    covariance : ndarray\n",
        "      The state's covariance matrix (8x8 dimensional).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (ndarray, ndarray)\n",
        "      Returns the projected mean and covariance matrix of the given state\n",
        "      estimate.\n",
        "\n",
        "    \"\"\"\n",
        "    std = [\n",
        "      self._std_weight_position * mean[3],\n",
        "      self._std_weight_position * mean[3],\n",
        "      1e-1,\n",
        "      self._std_weight_position * mean[3]]\n",
        "    innovation_cov = np.diag(np.square(std))\n",
        "\n",
        "    mean = np.dot(self._update_mat, mean)\n",
        "    covariance = np.linalg.multi_dot((\n",
        "      self._update_mat, covariance, self._update_mat.T))\n",
        "    return mean, covariance + innovation_cov\n",
        "\n",
        "  def update(self, mean, covariance, measurement):\n",
        "    \"\"\"Run Kalman filter correction step.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "      The predicted state's mean vector (8 dimensional).\n",
        "    covariance : ndarray\n",
        "      The state's covariance matrix (8x8 dimensional).\n",
        "    measurement : ndarray\n",
        "      The 4 dimensional measurement vector (x, y, a, h), where (x, y)\n",
        "      is the center position, a the aspect ratio, and h the height of the\n",
        "      bounding box.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (ndarray, ndarray)\n",
        "      Returns the measurement-corrected state distribution.\n",
        "\n",
        "    \"\"\"\n",
        "    projected_mean, projected_cov = self.project(mean, covariance)\n",
        "\n",
        "    chol_factor, lower = scipy.linalg.cho_factor(\n",
        "      projected_cov, lower=True, check_finite=False)\n",
        "    kalman_gain = scipy.linalg.cho_solve(\n",
        "      (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\n",
        "      check_finite=False).T\n",
        "    innovation = measurement - projected_mean\n",
        "\n",
        "    new_mean = mean + np.dot(innovation, kalman_gain.T)\n",
        "    new_covariance = covariance - np.linalg.multi_dot((\n",
        "      kalman_gain, projected_cov, kalman_gain.T))\n",
        "    return new_mean, new_covariance\n",
        "\n",
        "  def gating_distance(self, mean, covariance, measurements,\n",
        "            only_position=False):\n",
        "    \"\"\"Compute gating distance between state distribution and measurements.\n",
        "\n",
        "    A suitable distance threshold can be obtained from `chi2inv95`. If\n",
        "    `only_position` is False, the chi-square distribution has 4 degrees of\n",
        "    freedom, otherwise 2.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : ndarray\n",
        "      Mean vector over the state distribution (8 dimensional).\n",
        "    covariance : ndarray\n",
        "      Covariance of the state distribution (8x8 dimensional).\n",
        "    measurements : ndarray\n",
        "      An Nx4 dimensional matrix of N measurements, each in\n",
        "      format (x, y, a, h) where (x, y) is the bounding box center\n",
        "      position, a the aspect ratio, and h the height.\n",
        "    only_position : Optional[bool]\n",
        "      If True, distance computation is done with respect to the bounding\n",
        "      box center position only.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      Returns an array of length N, where the i-th element contains the\n",
        "      squared Mahalanobis distance between (mean, covariance) and\n",
        "      `measurements[i]`.\n",
        "\n",
        "    \"\"\"\n",
        "    mean, covariance = self.project(mean, covariance)\n",
        "    if only_position:\n",
        "      mean, covariance = mean[:2], covariance[:2, :2]\n",
        "      measurements = measurements[:, :2]\n",
        "\n",
        "    cholesky_factor = np.linalg.cholesky(covariance)\n",
        "    d = measurements - mean\n",
        "    z = scipy.linalg.solve_triangular(\n",
        "      cholesky_factor, d.T, lower=True, check_finite=False,\n",
        "      overwrite_b=True)\n",
        "    squared_maha = np.sum(z * z, axis=0)\n",
        "    return squared_maha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cqzj_mUn55C",
        "colab_type": "text"
      },
      "source": [
        "#Tracking Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfTMpXFGdlNY",
        "colab_type": "text"
      },
      "source": [
        "### Track class and helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1T0Z5EZdhZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Helpers for the track class, should be moved to a utils file later\n",
        "\n",
        "class CostTypes(Enum):\n",
        "  \"\"\"Enum for cost types used in distance matrix calculation.\"\"\"\n",
        "  IOU = 1 # intersection over union\n",
        "  KEYPOINT_THRESHOLDING = 2 # keypoint algorithm in function pck_distance\n",
        "\n",
        "def xyxy_to_xyah(bbox):\n",
        "  \"\"\"Converts a bounding box from x1y1x2y2 to xyah format.\n",
        "\n",
        "  xyah format is (x_center, y_center, aspect_ratio, height) where\n",
        "  the aspect ratio is width / height.\n",
        "\n",
        "  Args:\n",
        "    bbox: np.ndarray; A length 4 numpy array in x1y1x2y2 format.\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray; A length 4 array containing the same box in xyah format.\n",
        "  \"\"\"\n",
        "  ret = bbox.copy()\n",
        "  ret[2:] -= ret[:2]\n",
        "  ret[:2] += ret[2:] / 2\n",
        "  ret[2] /= ret[3]\n",
        "  return ret\n",
        "\n",
        "def xyah_to_xyxy(bbox):\n",
        "  \"\"\"Converts a bounding box from xyah to x1y1x2y2 format.\n",
        "\n",
        "  xyah format is (x_center, y_center, aspect_ratio, height) where\n",
        "  the aspect ratio is width / height.\n",
        "\n",
        "  Args:\n",
        "    bbox: np.ndarray; A length 4 numpy array in xyah format.\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray; A length 4 array containing the same box in x1y1x2y2 format.\n",
        "  \"\"\"\n",
        "  ret = bbox.copy()\n",
        "  ret[2] *= ret[3]\n",
        "  ret[:2] -= ret[2:] / 2\n",
        "  ret[2:] += ret[:2]\n",
        "  return ret\n",
        "\n",
        "def get_predictions_from_active_tracks(tracks, t, max_age=3, use_kf=True):\n",
        "  \"\"\"Gets predictions for frame t from the tracks of age less than max_age.\n",
        "\n",
        "  Args:\n",
        "    tracks: list[Track]; A list of all tracks in the scene\n",
        "    t: int; The frame of the desired predictions\n",
        "    max_age: int; The maximum number of frames for which the track has not been\n",
        "      updated.\n",
        "    use_kf: bool; True iff the kalman filter is used in predicted box locations.\n",
        "  \n",
        "  Returns:\n",
        "    bbox_preds: nx4 np.ndarray; The predictions from each track for frame t.\n",
        "    kpt_preds: nx17x3 np.ndarray; The predicted keypoints for each track for frame t.\n",
        "    idxs: list[int]; The index of the predictions in the tracks list.\n",
        "      predictions[i] comes from the track tracks[idxs[i]].\n",
        "  \"\"\"\n",
        "  bbox_preds = []\n",
        "  kpt_preds = []\n",
        "  idxs = []\n",
        "\n",
        "  if not tracks:\n",
        "    return [], [], []\n",
        "  \n",
        "  for i in range(len(tracks)):\n",
        "    track = tracks[i]\n",
        "    if track.is_recently_updated(t, max_age):\n",
        "      bbox_preds.append(track.predict(t, use_kf=False))\n",
        "      kpt_preds.append(track.predict(t, kpt=True))\n",
        "      idxs.append(i)\n",
        "  \n",
        "  bbox_preds = np.stack(bbox_preds, axis=0)\n",
        "  kpt_preds = np.stack(kpt_preds, axis=0)\n",
        "  return bbox_preds, kpt_preds, idxs\n",
        "\n",
        "### End helpers\n",
        "\n",
        "\n",
        "# Design doc: \n",
        "# https://docs.google.com/document/d/1ATAqPbnDgWFUZsYsIF_K4sSXrCuYJx6n19Lr1sQW-98\n",
        "\n",
        "# Question: Retroactively infer between short gaps with kalman filter? could be\n",
        "# worthwhile\n",
        "class Track:\n",
        "  \"\"\"A track of bounding boxes and keyframes for a subject in a video.\n",
        "\n",
        "  A track storing the location of a subject in a video. The track can also\n",
        "  predict the future location of the subject at a future time using a Kalman\n",
        "  filter. Throws a ValueError if asked to give the location of the subject\n",
        "  at a frame before initialization. Frames are 0-indexed.\n",
        "\n",
        "  Attributes:\n",
        "    all_boxes: list of np.ndarray; A list containing a numpy array of the\n",
        "     detected bounding boxes for each frame in x1y1x2y2 format.\n",
        "    all_keypoints: list of np.ndarray; A list containing a numpy array of the\n",
        "      estimated keypoints for each of the 17 joints. See JOINT_NAMES for details.\n",
        "    kalman_filter: KalmanFilter; A kalman filter for predicting the bounding box\n",
        "      position in future frames. The filter does not store the estimates.\n",
        "    _mean: np.ndarray; The 8-dimensional mean state estimate from the kalman filter.\n",
        "      The estimate is of format (x, y, a, h, xv, yv, av, hv) where the *v's are the\n",
        "      velocity estimates.\n",
        "    _cov: np.ndarray; The 8x8 covariance matrix for the kalman filter's mean estimates.\n",
        "    idx_list: list[int]; A list of the index for the subject in each frame. If \n",
        "      the subject was not detected in that frame, returns -1. Starts at the\n",
        "      frame where the track was initialized.\n",
        "    start_frame: int; The frame where the track was initialized.\n",
        "    most_recent: int; The most recent frame where the track was updated.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, all_boxes, all_keypoints, frame_idx, t):\n",
        "    self.all_boxes = all_boxes\n",
        "    self.all_keypoints = all_keypoints\n",
        "    self.kalman_filter = KalmanFilter()\n",
        "    bbox = all_boxes[t][frame_idx]\n",
        "    bbox_xyah = xyxy_to_xyah(bbox)\n",
        "    self._mean, self._cov = self.kalman_filter.initiate(bbox_xyah)\n",
        "    self.idx_list = [frame_idx]\n",
        "    self.start_frame = t\n",
        "    self.most_recent = t\n",
        "\n",
        "  def is_recently_updated(self, t, max_age=3):\n",
        "    \"\"\"Returns True if this track has been updated in the last x frames.\n",
        "\n",
        "    Args:\n",
        "      t: int; The current frame index\n",
        "      max_age: int; The amount of frames to consider the update recent.\n",
        "    \n",
        "    Returns:\n",
        "      bool; True iff the track has been updated in the last x frames.\n",
        "    \"\"\"\n",
        "    return abs(t - self.most_recent) < max_age\n",
        "\n",
        "  def predict(self, t, kpt=False, keep_vel=False, use_kf=True):\n",
        "    \"\"\"Predict the subject's position at frame t.\n",
        "\n",
        "    Uses the Kalman filter to predict the box location at frame t.\n",
        "    Gives the most recent set of keypoints if kpt is true.\n",
        "\n",
        "    Args:\n",
        "      t: int; The desired frame index\n",
        "      kpt: bool; True iff we want to get the keypoints instead of the bounding box.\n",
        "      keep_vel: bool; True iff we want to get the full 8 dimensional state.\n",
        "      use_kf: bool; True iff we use the kalman filter to predict\n",
        "    \n",
        "    Returns:\n",
        "      np.ndarray; A numpy array storing the bounding box in x1y1x2y2 format or\n",
        "        the keypoint, depending on kpt.\n",
        "    \n",
        "    Raises:\n",
        "      ValueError: Location at a frame prior to the start of the track was requested.\n",
        "    \"\"\"\n",
        "    if t <= self.most_recent:\n",
        "      return get_val(t, kpt=False)\n",
        "    else:\n",
        "      if kpt:\n",
        "        return self.all_keypoints[self.most_recent][self.idx_list[self.most_recent - self.start_frame]]\n",
        "      elif not use_kf:\n",
        "        return self.all_boxes[self.most_recent][self.idx_list[self.most_recent - self.start_frame]]\n",
        "      # Note: This is not efficient for t far beyond self.most_recent\n",
        "      t_temp = self.most_recent\n",
        "      mean, cov = self._mean, self._cov\n",
        "      if t > t_temp + 250:\n",
        "        # catch for excessive predictions, likely from reId failing\n",
        "        # It can still predict, but would only predict this far in advance from an error.\n",
        "        raise ValueError(f\"Track prediction {t - self.start_frame} frames ahead requested on frame {t}. Frame predictions of 250+ frames are not supported.\")\n",
        "      \n",
        "      while t_temp < t:\n",
        "        mean, cov = self.kalman_filter.predict(mean, cov)\n",
        "        t_temp += 1\n",
        "      \n",
        "      if not keep_vel:\n",
        "        bbox_xyah, _ = self.kalman_filter.project(mean, cov)\n",
        "        bbox_xyxy = xyah_to_xyxy(bbox_xyah)\n",
        "        return bbox_xyxy\n",
        "      else:\n",
        "        return mean\n",
        "\n",
        "  \n",
        "  def get_val(self, t, kpt=False):\n",
        "    \"\"\"Returns the value of the bounding box at frame t.\n",
        "\n",
        "    Args:\n",
        "      t: int; The desired frame index\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray; A numpy array of shape (4, ) storing the bounding box in\n",
        "        x1y1x2y2 format.\n",
        "    \n",
        "    Raises:\n",
        "      ValueError: Location at a frame prior to the start of the track was requested.\n",
        "    \"\"\"\n",
        "    if t < self.start_frame:\n",
        "      raise ValueError(f\"Track starting on frame {self.start_frame} queried for value at frame {t}.\")\n",
        "    elif t <= self.most_recent:\n",
        "      idx = self.idx_list[t - self.most_recent]\n",
        "      if idx == -1:\n",
        "        if kpt:\n",
        "          ret = np.empty((17, 3), dtype=np.float64)\n",
        "          ret[:] = np.nan\n",
        "          return ret\n",
        "        else:\n",
        "          return np.array([np.nan, np.nan, np.nan, np.nan])\n",
        "      else:\n",
        "        if kpt:\n",
        "          return all_keypoints[t][idx]\n",
        "        else:\n",
        "          return all_boxes[t][idx]\n",
        "    else:\n",
        "      raise ValueError(f\"Track does not have any value at frame {t}.\")\n",
        "\n",
        "\n",
        "  def update(self, box_idx, t, filter_cutoff=5):\n",
        "    \"\"\"Updates the track to contain all_boxes[box_idx] at frame t.\n",
        "\n",
        "    Updates the stored memory of the track and also updates the kalman filter\n",
        "    using the value. If the new frame is more than `filter_cutoff` frames after\n",
        "    the last update to the track, we will re-initialize the kalman filter based\n",
        "    on the new observation, as the constant velocity process model is not accurate\n",
        "    on longer time scales without continuous observations.\n",
        "\n",
        "    Args:\n",
        "      box_idx: int; The index of the detection corresponding to this track at frame t.\n",
        "      t: int; The index of the frame.\n",
        "      filter_cutoff: int; The number of frames ahead of a recent update before which\n",
        "        the kalman filter is re-initialized on this new observation.\n",
        "    \n",
        "    Raises:\n",
        "      ValueError: Track updated at frame where value is already set.\n",
        "    \"\"\"\n",
        "    # Update the idx list\n",
        "    if t <= self.most_recent:\n",
        "      raise ValueError(f\"Attempted to update track updated at frame {self.most_recent} with a previous value at frame {t}.\")\n",
        "    elif t == self.most_recent + 1:\n",
        "      self.idx_list.append(box_idx)\n",
        "    else:\n",
        "      len_to_add = t - 1 - self.most_recent\n",
        "      extra = [-1] * len_to_add\n",
        "      self.idx_list.extend(extra)\n",
        "      self.idx_list.append(box_idx)\n",
        "    \n",
        "    \n",
        "    # Get bounding box and convert it\n",
        "    bbox_xyxy = all_boxes[t][box_idx]\n",
        "    bbox_xyah = xyxy_to_xyah(bbox_xyxy)\n",
        "\n",
        "    # re-initialize filter if too many frames have passed\n",
        "    if t > self.most_recent + filter_cutoff:\n",
        "      self._mean, self._cov = self.kalman_filter.initiate(bbox_xyah)\n",
        "      return\n",
        "    \n",
        "    # Update the kalman filter\n",
        "    pred = self.predict(t, keep_vel=True)\n",
        "    self._mean, self._cov = self.kalman_filter.update(pred, self._cov, bbox_xyah)\n",
        "\n",
        "    self.most_recent = t\n",
        "\n",
        "  def get_full_track(self):\n",
        "    \"\"\"Get a track of bounding boxes of length equal to the video length.\n",
        "\n",
        "    For frames where the subject is not detected, returns np.nan for each\n",
        "    bounding box coordinate.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray; nx4 numpy array containing the bounding box for the subject\n",
        "        at each of the n frames in format x1y1x2y2.\n",
        "    \"\"\"\n",
        "    n = len(self.all_boxes)\n",
        "    full_track = np.zeros((n, 4))\n",
        "    for i in range(n):\n",
        "      if i < self.start_frame: # before first detection\n",
        "        full_track[i] = [np.nan] * 4\n",
        "      elif i <= self.most_recent: # in known region\n",
        "        full_track[i] = self.all_boxes[i][self.idx_list[i - self.start_frame]]\n",
        "      else: # after last detection \n",
        "        full_track[i] = [np.nan] * 4\n",
        "    \n",
        "    return full_track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqSFHF6oXEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L106\n",
        "def compute_pairwise_iou(a, b):\n",
        "  \"\"\"Computes the pairwise intersection over union for the arrays of boxes a and b.\n",
        "\n",
        "  Args:\n",
        "    a: np.ndarray; Array of N boxes in format x1y1x2y2.\n",
        "    b: np.ndarray; Array of M boxes in format x1y1x2y2.\n",
        "  \n",
        "  Returns:\n",
        "    np.ndarray; A NxM array where the entry at (i, j) is the intersection over\n",
        "      union of box i from a, and box j from b.\n",
        "  \"\"\"\n",
        "\n",
        "  C = 1 - bbox_overlaps(\n",
        "    np.ascontiguousarray(a, dtype=np.float64),\n",
        "    np.ascontiguousarray(b, dtype=np.float64),\n",
        "  )\n",
        "  return C\n",
        "\n",
        "# Based on\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/utils/keypoints.py#L266\n",
        "def compute_head_size(kps, kpt_names):\n",
        "    \"\"\"Estimates the head size of the subject based on the named keypoints.\n",
        "\n",
        "    This function expects the keypoints to be the 17 used by detectron2, in\n",
        "    particular including nose and shoulder estimates. This is a very rough estimate.\n",
        "    Based on\n",
        "    https://github.com/leonid-pishchulin/poseval/blob/954d8d84f459e942a185f835fc2a0fbdee5ce354/py/eval_helpers.py#L73  # noQA\n",
        "\n",
        "    Args:\n",
        "      kps: np.ndarray; The keypoints for this subject.\n",
        "      kpt_names: list[string]; The ordered list of keypoint names.\n",
        "    \n",
        "    Returns:\n",
        "      float; An estimate of the head size of the subject\n",
        "    \"\"\"\n",
        "    nose = kps[:2, kpt_names.index('nose')]\n",
        "    shoulder = kps[:2, kpt_names.index('left_shoulder')]\n",
        "    # 0.6 x hypotenuse of the head, but don't have those kpts\n",
        "\n",
        "    # The above is from detectrons previous heuristic where they had access to\n",
        "    # the top and bottom of the head, which we do not have access to.\n",
        "    # Thus, I chose two keypoints which have vertical displacement so that\n",
        "    # the estimate will not be extremely small when we have a profile view\n",
        "    # of the subject.\n",
        "\n",
        "    return .4 * np.linalg.norm(nose - shoulder) + 1  # to avoid 0s\n",
        "\n",
        "\n",
        "def pck_distance(kps_a, kps_b, kpt_names=JOINT_NAMES, dist_thresh=0.5):\n",
        "    \"\"\"Compute distance between the 2 keypoints, where each is represented\n",
        "    as a 3x17 or 4x17 np.ndarray.\n",
        "\n",
        "      Computes the proportion of keypoints which are a threshold away from the\n",
        "    corresponding keypoint in the outcome. The threshold is based on the size\n",
        "    of the individual's head.\n",
        "\n",
        "    Args:\n",
        "      kps_a: np.ndarray; The keypoints of subject A.\n",
        "      kps_b: np.ndarray; The keypoints of subject B.\n",
        "      kpt_names: list[string]; The ordered names of the keypoints.\n",
        "      dist_thresh: float; The number of 'head_sizes' away from the previous\n",
        "        corresponding keypoint to be considered accurate.\n",
        "    \n",
        "    Returns:\n",
        "      float; The PCK (Percentage of Correct Keypoints) distance between the two subjects.\n",
        "    \"\"\"\n",
        "    # compute head size as heuristic scale for point separation\n",
        "    head_size = compute_head_size(kps_a, kpt_names)\n",
        "    # distance between all points\n",
        "    normed_dist = np.linalg.norm(kps_a[:2] - kps_b[:2], axis=0) / head_size\n",
        "    match = normed_dist < dist_thresh\n",
        "    pck = np.sum(match) / match.size\n",
        "    pck_dist = 1.0 - pck\n",
        "    return pck_dist\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L114\n",
        "def compute_pairwise_kpt_distance(a, b, kpt_names=JOINT_NAMES):\n",
        "  \"\"\"Computes a distance matrix between two lists of keypoints based on PCK.\n",
        "\n",
        "  This tries to recreate the assignGT function from the evaluation code_dir\n",
        "  https://github.com/leonid-pishchulin/poseval/blob/954d8d84f459e942a185f835fc2a0fbdee5ce354/py/eval_helpers.py#L423  # noQA\n",
        "  Main points:\n",
        "      prToGT is the prediction_to_gt output that I want to recreate\n",
        "      Essentially it represents a form of PCK metric\n",
        "\n",
        "  Args:\n",
        "     a, b (poses): Two sets of poses to match\n",
        "     Each \"poses\" is represented as a list of 3x17 or 4x17 np.ndarray\n",
        "    \n",
        "  Returns:\n",
        "    np.ndarray; The pairwise keypoint distance between the subjects in A and B.\n",
        "  \"\"\"\n",
        "  res = np.zeros((len(a), len(b)))\n",
        "  for i in range(len(a)):\n",
        "    for j in range(len(b)):\n",
        "      res[i, j] = pck_distance(a[i], b[j], kpt_names)\n",
        "  return res\n",
        "\n",
        "# based on facebook research detect and track compute_distance_matrix\n",
        "def compute_distance_matrix(\n",
        "    prev_boxes, prev_keypoints,\n",
        "    cur_boxes, cur_kpt, max_age=3,\n",
        "    cost_types=[CostTypes.IOU], cost_weights=[1.0],\n",
        "):\n",
        "  \"\"\"Computes a distance matrix using the tracks and current\n",
        "  boxes and keypoints.\n",
        "\n",
        "  Uses the tracks to get the list of previous boxes and keypoints from recently\n",
        "  updated tracks. \n",
        "  \"\"\"\n",
        "  assert(len(cost_weights) == len(cost_types))\n",
        "  all_Cs = []\n",
        "  for cost_type, cost_weight in zip(cost_types, cost_weights):\n",
        "    if cost_weight == 0:\n",
        "      continue\n",
        "    if cost_type == CostTypes.IOU:\n",
        "      all_Cs.append(compute_pairwise_iou(prev_boxes, cur_boxes))\n",
        "    elif cost_type == CostTypes.KEYPOINT_THRESHOLDING:\n",
        "      all_Cs.append(compute_pairwise_kpt_distance(\n",
        "        prev_keypoints, cur_kpt))\n",
        "    else:\n",
        "      raise NotImplementedError('Unknown cost type {}'.format(cost_type))\n",
        "    all_Cs[-1] *= cost_weight\n",
        "  return np.sum(np.stack(all_Cs, axis=0), axis=0)\n",
        "\n",
        "# based on\n",
        "# https://github.com/facebookresearch/DetectAndTrack/blob/d66734498a4331cd6fde87d8269499b8577a2842/lib/core/tracking_engine.py#L184\n",
        "def bipartite_matching_greedy(C, max_cost=1.0):\n",
        "  \"\"\"\n",
        "  Computes the bipartite matching between the rows and columns, given the\n",
        "  cost matrix, C. If the cost is greater than or equal to max_cost, the rows and columns\n",
        "  will be matched with the index -1.\n",
        "  \"\"\"\n",
        "  C = C.copy()  # to avoid affecting the original matrix\n",
        "  prev_ids = []\n",
        "  cur_ids = []\n",
        "  row_ids = np.arange(C.shape[0])\n",
        "  col_ids = np.arange(C.shape[1])\n",
        "  while C.size > 0:\n",
        "    # Find the lowest cost element\n",
        "    i, j = np.unravel_index(C.argmin(), C.shape)\n",
        "    # If all remaining costs are greater than max_cost, then\n",
        "    # set the rest of the rows/cols as unmatched and return.\n",
        "    if C.min() >= max_cost:\n",
        "      for row_id in row_ids:\n",
        "        prev_ids.append(row_id)\n",
        "        cur_ids.append(-1)\n",
        "      for col_id in col_ids:\n",
        "        prev_ids.append(-1)\n",
        "        cur_ids.append(col_id)\n",
        "      return prev_ids, cur_ids\n",
        "    # Add to results and remove from the cost matrix\n",
        "    row_id = row_ids[i]\n",
        "    col_id = col_ids[j]\n",
        "    prev_ids.append(row_id)\n",
        "    cur_ids.append(col_id)\n",
        "    C = np.delete(C, i, 0)\n",
        "    C = np.delete(C, j, 1)\n",
        "    row_ids = np.delete(row_ids, i, 0)\n",
        "    col_ids = np.delete(col_ids, j, 0)\n",
        "  return prev_ids, cur_ids\n",
        "\n",
        "def compute_matches(tracks, t, max_age,\n",
        "                    cur_boxes, cur_keypoints,\n",
        "                     cost_types, cost_weights,\n",
        "                     bipart_match_algo, C=None, track_idx=None):\n",
        "  \"\"\"\n",
        "  C (cost matrix): num_prev_boxes x num_current_boxes\n",
        "  Optionally input the cost matrix, in which case you can input dummy values\n",
        "  for the boxes and poses\n",
        "  Returns:\n",
        "      matches: A 1D np.ndarray with as many elements as boxes in current\n",
        "      frame (cur_boxes). For each, there is an integer to index the previous\n",
        "      frame box that it matches to, or -1 if it doesnot match to any previous\n",
        "      box.\n",
        "  \"\"\"\n",
        "  # If there are no tracks, just set everything as new tracks\n",
        "  if not tracks:\n",
        "    nboxes = cur_boxes.shape[0]\n",
        "    matches = -np.ones((nboxes,), dtype=np.int32)\n",
        "    return matches\n",
        "  \n",
        "  # matches structure keeps track of which of the current boxes matches to\n",
        "  # which box in the previous frame. If any idx remains -1, it will be set\n",
        "  # as a new track.\n",
        "  if C is None:\n",
        "    nboxes = cur_boxes.shape[0]\n",
        "    matches = -np.ones((nboxes,), dtype=np.int32)\n",
        "    prev_boxes, prev_keypoints, track_idx = get_predictions_from_active_tracks(tracks, t, max_age, use_kf=True)\n",
        "    C = compute_distance_matrix(prev_boxes, prev_keypoints,\n",
        "        cur_boxes, cur_keypoints, max_age=max_age,\n",
        "        cost_types=cost_types,\n",
        "        cost_weights=cost_weights)\n",
        "  else:\n",
        "    matches = -np.ones((C.shape[1],), dtype=np.int32)\n",
        "  \n",
        "  if bipart_match_algo == 'hungarian':\n",
        "    prev_inds, next_inds = scipy.optimize.linear_sum_assignment(C)\n",
        "  elif bipart_match_algo == 'greedy':\n",
        "    prev_inds, next_inds = bipartite_matching_greedy(C)\n",
        "  else:\n",
        "    raise NotImplementedError('Unknown matching algo: {}'.format(bipart_match_algo))\n",
        "    \n",
        "  assert(len(prev_inds) == len(next_inds))\n",
        "  for i in range(len(prev_inds)):\n",
        "    if next_inds[i] == -1:\n",
        "      # If no match was found for the track, continue\n",
        "      continue\n",
        "    elif prev_inds[i] == -1:\n",
        "      # If no match was found for the box, leave it as -1\n",
        "      matches[next_inds[i]] = -1\n",
        "    else:\n",
        "      matches[next_inds[i]] = track_idx[prev_inds[i]]\n",
        "  return matches\n",
        "\n",
        "def update_tracks(tracks, matches, t, all_boxes, all_keypoints):\n",
        "  \"\"\"Updates the tracks for frame t given the matches, and creates new tracks when necessary.\n",
        "\n",
        "  Args:\n",
        "  \"\"\"\n",
        "  new_tracks.append(0)\n",
        "  for i in range(len(matches)):\n",
        "    idx = matches[i]\n",
        "    if idx == -1:\n",
        "      # There was no previous track, so we instantiate a new track\n",
        "      tracks.append(Track(all_boxes, all_keypoints, i, t))\n",
        "      new_tracks[-1] += 1\n",
        "    else:\n",
        "      # Update the track with the new data\n",
        "      track = tracks[idx]\n",
        "      track.update(i, t)\n",
        "\n",
        "\n",
        "def run_tracker(all_boxes, all_keypoints, max_age, matching_algo=\"greedy\", \n",
        "                cost_types=[CostTypes.IOU], cost_weights=[1.0]):\n",
        "  \"\"\"Runs the full tracker on the boxes and keypoints.\n",
        "\n",
        "  Args:\n",
        "    all_boxes: list[np.ndarray]; The bounding boxes by frame.\n",
        "    all_keypoints: list[np.ndarray]; The keypoints by frame.\n",
        "    max_age: int; The maximum number of frames during which a track can have no\n",
        "      matches before no longer being considered for new frames.\n",
        "    matching_algo: ('greedy', 'hungarian'); The matching algorithm used to match\n",
        "      detections between frames\n",
        "    cost_types: list[CostTypes]; The cost types used in the distance calculation.\n",
        "    cost_weights: list[float]; The weights attached to the cost type with the same\n",
        "      index in the distance calculation.\n",
        "  \n",
        "  Returns:\n",
        "    list[np.ndarray]: The tracks by frame. Each track is of length len(all_boxes)\n",
        "      and contains the bounding box coordinates by frame. The tracks use np.nan\n",
        "      when detections are not present.\n",
        "  \"\"\"\n",
        "  n = len(all_boxes)\n",
        "  tracks = []\n",
        "  for i in tqdm(range(n)):\n",
        "    cur_boxes = all_boxes[i]\n",
        "    cur_kpts = all_keypoints[i]\n",
        "    matches = compute_matches(tracks, i, max_age, cur_boxes, cur_kpts,\n",
        "                              cost_types, cost_weights, matching_algo)\n",
        "    update_tracks(tracks, matches, i, all_boxes, all_keypoints)\n",
        "  \n",
        "  arr_tracks = []\n",
        "  for track in tracks:\n",
        "    arr_tracks.append(track.get_full_track())\n",
        "  \n",
        "  return arr_tracks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCEqbMtLsEky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3f721bacdf884fdb843ca28c9dcbb6ff",
            "f85868bfee944d52b3f1f6b9388b1c3e",
            "ad1eb9302b9f400e91d03e236bfbd930",
            "f6e3570952794999928c489f34ae170a",
            "d43d0baec6de428abac3ce8688e8a551",
            "ff91a83786254af9a7a777f6a3a04c12",
            "3c21b5f9fbf9409299cb4963e703c304",
            "ec6330fb0f52458491afa016054839cf"
          ]
        },
        "outputId": "355e5b7d-9a44-4604-bc5f-ad311b2600fb"
      },
      "source": [
        "new_tracks = []\n",
        "tracks = run_tracker(all_boxes, all_keypoints, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f721bacdf884fdb843ca28c9dcbb6ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=810.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUxF5NRIIGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "690f711d-2f11-422e-e2b9-9e8237c98d7e"
      },
      "source": [
        "print(new_tracks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[98, 5, 6, 5, 3, 2, 4, 3, 4, 2, 5, 3, 3, 2, 2, 1, 3, 2, 3, 6, 4, 0, 6, 5, 2, 6, 5, 5, 4, 2, 6, 2, 4, 2, 1, 4, 0, 1, 3, 1, 5, 3, 3, 3, 2, 1, 1, 3, 4, 4, 3, 3, 3, 2, 0, 3, 5, 6, 3, 3, 3, 2, 4, 6, 2, 2, 1, 3, 3, 8, 5, 5, 2, 2, 3, 6, 6, 0, 4, 5, 2, 4, 4, 4, 4, 2, 3, 2, 3, 4, 2, 1, 6, 1, 4, 1, 1, 6, 4, 2, 3, 1, 3, 6, 3, 2, 5, 9, 5, 1, 2, 2, 6, 2, 4, 2, 3, 1, 5, 2, 3, 6, 2, 4, 4, 6, 5, 3, 3, 2, 3, 6, 4, 1, 4, 6, 5, 7, 4, 5, 6, 5, 4, 5, 2, 4, 5, 2, 2, 7, 4, 4, 4, 5, 1, 1, 3, 2, 4, 2, 1, 6, 1, 6, 5, 4, 2, 8, 4, 2, 4, 3, 2, 1, 2, 4, 5, 2, 5, 4, 8, 5, 5, 2, 3, 4, 3, 2, 5, 4, 6, 2, 6, 2, 1, 4, 3, 2, 7, 4, 4, 2, 4, 5, 4, 3, 1, 4, 1, 2, 5, 2, 3, 2, 1, 0, 3, 3, 4, 4, 1, 3, 5, 5, 3, 3, 1, 4, 5, 3, 6, 1, 2, 5, 3, 5, 3, 0, 2, 6, 6, 8, 1, 3, 4, 5, 0, 2, 8, 5, 4, 4, 5, 2, 2, 5, 3, 2, 2, 3, 6, 3, 5, 1, 5, 4, 5, 4, 4, 7, 1, 5, 3, 2, 3, 2, 2, 5, 3, 5, 5, 5, 3, 5, 7, 6, 5, 3, 4, 2, 6, 4, 2, 4, 3, 2, 3, 1, 3, 5, 4, 2, 6, 1, 5, 4, 2, 1, 1, 10, 1, 6, 5, 2, 6, 3, 4, 1, 2, 2, 8, 2, 5, 4, 2, 5, 3, 4, 3, 4, 3, 8, 5, 4, 4, 6, 6, 3, 7, 4, 2, 1, 1, 4, 2, 7, 7, 4, 5, 7, 2, 6, 4, 2, 3, 1, 5, 4, 3, 1, 5, 5, 5, 3, 4, 6, 4, 4, 4, 0, 7, 4, 2, 5, 3, 5, 4, 3, 3, 8, 3, 1, 7, 5, 3, 5, 2, 3, 3, 5, 3, 3, 6, 2, 7, 6, 3, 2, 6, 3, 6, 4, 5, 6, 4, 4, 3, 4, 5, 5, 3, 2, 4, 2, 3, 2, 4, 6, 4, 3, 5, 5, 2, 6, 1, 5, 0, 5, 2, 3, 4, 8, 7, 1, 4, 2, 3, 3, 2, 6, 3, 3, 4, 8, 3, 1, 0, 7, 3, 2, 8, 1, 5, 2, 4, 1, 1, 3, 2, 3, 8, 2, 2, 3, 6, 5, 5, 4, 6, 10, 4, 1, 9, 4, 3, 3, 6, 6, 2, 2, 6, 5, 3, 1, 5, 4, 2, 4, 6, 4, 1, 3, 6, 1, 1, 6, 5, 1, 9, 4, 6, 3, 1, 6, 3, 2, 3, 4, 1, 2, 7, 2, 4, 4, 2, 3, 3, 2, 1, 2, 3, 4, 3, 4, 4, 3, 4, 4, 2, 3, 3, 2, 7, 4, 5, 3, 3, 4, 6, 3, 2, 6, 4, 2, 1, 7, 3, 4, 3, 6, 2, 3, 4, 9, 5, 5, 4, 4, 6, 1, 2, 1, 2, 1, 5, 3, 5, 2, 2, 4, 3, 3, 3, 8, 2, 1, 6, 4, 3, 2, 8, 4, 4, 5, 2, 5, 2, 5, 4, 6, 5, 3, 6, 3, 8, 4, 3, 8, 3, 2, 6, 4, 5, 5, 1, 4, 0, 7, 4, 2, 2, 3, 0, 4, 2, 8, 6, 1, 6, 1, 6, 2, 4, 5, 3, 6, 6, 3, 6, 1, 6, 6, 5, 5, 3, 5, 4, 1, 0, 0, 6, 10, 5, 4, 2, 3, 2, 4, 4, 5, 4, 2, 5, 5, 9, 2, 3, 8, 5, 3, 4, 2, 4, 7, 0, 2, 4, 11, 1, 6, 4, 3, 3, 4, 4, 1, 5, 4, 2, 4, 6, 3, 4, 6, 5, 4, 1, 8, 4, 2, 2, 4, 6, 1, 3, 8, 5, 4, 5, 7, 2, 1, 9, 5, 2, 6, 1, 7, 7, 4, 2, 5, 2, 3, 3, 3, 2, 0, 8, 5, 2, 4, 6, 4, 4, 2, 4, 3, 6, 3, 5, 6, 4, 2, 6, 3, 3, 4, 5, 9, 4, 6, 4, 5, 3, 4, 2, 5, 2, 5, 6, 6, 5, 6, 7, 5, 5, 4, 4, 1, 5, 4, 6, 4, 3, 6, 8, 4, 6, 2, 3, 2, 1, 1, 3, 7, 2, 3, 1, 6, 6, 5, 3, 4, 5, 2, 6, 6, 3, 5, 0, 4, 4, 6, 5, 3, 2, 5, 4, 2, 1, 1, 4, 4, 2, 3, 6, 2, 7, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKacr2e5DIbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVE_TRACKS = True\n",
        "\n",
        "if SAVE_TRACKS:\n",
        "  filename = 'tracks.pickle'\n",
        "  !rm $filename\n",
        "  with open(filename, mode='wb') as f:\n",
        "    pickle.dump(tracks, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atlQf_TKIb5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "if 1:\n",
        "  !rm demo.avi\n",
        "  out = cv2.VideoWriter('demo.avi', fourcc, 25, (1280, 720))\n",
        "  cap = cv2.VideoCapture('oculus.mp4-reenc.avi')\n",
        "  used_tracks = tracks[0:10]\n",
        "  for i in range(300):\n",
        "    ret, frame = cap.read()\n",
        "    for track in used_tracks:\n",
        "      if any(np.isnan(track[i])):\n",
        "        continue\n",
        "      x1, y1, x2, y2 = track[i]\n",
        "      x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "      cv2.line(frame, (x1, y1), (x1, y2), 255, 2)\n",
        "      cv2.line(frame, (x1, y1), (x2, y1), 255, 2)\n",
        "      cv2.line(frame, (x2, y2), (x1, y2), 255, 2)\n",
        "      cv2.line(frame, (x2, y2), (x2, y1), 255, 2)\n",
        "    out.write(frame)\n",
        "  cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sox-zm5pI1ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}